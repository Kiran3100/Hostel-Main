### Combined Content from Folder: C:\Hostel-Main\app\repositories\file_management ###



# ===== Folder: C:\Hostel-Main\app\repositories\file_management =====

# --- File: C:\Hostel-Main\app\repositories\file_management\document_upload_repository.py ---
"""
Document Upload Repository

Document-specific operations with OCR processing, verification workflows,
and expiry tracking.
"""

from datetime import datetime, timedelta, date as Date
from typing import List, Optional, Dict, Any, Tuple
from sqlalchemy import and_, or_, func, desc, asc, case
from sqlalchemy.orm import Session, joinedload, selectinload

from app.repositories.base.base_repository import BaseRepository
from app.repositories.base.query_builder import QueryBuilder
from app.repositories.base.pagination import PaginationManager, PaginatedResult
from app.models.file_management.document_upload import (
    DocumentUpload,
    DocumentType,
    DocumentValidation,
    DocumentOCR,
    DocumentVerification,
    DocumentExpiry,
)
from app.models.file_management.file_upload import FileUpload
from app.models.user.user import User
from app.models.student.student import Student


class DocumentUploadRepository(BaseRepository[DocumentUpload]):
    """
    Repository for document upload operations with OCR, verification,
    and compliance tracking.
    """

    def __init__(self, db_session: Session):
        super().__init__(DocumentUpload, db_session)

    # ============================================================================
    # CORE DOCUMENT OPERATIONS
    # ============================================================================

    async def create_document_upload(
        self,
        file_id: str,
        document_data: Dict[str, Any],
        uploaded_by_user_id: str,
        audit_context: Optional[Dict[str, Any]] = None,
    ) -> DocumentUpload:
        """
        Create document upload with validation and processing setup.

        Args:
            file_id: Associated file upload ID
            document_data: Document metadata
            uploaded_by_user_id: User uploading the document
            audit_context: Audit context

        Returns:
            Created DocumentUpload

        Raises:
            ValidationException: If document data is invalid
        """
        # Validate document type exists
        doc_type = await self.get_document_type(document_data["document_type"])
        if not doc_type:
            raise ValueError(f"Invalid document type: {document_data['document_type']}")

        document = DocumentUpload(
            file_id=file_id,
            document_id=document_data["document_id"],
            document_type=document_data["document_type"],
            document_subtype=document_data.get("document_subtype"),
            description=document_data.get("description"),
            reference_number=document_data.get("reference_number"),
            issue_date=document_data.get("issue_date"),
            expiry_date=document_data.get("expiry_date"),
            issuing_authority=document_data.get("issuing_authority"),
            uploaded_by_user_id=uploaded_by_user_id,
            student_id=document_data.get("student_id"),
            enable_ocr=document_data.get("enable_ocr", True),
            auto_verify=document_data.get("auto_verify", False),
            redact_sensitive_info=document_data.get("redact_sensitive_info", False),
            ocr_status="pending" if document_data.get("enable_ocr") else "skipped",
            verification_status="pending",
            status="pending",
        )

        created_doc = await self.create(document, audit_context)

        # Setup expiry tracking if applicable
        if created_doc.expiry_date:
            await self.create_expiry_tracking(
                created_doc.id,
                created_doc.expiry_date,
                owner_type="student" if created_doc.student_id else "user",
                owner_id=created_doc.student_id or uploaded_by_user_id,
                owner_email=document_data.get("owner_email"),
            )

        return created_doc

    async def find_by_document_id(
        self,
        document_id: str,
        load_relationships: bool = False,
    ) -> Optional[DocumentUpload]:
        """
        Find document by unique document ID.

        Args:
            document_id: Document identifier
            load_relationships: Whether to load relationships

        Returns:
            DocumentUpload if found
        """
        query = self.db_session.query(DocumentUpload).filter(
            DocumentUpload.document_id == document_id
        )

        if load_relationships:
            query = query.options(
                joinedload(DocumentUpload.file),
                joinedload(DocumentUpload.uploaded_by),
                joinedload(DocumentUpload.student),
                selectinload(DocumentUpload.validations),
                joinedload(DocumentUpload.ocr_result),
                selectinload(DocumentUpload.verifications),
                joinedload(DocumentUpload.expiry_tracking),
            )

        return query.first()

    async def find_by_reference_number(
        self,
        reference_number: str,
        document_type: Optional[str] = None,
    ) -> List[DocumentUpload]:
        """
        Find documents by reference number.

        Args:
            reference_number: Document reference number
            document_type: Optional document type filter

        Returns:
            List of matching documents
        """
        query = self.db_session.query(DocumentUpload).filter(
            DocumentUpload.reference_number == reference_number
        )

        if document_type:
            query = query.filter(DocumentUpload.document_type == document_type)

        return query.all()

    async def search_documents(
        self,
        criteria: Dict[str, Any],
        pagination: Optional[Dict[str, Any]] = None,
        sort_by: str = "created_at",
        sort_order: str = "desc",
    ) -> PaginatedResult[DocumentUpload]:
        """
        Search documents with flexible criteria.

        Args:
            criteria: Search criteria
            pagination: Pagination parameters
            sort_by: Sort field
            sort_order: Sort order

        Returns:
            Paginated document results

        Criteria:
            - document_type: Filter by document type
            - document_subtype: Filter by subtype
            - uploaded_by_user_id: Filter by uploader
            - student_id: Filter by student
            - verification_status: Filter by verification status
            - ocr_status: Filter by OCR status
            - is_expired: Filter by expiry status
            - verified: Filter by verified flag
            - reference_number: Search by reference number
            - expiring_within_days: Documents expiring within N days
            - issued_after: Documents issued after date
            - issued_before: Documents issued before date
        """
        query = QueryBuilder(DocumentUpload, self.db_session)

        if "document_type" in criteria:
            query = query.where(
                DocumentUpload.document_type == criteria["document_type"]
            )

        if "document_subtype" in criteria:
            query = query.where(
                DocumentUpload.document_subtype == criteria["document_subtype"]
            )

        if "uploaded_by_user_id" in criteria:
            query = query.where(
                DocumentUpload.uploaded_by_user_id == criteria["uploaded_by_user_id"]
            )

        if "student_id" in criteria:
            query = query.where(DocumentUpload.student_id == criteria["student_id"])

        if "verification_status" in criteria:
            query = query.where(
                DocumentUpload.verification_status == criteria["verification_status"]
            )

        if "ocr_status" in criteria:
            query = query.where(DocumentUpload.ocr_status == criteria["ocr_status"])

        if "is_expired" in criteria:
            query = query.where(DocumentUpload.is_expired == criteria["is_expired"])

        if "verified" in criteria:
            query = query.where(DocumentUpload.verified == criteria["verified"])

        if "reference_number" in criteria:
            query = query.where(
                DocumentUpload.reference_number.like(
                    f"%{criteria['reference_number']}%"
                )
            )

        if "expiring_within_days" in criteria:
            expiry_threshold = Date.today() + timedelta(
                days=criteria["expiring_within_days"]
            )
            query = query.where(
                DocumentUpload.expiry_date.isnot(None),
                DocumentUpload.expiry_date <= expiry_threshold,
                DocumentUpload.is_expired == False,
            )

        if "issued_after" in criteria:
            query = query.where(DocumentUpload.issue_date >= criteria["issued_after"])

        if "issued_before" in criteria:
            query = query.where(
                DocumentUpload.issue_date <= criteria["issued_before"]
            )

        # Apply sorting
        sort_field = getattr(DocumentUpload, sort_by, DocumentUpload.created_at)
        if sort_order == "desc":
            query = query.order_by(desc(sort_field))
        else:
            query = query.order_by(asc(sort_field))

        return await PaginationManager.paginate(
            query.build(), pagination or {"page": 1, "page_size": 50}
        )

    async def get_student_documents(
        self,
        student_id: str,
        document_type: Optional[str] = None,
        verified_only: bool = False,
    ) -> List[DocumentUpload]:
        """
        Get all documents for a student.

        Args:
            student_id: Student identifier
            document_type: Optional document type filter
            verified_only: Only return verified documents

        Returns:
            List of student documents
        """
        query = self.db_session.query(DocumentUpload).filter(
            DocumentUpload.student_id == student_id
        )

        if document_type:
            query = query.filter(DocumentUpload.document_type == document_type)

        if verified_only:
            query = query.filter(DocumentUpload.verified == True)

        return query.order_by(desc(DocumentUpload.created_at)).all()

    # ============================================================================
    # DOCUMENT TYPE OPERATIONS
    # ============================================================================

    async def get_document_type(
        self,
        type_name: str,
    ) -> Optional[DocumentType]:
        """
        Get document type configuration.

        Args:
            type_name: Document type name

        Returns:
            DocumentType if found
        """
        return (
            self.db_session.query(DocumentType)
            .filter(
                DocumentType.type_name == type_name,
                DocumentType.is_active == True,
            )
            .first()
        )

    async def get_all_document_types(
        self,
        category: Optional[str] = None,
        is_mandatory: Optional[bool] = None,
        is_active: bool = True,
    ) -> List[DocumentType]:
        """
        Get all document types.

        Args:
            category: Filter by category
            is_mandatory: Filter by mandatory flag
            is_active: Filter by active status

        Returns:
            List of document types
        """
        query = self.db_session.query(DocumentType).filter(
            DocumentType.is_active == is_active
        )

        if category:
            query = query.filter(DocumentType.category == category)

        if is_mandatory is not None:
            query = query.filter(DocumentType.is_mandatory == is_mandatory)

        return query.order_by(
            DocumentType.display_order, DocumentType.type_name
        ).all()

    async def create_document_type(
        self,
        type_data: Dict[str, Any],
        audit_context: Optional[Dict[str, Any]] = None,
    ) -> DocumentType:
        """
        Create new document type configuration.

        Args:
            type_data: Document type configuration
            audit_context: Audit context

        Returns:
            Created DocumentType
        """
        doc_type = DocumentType(
            type_name=type_data["type_name"],
            display_name=type_data["display_name"],
            description=type_data.get("description"),
            category=type_data["category"],
            requires_verification=type_data.get("requires_verification", True),
            requires_expiry_date=type_data.get("requires_expiry_date", False),
            requires_reference_number=type_data.get("requires_reference_number", False),
            accepted_formats=type_data.get("accepted_formats"),
            max_size_bytes=type_data.get("max_size_bytes"),
            min_resolution=type_data.get("min_resolution"),
            enable_ocr_by_default=type_data.get("enable_ocr_by_default", True),
            ocr_fields=type_data.get("ocr_fields", []),
            default_validity_days=type_data.get("default_validity_days"),
            expiry_alert_days=type_data.get("expiry_alert_days", 30),
            is_active=type_data.get("is_active", True),
            is_mandatory=type_data.get("is_mandatory", False),
            display_order=type_data.get("display_order", 0),
            validation_rules=type_data.get("validation_rules", {}),
            metadata_schema=type_data.get("metadata_schema", {}),
        )

        self.db_session.add(doc_type)
        self.db_session.commit()
        return doc_type

    async def validate_document_requirements(
        self,
        document_type: str,
        document_data: Dict[str, Any],
    ) -> Tuple[bool, List[str]]:
        """
        Validate document against type requirements.

        Args:
            document_type: Document type name
            document_data: Document data to validate

        Returns:
            Tuple of (is_valid, error_messages)
        """
        doc_type = await self.get_document_type(document_type)
        if not doc_type:
            return False, [f"Invalid document type: {document_type}"]

        errors = []

        # Check required fields
        if doc_type.requires_reference_number and not document_data.get(
            "reference_number"
        ):
            errors.append("Reference number is required for this document type")

        if doc_type.requires_expiry_date and not document_data.get("expiry_date"):
            errors.append("Expiry date is required for this document type")

        # Check file format
        if doc_type.accepted_formats and document_data.get("content_type"):
            if document_data["content_type"] not in doc_type.accepted_formats:
                errors.append(
                    f"File format not accepted. Allowed: {', '.join(doc_type.accepted_formats)}"
                )

        # Check file size
        if doc_type.max_size_bytes and document_data.get("size_bytes"):
            if document_data["size_bytes"] > doc_type.max_size_bytes:
                errors.append(
                    f"File size exceeds maximum of {doc_type.max_size_bytes} bytes"
                )

        return len(errors) == 0, errors

    # ============================================================================
    # OCR OPERATIONS
    # ============================================================================

    async def create_ocr_result(
        self,
        document_id: str,
        ocr_data: Dict[str, Any],
    ) -> DocumentOCR:
        """
        Store OCR processing results.

        Args:
            document_id: Document identifier (internal ID)
            ocr_data: OCR results and extracted data

        Returns:
            Created DocumentOCR
        """
        ocr = DocumentOCR(
            document_id=document_id,
            ocr_status=ocr_data["ocr_status"],
            confidence_score=ocr_data.get("confidence_score"),
            full_text=ocr_data.get("full_text"),
            text_length=len(ocr_data.get("full_text", "")),
            extracted_fields=ocr_data.get("extracted_fields", {}),
            extracted_name=ocr_data.get("extracted_name"),
            extracted_id_number=ocr_data.get("extracted_id_number"),
            extracted_dob=ocr_data.get("extracted_dob"),
            extracted_address=ocr_data.get("extracted_address"),
            extracted_issue_date=ocr_data.get("extracted_issue_date"),
            extracted_expiry_date=ocr_data.get("extracted_expiry_date"),
            ocr_engine=ocr_data.get("ocr_engine", "tesseract"),
            ocr_engine_version=ocr_data.get("ocr_engine_version"),
            language_detected=ocr_data.get("language_detected"),
            processing_time_seconds=ocr_data.get("processing_time_seconds"),
            processed_at=datetime.utcnow(),
            total_pages=ocr_data.get("total_pages", 1),
            pages_data=ocr_data.get("pages_data", []),
            error_message=ocr_data.get("error_message"),
        )

        self.db_session.add(ocr)
        self.db_session.commit()

        # Update document OCR status
        document = await self.find_by_id(document_id)
        document.ocr_completed = ocr_data["ocr_status"] == "completed"
        document.ocr_status = ocr_data["ocr_status"]
        if ocr_data.get("full_text"):
            document.extracted_text_preview = ocr_data["full_text"][:500]
        self.db_session.commit()

        return ocr

    async def get_ocr_result(
        self,
        document_id: str,
    ) -> Optional[DocumentOCR]:
        """
        Get OCR result for document.

        Args:
            document_id: Document identifier

        Returns:
            DocumentOCR if exists
        """
        return (
            self.db_session.query(DocumentOCR)
            .filter(DocumentOCR.document_id == document_id)
            .first()
        )

    async def find_documents_pending_ocr(
        self,
        limit: int = 100,
        priority_document_types: Optional[List[str]] = None,
    ) -> List[DocumentUpload]:
        """
        Find documents pending OCR processing.

        Args:
            limit: Maximum results
            priority_document_types: Document types to prioritize

        Returns:
            List of documents needing OCR
        """
        query = self.db_session.query(DocumentUpload).filter(
            DocumentUpload.enable_ocr == True,
            DocumentUpload.ocr_status == "pending",
        )

        if priority_document_types:
            # Priority documents first
            priority_docs = (
                query.filter(DocumentUpload.document_type.in_(priority_document_types))
                .order_by(asc(DocumentUpload.created_at))
                .limit(limit // 2)
                .all()
            )

            # Then other documents
            other_docs = (
                query.filter(
                    DocumentUpload.document_type.notin_(priority_document_types)
                )
                .order_by(asc(DocumentUpload.created_at))
                .limit(limit - len(priority_docs))
                .all()
            )

            return priority_docs + other_docs
        else:
            return query.order_by(asc(DocumentUpload.created_at)).limit(limit).all()

    async def update_ocr_status(
        self,
        document_id: str,
        status: str,
        error_message: Optional[str] = None,
    ) -> DocumentUpload:
        """
        Update document OCR status.

        Args:
            document_id: Document identifier
            status: OCR status
            error_message: Error message if failed

        Returns:
            Updated DocumentUpload
        """
        document = await self.find_by_id(document_id)
        if not document:
            raise ValueError(f"Document not found: {document_id}")

        document.ocr_status = status
        document.ocr_completed = status == "completed"

        if status == "failed" and error_message:
            # Create OCR result with error
            ocr = DocumentOCR(
                document_id=document_id,
                ocr_status="failed",
                error_message=error_message,
                processed_at=datetime.utcnow(),
            )
            self.db_session.add(ocr)

        self.db_session.commit()
        return document

    # ============================================================================
    # DOCUMENT VALIDATION OPERATIONS
    # ============================================================================

    async def create_validation_result(
        self,
        document_id: str,
        validation_data: Dict[str, Any],
        validated_by_user_id: Optional[str] = None,
    ) -> DocumentValidation:
        """
        Store document validation results.

        Args:
            document_id: Document identifier
            validation_data: Validation results
            validated_by_user_id: User who validated (if manual)

        Returns:
            Created DocumentValidation
        """
        validation = DocumentValidation(
            document_id=document_id,
            validation_type=validation_data["validation_type"],
            is_valid=validation_data["is_valid"],
            validation_score=validation_data.get("validation_score"),
            checks_passed=validation_data.get("checks_passed", []),
            checks_failed=validation_data.get("checks_failed", []),
            warnings=validation_data.get("warnings", []),
            reason=validation_data.get("reason"),
            error_details=validation_data.get("error_details"),
            extracted_metadata=validation_data.get("extracted_metadata", {}),
            detected_type=validation_data.get("detected_type"),
            confidence_level=validation_data.get("confidence_level"),
            validated_at=datetime.utcnow(),
            validated_by_user_id=validated_by_user_id,
            validator_name=validation_data.get("validator_name"),
            validator_version=validation_data.get("validator_version"),
        )

        self.db_session.add(validation)
        self.db_session.commit()
        return validation

    async def get_validation_results(
        self,
        document_id: str,
        validation_type: Optional[str] = None,
    ) -> List[DocumentValidation]:
        """
        Get validation results for document.

        Args:
            document_id: Document identifier
            validation_type: Optional filter by type

        Returns:
            List of validation results
        """
        query = self.db_session.query(DocumentValidation).filter(
            DocumentValidation.document_id == document_id
        )

        if validation_type:
            query = query.filter(DocumentValidation.validation_type == validation_type)

        return query.order_by(desc(DocumentValidation.validated_at)).all()

    # ============================================================================
    # DOCUMENT VERIFICATION OPERATIONS
    # ============================================================================

    async def create_verification(
        self,
        document_id: str,
        verification_data: Dict[str, Any],
        verified_by_user_id: str,
        ip_address: Optional[str] = None,
        user_agent: Optional[str] = None,
    ) -> DocumentVerification:
        """
        Create document verification record.

        Args:
            document_id: Document identifier
            verification_data: Verification decision and details
            verified_by_user_id: User performing verification
            ip_address: Verifier IP address
            user_agent: Verifier user agent

        Returns:
            Created DocumentVerification
        """
        verification = DocumentVerification(
            document_id=document_id,
            verified_by_user_id=verified_by_user_id,
            verification_status=verification_data["verification_status"],
            verification_type=verification_data.get("verification_type", "manual"),
            verification_notes=verification_data.get("verification_notes"),
            rejection_reason=verification_data.get("rejection_reason"),
            verified_reference_number=verification_data.get("verified_reference_number"),
            verified_issue_date=verification_data.get("verified_issue_date"),
            verified_expiry_date=verification_data.get("verified_expiry_date"),
            verification_checklist=verification_data.get("verification_checklist", {}),
            authenticity_score=verification_data.get("authenticity_score"),
            verified_at=datetime.utcnow(),
            ip_address=ip_address,
            user_agent=user_agent,
        )

        self.db_session.add(verification)

        # Update document verification status
        document = await self.find_by_id(document_id)
        document.verification_status = verification_data["verification_status"]
        document.verified = verification_data["verification_status"] == "approved"
        document.verified_by_user_id = verified_by_user_id
        document.verified_at = datetime.utcnow()
        document.verification_notes = verification_data.get("verification_notes")

        if verification_data["verification_status"] == "rejected":
            document.rejection_reason = verification_data.get("rejection_reason")
            document.status = "rejected"
        elif verification_data["verification_status"] == "approved":
            document.status = "verified"

        # Update verified data if provided
        if verification_data.get("verified_reference_number"):
            document.reference_number = verification_data["verified_reference_number"]
        if verification_data.get("verified_issue_date"):
            document.issue_date = verification_data["verified_issue_date"]
        if verification_data.get("verified_expiry_date"):
            document.expiry_date = verification_data["verified_expiry_date"]
            # Update expiry tracking
            if document.expiry_tracking:
                document.expiry_tracking.expiry_date = verification_data[
                    "verified_expiry_date"
                ]
                await self._recalculate_expiry_days(document.expiry_tracking.id)

        self.db_session.commit()
        return verification

    async def get_verification_history(
        self,
        document_id: str,
    ) -> List[DocumentVerification]:
        """
        Get complete verification history for document.

        Args:
            document_id: Document identifier

        Returns:
            List of verifications ordered by date
        """
        return (
            self.db_session.query(DocumentVerification)
            .filter(DocumentVerification.document_id == document_id)
            .order_by(desc(DocumentVerification.verified_at))
            .all()
        )

    async def find_documents_pending_verification(
        self,
        document_type: Optional[str] = None,
        student_id: Optional[str] = None,
        limit: int = 100,
    ) -> List[DocumentUpload]:
        """
        Find documents pending verification.

        Args:
            document_type: Filter by document type
            student_id: Filter by student
            limit: Maximum results

        Returns:
            List of documents pending verification
        """
        query = self.db_session.query(DocumentUpload).filter(
            DocumentUpload.verification_status == "pending"
        )

        if document_type:
            query = query.filter(DocumentUpload.document_type == document_type)

        if student_id:
            query = query.filter(DocumentUpload.student_id == student_id)

        return query.order_by(asc(DocumentUpload.created_at)).limit(limit).all()

    async def get_verification_statistics(
        self,
        start_date: Optional[datetime] = None,
        end_date: Optional[datetime] = None,
        verified_by_user_id: Optional[str] = None,
    ) -> Dict[str, Any]:
        """
        Get verification statistics.

        Args:
            start_date: Start date filter
            end_date: End date filter
            verified_by_user_id: Filter by verifier

        Returns:
            Verification statistics
        """
        query = self.db_session.query(
            func.count(DocumentVerification.id).label("total_verifications"),
            func.count(
                case([(DocumentVerification.verification_status == "approved", 1)])
            ).label("approved"),
            func.count(
                case([(DocumentVerification.verification_status == "rejected", 1)])
            ).label("rejected"),
            func.count(
                case([(DocumentVerification.verification_status == "needs_review", 1)])
            ).label("needs_review"),
            func.avg(DocumentVerification.authenticity_score).label(
                "avg_authenticity_score"
            ),
        )

        if start_date:
            query = query.filter(DocumentVerification.verified_at >= start_date)

        if end_date:
            query = query.filter(DocumentVerification.verified_at <= end_date)

        if verified_by_user_id:
            query = query.filter(
                DocumentVerification.verified_by_user_id == verified_by_user_id
            )

        result = query.first()

        return {
            "total_verifications": result.total_verifications or 0,
            "approved": result.approved or 0,
            "rejected": result.rejected or 0,
            "needs_review": result.needs_review or 0,
            "average_authenticity_score": round(
                result.avg_authenticity_score or 0, 2
            ),
        }

    # ============================================================================
    # DOCUMENT EXPIRY OPERATIONS
    # ============================================================================

    async def create_expiry_tracking(
        self,
        document_id: str,
        expiry_date: Date,
        owner_type: str,
        owner_id: str,
        owner_email: Optional[str] = None,
        alert_threshold_days: int = 30,
    ) -> DocumentExpiry:
        """
        Create expiry tracking for document.

        Args:
            document_id: Document identifier
            expiry_date: Document expiry date
            owner_type: Owner type (student, staff, etc.)
            owner_id: Owner identifier
            owner_email: Owner email for notifications
            alert_threshold_days: Days before expiry to alert

        Returns:
            Created DocumentExpiry
        """
        days_until_expiry = (expiry_date - Date.today()).days
        is_expired = days_until_expiry < 0

        # Determine urgency level
        if is_expired:
            urgency_level = "critical"
        elif days_until_expiry <= 7:
            urgency_level = "high"
        elif days_until_expiry <= 30:
            urgency_level = "medium"
        else:
            urgency_level = "low"

        expiry = DocumentExpiry(
            document_id=document_id,
            expiry_date=expiry_date,
            days_until_expiry=days_until_expiry,
            is_expired=is_expired,
            alert_threshold_days=alert_threshold_days,
            owner_type=owner_type,
            owner_id=owner_id,
            owner_email=owner_email,
            urgency_level=urgency_level,
            last_calculated_at=datetime.utcnow(),
        )

        self.db_session.add(expiry)
        self.db_session.commit()
        return expiry

    async def update_expiry_calculations(
        self,
        batch_size: int = 1000,
    ) -> int:
        """
        Update expiry calculations for all tracked documents.

        Args:
            batch_size: Number of records to update per batch

        Returns:
            Number of records updated
        """
        expiries = (
            self.db_session.query(DocumentExpiry)
            .order_by(asc(DocumentExpiry.last_calculated_at))
            .limit(batch_size)
            .all()
        )

        updated_count = 0
        for expiry in expiries:
            await self._recalculate_expiry_days(expiry.id)
            updated_count += 1

        return updated_count

    async def _recalculate_expiry_days(
        self,
        expiry_id: str,
    ) -> DocumentExpiry:
        """
        Recalculate expiry days and urgency for a document.

        Args:
            expiry_id: DocumentExpiry identifier

        Returns:
            Updated DocumentExpiry
        """
        expiry = self.db_session.query(DocumentExpiry).get(expiry_id)
        if not expiry:
            raise ValueError(f"Expiry tracking not found: {expiry_id}")

        days_until_expiry = (expiry.expiry_date - Date.today()).days
        is_expired = days_until_expiry < 0

        # Update urgency level
        if is_expired:
            urgency_level = "critical"
        elif days_until_expiry <= 7:
            urgency_level = "high"
        elif days_until_expiry <= 30:
            urgency_level = "medium"
        else:
            urgency_level = "low"

        expiry.days_until_expiry = days_until_expiry
        expiry.is_expired = is_expired
        expiry.urgency_level = urgency_level
        expiry.last_calculated_at = datetime.utcnow()

        # Update parent document
        document = expiry.document
        document.is_expired = is_expired
        if is_expired:
            document.verification_status = "expired"
            document.status = "expired"

        self.db_session.commit()
        return expiry

    async def find_expiring_documents(
        self,
        days_threshold: int = 30,
        owner_type: Optional[str] = None,
        urgency_level: Optional[str] = None,
        limit: int = 100,
    ) -> List[DocumentExpiry]:
        """
        Find documents expiring within threshold.

        Args:
            days_threshold: Days until expiry threshold
            owner_type: Filter by owner type
            urgency_level: Filter by urgency level
            limit: Maximum results

        Returns:
            List of expiring documents
        """
        query = self.db_session.query(DocumentExpiry).filter(
            DocumentExpiry.is_expired == False,
            DocumentExpiry.days_until_expiry <= days_threshold,
            DocumentExpiry.days_until_expiry >= 0,
        )

        if owner_type:
            query = query.filter(DocumentExpiry.owner_type == owner_type)

        if urgency_level:
            query = query.filter(DocumentExpiry.urgency_level == urgency_level)

        return (
            query.order_by(asc(DocumentExpiry.days_until_expiry)).limit(limit).all()
        )

    async def find_expired_documents(
        self,
        owner_type: Optional[str] = None,
        document_type: Optional[str] = None,
        limit: int = 100,
    ) -> List[DocumentExpiry]:
        """
        Find expired documents.

        Args:
            owner_type: Filter by owner type
            document_type: Filter by document type
            limit: Maximum results

        Returns:
            List of expired documents
        """
        query = (
            self.db_session.query(DocumentExpiry)
            .join(DocumentUpload, DocumentExpiry.document_id == DocumentUpload.id)
            .filter(DocumentExpiry.is_expired == True)
        )

        if owner_type:
            query = query.filter(DocumentExpiry.owner_type == owner_type)

        if document_type:
            query = query.filter(DocumentUpload.document_type == document_type)

        return (
            query.order_by(asc(DocumentExpiry.expiry_date)).limit(limit).all()
        )

    async def send_expiry_alerts(
        self,
        batch_size: int = 100,
    ) -> int:
        """
        Send expiry alerts for documents needing notification.

        Args:
            batch_size: Number of alerts to send

        Returns:
            Number of alerts sent
        """
        # Find documents needing alerts
        expiries = (
            self.db_session.query(DocumentExpiry)
            .filter(
                DocumentExpiry.is_expired == False,
                DocumentExpiry.days_until_expiry <= DocumentExpiry.alert_threshold_days,
                or_(
                    DocumentExpiry.alert_sent == False,
                    DocumentExpiry.last_alert_sent_at.is_(None),
                    DocumentExpiry.last_alert_sent_at
                    < datetime.utcnow()
                    - timedelta(days=DocumentExpiry.alert_frequency_days),
                ),
            )
            .limit(batch_size)
            .all()
        )

        alerts_sent = 0
        for expiry in expiries:
            # TODO: Trigger actual alert notification
            expiry.alert_sent = True
            expiry.last_alert_sent_at = datetime.utcnow()
            expiry.alert_count += 1
            alerts_sent += 1

        self.db_session.commit()
        return alerts_sent

    async def mark_document_renewed(
        self,
        expiry_id: str,
        renewed_document_id: str,
    ) -> DocumentExpiry:
        """
        Mark document as renewed with new document reference.

        Args:
            expiry_id: Original document expiry ID
            renewed_document_id: New document ID

        Returns:
            Updated DocumentExpiry
        """
        expiry = self.db_session.query(DocumentExpiry).get(expiry_id)
        if not expiry:
            raise ValueError(f"Expiry tracking not found: {expiry_id}")

        expiry.renewal_requested = True
        expiry.renewal_requested_at = datetime.utcnow()
        expiry.renewed_document_id = renewed_document_id

        self.db_session.commit()
        return expiry

    # ============================================================================
    # ANALYTICS AND REPORTING
    # ============================================================================

    async def get_document_statistics(
        self,
        start_date: Optional[datetime] = None,
        end_date: Optional[datetime] = None,
        student_id: Optional[str] = None,
    ) -> Dict[str, Any]:
        """
        Get document upload and verification statistics.

        Args:
            start_date: Start date filter
            end_date: End date filter
            student_id: Filter by student

        Returns:
            Document statistics
        """
        query = self.db_session.query(
            func.count(DocumentUpload.id).label("total_documents"),
            func.count(
                case([(DocumentUpload.verified == True, 1)])
            ).label("verified_documents"),
            func.count(
                case([(DocumentUpload.verification_status == "pending", 1)])
            ).label("pending_verification"),
            func.count(
                case([(DocumentUpload.verification_status == "rejected", 1)])
            ).label("rejected_documents"),
            func.count(
                case([(DocumentUpload.is_expired == True, 1)])
            ).label("expired_documents"),
            func.count(
                case([(DocumentUpload.ocr_completed == True, 1)])
            ).label("ocr_completed"),
        )

        if start_date:
            query = query.filter(DocumentUpload.created_at >= start_date)

        if end_date:
            query = query.filter(DocumentUpload.created_at <= end_date)

        if student_id:
            query = query.filter(DocumentUpload.student_id == student_id)

        result = query.first()

        return {
            "total_documents": result.total_documents or 0,
            "verified_documents": result.verified_documents or 0,
            "pending_verification": result.pending_verification or 0,
            "rejected_documents": result.rejected_documents or 0,
            "expired_documents": result.expired_documents or 0,
            "ocr_completed": result.ocr_completed or 0,
        }

    async def get_document_type_distribution(
        self,
        student_id: Optional[str] = None,
    ) -> List[Dict[str, Any]]:
        """
        Get distribution of documents by type.

        Args:
            student_id: Filter by student

        Returns:
            List of document type statistics
        """
        query = self.db_session.query(
            DocumentUpload.document_type,
            func.count(DocumentUpload.id).label("count"),
            func.count(
                case([(DocumentUpload.verified == True, 1)])
            ).label("verified_count"),
        ).group_by(DocumentUpload.document_type)

        if student_id:
            query = query.filter(DocumentUpload.student_id == student_id)

        results = query.order_by(desc("count")).all()

        return [
            {
                "document_type": row.document_type,
                "count": row.count,
                "verified_count": row.verified_count,
            }
            for row in results
        ]

    async def get_expiry_summary(
        self,
        owner_type: Optional[str] = None,
    ) -> Dict[str, Any]:
        """
        Get summary of document expiry status.

        Args:
            owner_type: Filter by owner type

        Returns:
            Expiry summary statistics
        """
        query = self.db_session.query(
            func.count(DocumentExpiry.id).label("total_tracked"),
            func.count(
                case([(DocumentExpiry.is_expired == True, 1)])
            ).label("expired"),
            func.count(
                case([(DocumentExpiry.urgency_level == "critical", 1)])
            ).label("critical"),
            func.count(
                case([(DocumentExpiry.urgency_level == "high", 1)])
            ).label("high_urgency"),
            func.count(
                case([(DocumentExpiry.urgency_level == "medium", 1)])
            ).label("medium_urgency"),
            func.count(
                case([(DocumentExpiry.urgency_level == "low", 1)])
            ).label("low_urgency"),
        )

        if owner_type:
            query = query.filter(DocumentExpiry.owner_type == owner_type)

        result = query.first()

        return {
            "total_tracked": result.total_tracked or 0,
            "expired": result.expired or 0,
            "critical": result.critical or 0,
            "high_urgency": result.high_urgency or 0,
            "medium_urgency": result.medium_urgency or 0,
            "low_urgency": result.low_urgency or 0,
        }

    async def check_student_document_compliance(
        self,
        student_id: str,
    ) -> Dict[str, Any]:
        """
        Check student's document compliance status.

        Args:
            student_id: Student identifier

        Returns:
            Compliance status and missing documents
        """
        # Get all mandatory document types
        mandatory_types = await self.get_all_document_types(is_mandatory=True)

        # Get student's documents
        student_docs = await self.get_student_documents(
            student_id=student_id, verified_only=True
        )

        student_doc_types = {doc.document_type for doc in student_docs}
        mandatory_type_names = {dt.type_name for dt in mandatory_types}

        missing_types = mandatory_type_names - student_doc_types
        expired_docs = [doc for doc in student_docs if doc.is_expired]
        pending_verification = [
            doc for doc in student_docs if doc.verification_status == "pending"
        ]

        is_compliant = (
            len(missing_types) == 0
            and len(expired_docs) == 0
            and len(pending_verification) == 0
        )

        return {
            "is_compliant": is_compliant,
            "total_mandatory_types": len(mandatory_types),
            "total_documents": len(student_docs),
            "verified_documents": len([d for d in student_docs if d.verified]),
            "missing_document_types": list(missing_types),
            "expired_documents": [
                {"document_id": doc.document_id, "document_type": doc.document_type}
                for doc in expired_docs
            ],
            "pending_verification": [
                {"document_id": doc.document_id, "document_type": doc.document_type}
                for doc in pending_verification
            ],
        }

# --- File: C:\Hostel-Main\app\repositories\file_management\file_aggregate_repository.py ---
"""
File Aggregate Repository

Cross-cutting aggregations, analytics, and complex queries across
all file management entities.
"""

from datetime import datetime, timedelta, date as Date
from typing import List, Optional, Dict, Any, Tuple
from sqlalchemy import and_, or_, func, desc, asc, case, distinct
from sqlalchemy.orm import Session, joinedload

from app.repositories.base.base_repository import BaseRepository
from app.models.file_management.file_upload import FileUpload, FileQuota
from app.models.file_management.image_upload import (
    ImageUpload,
    ImageProcessing,
    ImageOptimization,
)
from app.models.file_management.document_upload import (
    DocumentUpload,
    DocumentExpiry,
    DocumentVerification,
)
from app.models.file_management.file_metadata import (
    FileAnalytics,
    FileAccessLog,
    FileFavorite,
)


class FileAggregateRepository(BaseRepository[FileUpload]):
    """
    Repository for cross-cutting file management operations,
    aggregations, and complex analytics.
    """

    def __init__(self, db_session: Session):
        super().__init__(FileUpload, db_session)

    # ============================================================================
    # COMPREHENSIVE DASHBOARD ANALYTICS
    # ============================================================================

    async def get_dashboard_summary(
        self,
        user_id: Optional[str] = None,
        hostel_id: Optional[str] = None,
        start_date: Optional[datetime] = None,
        end_date: Optional[datetime] = None,
    ) -> Dict[str, Any]:
        """
        Get comprehensive dashboard summary for files, images, and documents.

        Args:
            user_id: Filter by user
            hostel_id: Filter by hostel
            start_date: Start date for time-based metrics
            end_date: End date for time-based metrics

        Returns:
            Complete dashboard summary with all metrics
        """
        # File upload metrics
        file_query = self.db_session.query(
            func.count(FileUpload.id).label("total_files"),
            func.sum(FileUpload.size_bytes).label("total_storage"),
            func.count(
                case([(FileUpload.processing_status == "completed", 1)])
            ).label("processed_files"),
            func.count(
                case([(FileUpload.virus_scan_status == "clean", 1)])
            ).label("clean_files"),
            func.count(
                case([(FileUpload.virus_scan_status == "infected", 1)])
            ).label("infected_files"),
        ).filter(FileUpload.deleted_at.is_(None))

        if user_id:
            file_query = file_query.filter(FileUpload.uploaded_by_user_id == user_id)
        if hostel_id:
            file_query = file_query.filter(FileUpload.hostel_id == hostel_id)
        if start_date:
            file_query = file_query.filter(FileUpload.created_at >= start_date)
        if end_date:
            file_query = file_query.filter(FileUpload.created_at <= end_date)

        file_stats = file_query.first()

        # Image metrics
        image_query = (
            self.db_session.query(
                func.count(ImageUpload.id).label("total_images"),
                func.count(
                    case([(ImageUpload.variants_generated == True, 1)])
                ).label("images_with_variants"),
                func.count(
                    case([(ImageUpload.optimization_completed == True, 1)])
                ).label("optimized_images"),
            )
            .join(FileUpload, ImageUpload.file_id == FileUpload.file_id)
            .filter(FileUpload.deleted_at.is_(None))
        )

        if user_id:
            image_query = image_query.filter(
                FileUpload.uploaded_by_user_id == user_id
            )
        if hostel_id:
            image_query = image_query.filter(FileUpload.hostel_id == hostel_id)
        if start_date:
            image_query = image_query.filter(ImageUpload.created_at >= start_date)
        if end_date:
            image_query = image_query.filter(ImageUpload.created_at <= end_date)

        image_stats = image_query.first()

        # Document metrics
        doc_query = self.db_session.query(
            func.count(DocumentUpload.id).label("total_documents"),
            func.count(
                case([(DocumentUpload.verified == True, 1)])
            ).label("verified_documents"),
            func.count(
                case([(DocumentUpload.verification_status == "pending", 1)])
            ).label("pending_verification"),
            func.count(
                case([(DocumentUpload.is_expired == True, 1)])
            ).label("expired_documents"),
            func.count(
                case([(DocumentUpload.ocr_completed == True, 1)])
            ).label("ocr_completed"),
        )

        if user_id:
            doc_query = doc_query.filter(
                DocumentUpload.uploaded_by_user_id == user_id
            )
        if start_date:
            doc_query = doc_query.filter(DocumentUpload.created_at >= start_date)
        if end_date:
            doc_query = doc_query.filter(DocumentUpload.created_at <= end_date)

        doc_stats = doc_query.first()

        # Access analytics
        access_query = self.db_session.query(
            func.count(FileAccessLog.id).label("total_accesses"),
            func.count(
                case([(FileAccessLog.access_type == "view", 1)])
            ).label("views"),
            func.count(
                case([(FileAccessLog.access_type == "download", 1)])
            ).label("downloads"),
            func.count(distinct(FileAccessLog.accessed_by_user_id)).label(
                "unique_users"
            ),
        )

        if start_date:
            access_query = access_query.filter(
                FileAccessLog.accessed_at >= start_date
            )
        if end_date:
            access_query = access_query.filter(FileAccessLog.accessed_at <= end_date)

        access_stats = access_query.first()

        return {
            "files": {
                "total_files": file_stats.total_files or 0,
                "total_storage_bytes": file_stats.total_storage or 0,
                "processed_files": file_stats.processed_files or 0,
                "clean_files": file_stats.clean_files or 0,
                "infected_files": file_stats.infected_files or 0,
            },
            "images": {
                "total_images": image_stats.total_images or 0,
                "images_with_variants": image_stats.images_with_variants or 0,
                "optimized_images": image_stats.optimized_images or 0,
            },
            "documents": {
                "total_documents": doc_stats.total_documents or 0,
                "verified_documents": doc_stats.verified_documents or 0,
                "pending_verification": doc_stats.pending_verification or 0,
                "expired_documents": doc_stats.expired_documents or 0,
                "ocr_completed": doc_stats.ocr_completed or 0,
            },
            "access": {
                "total_accesses": access_stats.total_accesses or 0,
                "total_views": access_stats.views or 0,
                "total_downloads": access_stats.downloads or 0,
                "unique_users": access_stats.unique_users or 0,
            },
        }

    async def get_storage_analytics(
        self,
        user_id: Optional[str] = None,
        hostel_id: Optional[str] = None,
    ) -> Dict[str, Any]:
        """
        Get detailed storage analytics and breakdown.

        Args:
            user_id: Filter by user
            hostel_id: Filter by hostel

        Returns:
            Storage analytics with breakdowns
        """
        # Overall storage
        query = self.db_session.query(
            func.count(FileUpload.id).label("total_files"),
            func.sum(FileUpload.size_bytes).label("total_bytes"),
            func.avg(FileUpload.size_bytes).label("avg_file_size"),
            func.max(FileUpload.size_bytes).label("largest_file"),
            func.min(FileUpload.size_bytes).label("smallest_file"),
        ).filter(FileUpload.deleted_at.is_(None))

        if user_id:
            query = query.filter(FileUpload.uploaded_by_user_id == user_id)
        if hostel_id:
            query = query.filter(FileUpload.hostel_id == hostel_id)

        overall = query.first()

        # Storage by category
        category_query = (
            self.db_session.query(
                FileUpload.category,
                func.count(FileUpload.id).label("count"),
                func.sum(FileUpload.size_bytes).label("total_bytes"),
            )
            .filter(FileUpload.deleted_at.is_(None))
            .group_by(FileUpload.category)
        )

        if user_id:
            category_query = category_query.filter(
                FileUpload.uploaded_by_user_id == user_id
            )
        if hostel_id:
            category_query = category_query.filter(FileUpload.hostel_id == hostel_id)

        category_breakdown = [
            {
                "category": row.category or "uncategorized",
                "file_count": row.count,
                "total_bytes": row.total_bytes or 0,
                "percentage": (
                    (row.total_bytes / overall.total_bytes * 100)
                    if overall.total_bytes
                    else 0
                ),
            }
            for row in category_query.all()
        ]

        # Storage by content type
        content_type_query = (
            self.db_session.query(
                FileUpload.content_type,
                func.count(FileUpload.id).label("count"),
                func.sum(FileUpload.size_bytes).label("total_bytes"),
            )
            .filter(FileUpload.deleted_at.is_(None))
            .group_by(FileUpload.content_type)
            .order_by(desc("total_bytes"))
            .limit(10)
        )

        if user_id:
            content_type_query = content_type_query.filter(
                FileUpload.uploaded_by_user_id == user_id
            )
        if hostel_id:
            content_type_query = content_type_query.filter(
                FileUpload.hostel_id == hostel_id
            )

        content_type_breakdown = [
            {
                "content_type": row.content_type,
                "file_count": row.count,
                "total_bytes": row.total_bytes or 0,
            }
            for row in content_type_query.all()
        ]

        return {
            "overall": {
                "total_files": overall.total_files or 0,
                "total_bytes": overall.total_bytes or 0,
                "average_file_size_bytes": round(overall.avg_file_size or 0, 2),
                "largest_file_bytes": overall.largest_file or 0,
                "smallest_file_bytes": overall.smallest_file or 0,
            },
            "by_category": category_breakdown,
            "by_content_type": content_type_breakdown,
        }

    async def get_quota_overview(
        self,
        owner_type: Optional[str] = None,
    ) -> Dict[str, Any]:
        """
        Get comprehensive quota overview across all owners.

        Args:
            owner_type: Filter by owner type

        Returns:
            Quota overview statistics
        """
        query = self.db_session.query(
            func.count(FileQuota.id).label("total_quotas"),
            func.sum(FileQuota.quota_bytes).label("total_allocated"),
            func.sum(FileQuota.used_bytes).label("total_used"),
            func.sum(FileQuota.reserved_bytes).label("total_reserved"),
            func.count(
                case([(FileQuota.is_exceeded == True, 1)])
            ).label("exceeded_quotas"),
            func.avg(
                (FileQuota.used_bytes + FileQuota.reserved_bytes)
                / FileQuota.quota_bytes
                * 100
            ).label("avg_usage_percentage"),
        )

        if owner_type:
            query = query.filter(FileQuota.owner_type == owner_type)

        result = query.first()

        # Get quotas nearing limit (>80%)
        nearing_limit = (
            self.db_session.query(FileQuota)
            .filter(
                (FileQuota.used_bytes + FileQuota.reserved_bytes)
                / FileQuota.quota_bytes
                >= 0.8,
                FileQuota.is_exceeded == False,
            )
            .count()
        )

        return {
            "total_quotas": result.total_quotas or 0,
            "total_allocated_bytes": result.total_allocated or 0,
            "total_used_bytes": result.total_used or 0,
            "total_reserved_bytes": result.total_reserved or 0,
            "exceeded_quotas": result.exceeded_quotas or 0,
            "quotas_nearing_limit": nearing_limit,
            "average_usage_percentage": round(result.avg_usage_percentage or 0, 2),
        }

    # ============================================================================
    # TREND ANALYSIS
    # ============================================================================

    async def get_upload_trends(
        self,
        period_days: int = 30,
        user_id: Optional[str] = None,
        hostel_id: Optional[str] = None,
    ) -> List[Dict[str, Any]]:
        """
        Get upload trends over time period.

        Args:
            period_days: Number of days to analyze
            user_id: Filter by user
            hostel_id: Filter by hostel

        Returns:
            Daily upload statistics
        """
        start_date = datetime.utcnow() - timedelta(days=period_days)

        query = (
            self.db_session.query(
                func.date(FileUpload.created_at).label("date"),
                func.count(FileUpload.id).label("upload_count"),
                func.sum(FileUpload.size_bytes).label("total_bytes"),
            )
            .filter(
                FileUpload.created_at >= start_date,
                FileUpload.deleted_at.is_(None),
            )
            .group_by(func.date(FileUpload.created_at))
            .order_by(asc("date"))
        )

        if user_id:
            query = query.filter(FileUpload.uploaded_by_user_id == user_id)
        if hostel_id:
            query = query.filter(FileUpload.hostel_id == hostel_id)

        results = query.all()

        return [
            {
                "date": row.date.isoformat(),
                "upload_count": row.upload_count,
                "total_bytes": row.total_bytes or 0,
            }
            for row in results
        ]

    async def get_access_trends(
        self,
        period_days: int = 30,
        file_id: Optional[str] = None,
    ) -> List[Dict[str, Any]]:
        """
        Get file access trends over time period.

        Args:
            period_days: Number of days to analyze
            file_id: Filter by specific file

        Returns:
            Daily access statistics
        """
        start_date = datetime.utcnow() - timedelta(days=period_days)

        query = (
            self.db_session.query(
                func.date(FileAccessLog.accessed_at).label("date"),
                func.count(FileAccessLog.id).label("total_accesses"),
                func.count(
                    case([(FileAccessLog.access_type == "view", 1)])
                ).label("views"),
                func.count(
                    case([(FileAccessLog.access_type == "download", 1)])
                ).label("downloads"),
                func.count(distinct(FileAccessLog.accessed_by_user_id)).label(
                    "unique_users"
                ),
            )
            .filter(FileAccessLog.accessed_at >= start_date)
            .group_by(func.date(FileAccessLog.accessed_at))
            .order_by(asc("date"))
        )

        if file_id:
            query = query.filter(FileAccessLog.file_id == file_id)

        results = query.all()

        return [
            {
                "date": row.date.isoformat(),
                "total_accesses": row.total_accesses,
                "views": row.views,
                "downloads": row.downloads,
                "unique_users": row.unique_users,
            }
            for row in results
        ]

    # ============================================================================
    # TOP FILES AND RANKINGS
    # ============================================================================

    async def get_most_popular_files(
        self,
        limit: int = 20,
        period_days: Optional[int] = None,
        category: Optional[str] = None,
    ) -> List[Dict[str, Any]]:
        """
        Get most popular files by access metrics.

        Args:
            limit: Maximum results
            period_days: Limit to recent period
            category: Filter by category

        Returns:
            List of popular files with metrics
        """
        query = (
            self.db_session.query(
                FileUpload.file_id,
                FileUpload.filename,
                FileUpload.category,
                FileAnalytics.total_views,
                FileAnalytics.total_downloads,
                FileAnalytics.popularity_score,
                FileAnalytics.unique_viewers,
            )
            .join(FileAnalytics, FileUpload.file_id == FileAnalytics.file_id)
            .filter(FileUpload.deleted_at.is_(None))
        )

        if period_days:
            start_date = datetime.utcnow() - timedelta(days=period_days)
            query = query.filter(FileUpload.created_at >= start_date)

        if category:
            query = query.filter(FileUpload.category == category)

        results = query.order_by(desc(FileAnalytics.popularity_score)).limit(limit).all()

        return [
            {
                "file_id": row.file_id,
                "filename": row.filename,
                "category": row.category,
                "total_views": row.total_views,
                "total_downloads": row.total_downloads,
                "popularity_score": row.popularity_score,
                "unique_viewers": row.unique_viewers,
            }
            for row in results
        ]

    async def get_trending_files(
        self,
        limit: int = 20,
        category: Optional[str] = None,
    ) -> List[Dict[str, Any]]:
        """
        Get currently trending files based on recent activity.

        Args:
            limit: Maximum results
            category: Filter by category

        Returns:
            List of trending files
        """
        query = (
            self.db_session.query(
                FileUpload.file_id,
                FileUpload.filename,
                FileUpload.category,
                FileAnalytics.trending_score,
                FileAnalytics.views_today,
                FileAnalytics.views_this_week,
            )
            .join(FileAnalytics, FileUpload.file_id == FileAnalytics.file_id)
            .filter(FileUpload.deleted_at.is_(None))
        )

        if category:
            query = query.filter(FileUpload.category == category)

        results = query.order_by(desc(FileAnalytics.trending_score)).limit(limit).all()

        return [
            {
                "file_id": row.file_id,
                "filename": row.filename,
                "category": row.category,
                "trending_score": row.trending_score,
                "views_today": row.views_today,
                "views_this_week": row.views_this_week,
            }
            for row in results
        ]

    async def get_largest_files(
        self,
        limit: int = 20,
        category: Optional[str] = None,
        user_id: Optional[str] = None,
    ) -> List[Dict[str, Any]]:
        """
        Get largest files by size.

        Args:
            limit: Maximum results
            category: Filter by category
            user_id: Filter by user

        Returns:
            List of largest files
        """
        query = (
            self.db_session.query(
                FileUpload.file_id,
                FileUpload.filename,
                FileUpload.category,
                FileUpload.size_bytes,
                FileUpload.content_type,
                FileUpload.created_at,
            )
            .filter(FileUpload.deleted_at.is_(None))
        )

        if category:
            query = query.filter(FileUpload.category == category)

        if user_id:
            query = query.filter(FileUpload.uploaded_by_user_id == user_id)

        results = query.order_by(desc(FileUpload.size_bytes)).limit(limit).all()

        return [
            {
                "file_id": row.file_id,
                "filename": row.filename,
                "category": row.category,
                "size_bytes": row.size_bytes,
                "content_type": row.content_type,
                "created_at": row.created_at.isoformat(),
            }
            for row in results
        ]

    # ============================================================================
    # PROCESSING STATUS OVERVIEW
    # ============================================================================

    async def get_processing_overview(self) -> Dict[str, Any]:
        """
        Get comprehensive processing status overview.

        Returns:
            Processing status across all file types
        """
        # File processing
        file_processing = self.db_session.query(
            func.count(FileUpload.id).label("total"),
            func.count(
                case([(FileUpload.processing_status == "pending", 1)])
            ).label("pending"),
            func.count(
                case([(FileUpload.processing_status == "processing", 1)])
            ).label("processing"),
            func.count(
                case([(FileUpload.processing_status == "completed", 1)])
            ).label("completed"),
            func.count(
                case([(FileUpload.processing_status == "failed", 1)])
            ).label("failed"),
        ).filter(FileUpload.deleted_at.is_(None)).first()

        # Image processing queue
        image_processing = self.db_session.query(
            func.count(ImageProcessing.id).label("total"),
            func.count(
                case([(ImageProcessing.status == "pending", 1)])
            ).label("pending"),
            func.count(
                case([(ImageProcessing.status == "processing", 1)])
            ).label("processing"),
            func.count(
                case([(ImageProcessing.status == "completed", 1)])
            ).label("completed"),
            func.count(
                case([(ImageProcessing.status == "failed", 1)])
            ).label("failed"),
        ).first()

        # Document OCR
        doc_ocr = self.db_session.query(
            func.count(DocumentUpload.id).label("total"),
            func.count(
                case([(DocumentUpload.ocr_status == "pending", 1)])
            ).label("pending"),
            func.count(
                case([(DocumentUpload.ocr_status == "processing", 1)])
            ).label("processing"),
            func.count(
                case([(DocumentUpload.ocr_status == "completed", 1)])
            ).label("completed"),
            func.count(
                case([(DocumentUpload.ocr_status == "failed", 1)])
            ).label("failed"),
        ).first()

        # Document verification
        doc_verification = self.db_session.query(
            func.count(DocumentUpload.id).label("total"),
            func.count(
                case([(DocumentUpload.verification_status == "pending", 1)])
            ).label("pending"),
            func.count(
                case([(DocumentUpload.verification_status == "verified", 1)])
            ).label("verified"),
            func.count(
                case([(DocumentUpload.verification_status == "rejected", 1)])
            ).label("rejected"),
        ).first()

        return {
            "file_processing": {
                "total": file_processing.total or 0,
                "pending": file_processing.pending or 0,
                "processing": file_processing.processing or 0,
                "completed": file_processing.completed or 0,
                "failed": file_processing.failed or 0,
            },
            "image_processing": {
                "total": image_processing.total or 0,
                "pending": image_processing.pending or 0,
                "processing": image_processing.processing or 0,
                "completed": image_processing.completed or 0,
                "failed": image_processing.failed or 0,
            },
            "document_ocr": {
                "total": doc_ocr.total or 0,
                "pending": doc_ocr.pending or 0,
                "processing": doc_ocr.processing or 0,
                "completed": doc_ocr.completed or 0,
                "failed": doc_ocr.failed or 0,
            },
            "document_verification": {
                "total": doc_verification.total or 0,
                "pending": doc_verification.pending or 0,
                "verified": doc_verification.verified or 0,
                "rejected": doc_verification.rejected or 0,
            },
        }

    # ============================================================================
    # OPTIMIZATION OPPORTUNITIES
    # ============================================================================

    async def identify_optimization_opportunities(
        self,
        limit: int = 100,
    ) -> Dict[str, Any]:
        """
        Identify files that could benefit from optimization.

        Args:
            limit: Maximum files to analyze

        Returns:
            Optimization opportunities and recommendations
        """
        # Large unoptimized images
        large_images = (
            self.db_session.query(
                ImageUpload.id,
                FileUpload.file_id,
                FileUpload.filename,
                ImageUpload.original_size_bytes,
                ImageUpload.original_width,
                ImageUpload.original_height,
            )
            .join(FileUpload, ImageUpload.file_id == FileUpload.file_id)
            .filter(
                ImageUpload.optimization_completed == False,
                ImageUpload.original_size_bytes > 1024 * 1024,  # > 1MB
                FileUpload.deleted_at.is_(None),
            )
            .order_by(desc(ImageUpload.original_size_bytes))
            .limit(limit)
            .all()
        )

        # Images without variants
        images_without_variants = (
            self.db_session.query(ImageUpload)
            .join(FileUpload, ImageUpload.file_id == FileUpload.file_id)
            .filter(
                ImageUpload.generate_variants == True,
                ImageUpload.variants_generated == False,
                FileUpload.deleted_at.is_(None),
            )
            .count()
        )

        # Unused files (not accessed in 90 days)
        unused_threshold = datetime.utcnow() - timedelta(days=90)
        unused_files = (
            self.db_session.query(
                func.count(FileUpload.id).label("count"),
                func.sum(FileUpload.size_bytes).label("total_bytes"),
            )
            .filter(
                FileUpload.deleted_at.is_(None),
                or_(
                    FileUpload.last_accessed_at.is_(None),
                    FileUpload.last_accessed_at < unused_threshold,
                ),
                FileUpload.created_at < unused_threshold,
            )
            .first()
        )

        # Calculate potential savings from optimization
        unoptimized_images = (
            self.db_session.query(
                func.sum(ImageUpload.original_size_bytes).label("total_size")
            )
            .join(FileUpload, ImageUpload.file_id == FileUpload.file_id)
            .filter(
                ImageUpload.optimization_completed == False,
                FileUpload.deleted_at.is_(None),
            )
            .first()
        )

        # Get average optimization savings from completed optimizations
        avg_savings = (
            self.db_session.query(
                func.avg(ImageOptimization.reduction_percentage).label("avg_reduction")
            ).first()
        )

        estimated_savings = 0
        if unoptimized_images.total_size and avg_savings.avg_reduction:
            estimated_savings = int(
                unoptimized_images.total_size * (avg_savings.avg_reduction / 100)
            )

        return {
            "large_unoptimized_images": {
                "count": len(large_images),
                "files": [
                    {
                        "file_id": img.file_id,
                        "filename": img.filename,
                        "size_bytes": img.original_size_bytes,
                        "dimensions": f"{img.original_width}x{img.original_height}",
                    }
                    for img in large_images[:10]  # Return top 10
                ],
            },
            "images_needing_variants": images_without_variants,
            "unused_files": {
                "count": unused_files.count or 0,
                "total_bytes": unused_files.total_bytes or 0,
            },
            "optimization_potential": {
                "unoptimized_bytes": unoptimized_images.total_size or 0,
                "estimated_savings_bytes": estimated_savings,
                "average_reduction_percentage": round(
                    avg_savings.avg_reduction or 0, 2
                ),
            },
        }

    # ============================================================================
    # COMPLIANCE AND ALERTS
    # ============================================================================

    async def get_compliance_alerts(self) -> Dict[str, Any]:
        """
        Get compliance alerts for documents and files.

        Returns:
            Compliance alerts and warnings
        """
        # Expiring documents (within 30 days)
        expiring_threshold = Date.today() + timedelta(days=30)
        expiring_docs = (
            self.db_session.query(DocumentExpiry)
            .filter(
                DocumentExpiry.is_expired == False,
                DocumentExpiry.expiry_date <= expiring_threshold,
            )
            .count()
        )

        # Expired documents
        expired_docs = (
            self.db_session.query(DocumentExpiry)
            .filter(DocumentExpiry.is_expired == True)
            .count()
        )

        # Documents pending verification
        pending_verification = (
            self.db_session.query(DocumentUpload)
            .filter(DocumentUpload.verification_status == "pending")
            .count()
        )

        # Infected files
        infected_files = (
            self.db_session.query(FileUpload)
            .filter(
                FileUpload.virus_scan_status == "infected",
                FileUpload.deleted_at.is_(None),
            )
            .count()
        )

        # Failed processing
        failed_processing = (
            self.db_session.query(FileUpload)
            .filter(
                FileUpload.processing_status == "failed",
                FileUpload.deleted_at.is_(None),
            )
            .count()
        )

        # Quota exceeded
        quota_exceeded = (
            self.db_session.query(FileQuota)
            .filter(
                FileQuota.is_exceeded == True,
                FileQuota.is_enforced == True,
            )
            .count()
        )

        # Quota warnings (>80% usage)
        quota_warnings = (
            self.db_session.query(FileQuota)
            .filter(
                (FileQuota.used_bytes + FileQuota.reserved_bytes)
                / FileQuota.quota_bytes
                >= 0.8,
                FileQuota.is_exceeded == False,
            )
            .count()
        )

        return {
            "documents": {
                "expiring_soon": expiring_docs,
                "expired": expired_docs,
                "pending_verification": pending_verification,
            },
            "files": {
                "infected": infected_files,
                "failed_processing": failed_processing,
            },
            "quotas": {
                "exceeded": quota_exceeded,
                "warnings": quota_warnings,
            },
        }

    # ============================================================================
    # USER ACTIVITY INSIGHTS
    # ============================================================================

    async def get_user_file_activity(
        self,
        user_id: str,
        period_days: int = 30,
    ) -> Dict[str, Any]:
        """
        Get comprehensive file activity for a user.

        Args:
            user_id: User identifier
            period_days: Period to analyze

        Returns:
            User's file activity summary
        """
        start_date = datetime.utcnow() - timedelta(days=period_days)

        # Upload activity
        uploads = (
            self.db_session.query(
                func.count(FileUpload.id).label("count"),
                func.sum(FileUpload.size_bytes).label("total_bytes"),
            )
            .filter(
                FileUpload.uploaded_by_user_id == user_id,
                FileUpload.created_at >= start_date,
                FileUpload.deleted_at.is_(None),
            )
            .first()
        )

        # Access activity
        accesses = (
            self.db_session.query(
                func.count(FileAccessLog.id).label("total"),
                func.count(
                    case([(FileAccessLog.access_type == "view", 1)])
                ).label("views"),
                func.count(
                    case([(FileAccessLog.access_type == "download", 1)])
                ).label("downloads"),
            )
            .filter(
                FileAccessLog.accessed_by_user_id == user_id,
                FileAccessLog.accessed_at >= start_date,
            )
            .first()
        )

        # Favorites
        favorites_count = (
            self.db_session.query(FileFavorite)
            .filter(FileFavorite.user_id == user_id)
            .count()
        )

        # Recent uploads
        recent_uploads = (
            self.db_session.query(FileUpload)
            .filter(
                FileUpload.uploaded_by_user_id == user_id,
                FileUpload.deleted_at.is_(None),
            )
            .order_by(desc(FileUpload.created_at))
            .limit(5)
            .all()
        )

        # Recent accesses
        recent_accesses = (
            self.db_session.query(FileAccessLog)
            .filter(FileAccessLog.accessed_by_user_id == user_id)
            .order_by(desc(FileAccessLog.accessed_at))
            .limit(5)
            .all()
        )

        return {
            "period_days": period_days,
            "uploads": {
                "count": uploads.count or 0,
                "total_bytes": uploads.total_bytes or 0,
            },
            "accesses": {
                "total": accesses.total or 0,
                "views": accesses.views or 0,
                "downloads": accesses.downloads or 0,
            },
            "favorites_count": favorites_count,
            "recent_uploads": [
                {
                    "file_id": f.file_id,
                    "filename": f.filename,
                    "size_bytes": f.size_bytes,
                    "created_at": f.created_at.isoformat(),
                }
                for f in recent_uploads
            ],
            "recent_accesses": [
                {
                    "file_id": a.file_id,
                    "access_type": a.access_type,
                    "accessed_at": a.accessed_at.isoformat(),
                }
                for a in recent_accesses
            ],
        }

    # ============================================================================
    # BATCH CLEANUP OPERATIONS
    # ============================================================================

    async def cleanup_orphaned_records(
        self,
        batch_size: int = 100,
    ) -> Dict[str, int]:
        """
        Clean up orphaned records across all file management tables.

        Args:
            batch_size: Number of records to process per batch

        Returns:
            Count of cleaned records by type
        """
        cleaned = {
            "orphaned_analytics": 0,
            "orphaned_access_logs": 0,
            "orphaned_favorites": 0,
        }

        # Find and delete analytics for deleted files
        orphaned_analytics = (
            self.db_session.query(FileAnalytics)
            .outerjoin(FileUpload, FileAnalytics.file_id == FileUpload.file_id)
            .filter(FileUpload.id.is_(None))
            .limit(batch_size)
            .all()
        )

        for analytics in orphaned_analytics:
            self.db_session.delete(analytics)
            cleaned["orphaned_analytics"] += 1

        # Find and delete access logs for deleted files
        orphaned_logs = (
            self.db_session.query(FileAccessLog)
            .outerjoin(FileUpload, FileAccessLog.file_id == FileUpload.file_id)
            .filter(FileUpload.id.is_(None))
            .limit(batch_size)
            .all()
        )

        for log in orphaned_logs:
            self.db_session.delete(log)
            cleaned["orphaned_access_logs"] += 1

        # Find and delete favorites for deleted files
        orphaned_favorites = (
            self.db_session.query(FileFavorite)
            .outerjoin(FileUpload, FileFavorite.file_id == FileUpload.file_id)
            .filter(FileUpload.id.is_(None))
            .limit(batch_size)
            .all()
        )

        for favorite in orphaned_favorites:
            self.db_session.delete(favorite)
            cleaned["orphaned_favorites"] += 1

        self.db_session.commit()
        return cleaned

    async def archive_old_access_logs(
        self,
        days_old: int = 365,
        batch_size: int = 1000,
    ) -> int:
        """
        Archive old access logs.

        Args:
            days_old: Age threshold in days
            batch_size: Number of logs to archive

        Returns:
            Number of logs archived
        """
        threshold_date = datetime.utcnow() - timedelta(days=days_old)

        old_logs = (
            self.db_session.query(FileAccessLog)
            .filter(FileAccessLog.accessed_at < threshold_date)
            .limit(batch_size)
            .all()
        )

        # In a real implementation, you would archive to cold storage
        # For now, we'll just delete them
        for log in old_logs:
            self.db_session.delete(log)

        self.db_session.commit()
        return len(old_logs)

# --- File: C:\Hostel-Main\app\repositories\file_management\file_metadata_repository.py ---
"""
File Metadata Repository

Metadata, tagging, access control, versioning, analytics, and audit operations.
"""

from datetime import datetime, timedelta
from typing import List, Optional, Dict, Any, Set
from sqlalchemy import and_, or_, func, desc, asc, case
from sqlalchemy.orm import Session, joinedload

from app.repositories.base.base_repository import BaseRepository
from app.repositories.base.query_builder import QueryBuilder
from app.repositories.base.pagination import PaginationManager, PaginatedResult
from app.models.file_management.file_metadata import (
    FileTag,
    FileAccess,
    FileVersion,
    FileAnalytics,
    FileAccessLog,
    FileFavorite,
)
from app.models.file_management.file_upload import FileUpload


class FileMetadataRepository(BaseRepository[FileTag]):
    """
    Repository for file metadata, tagging, access control, versioning,
    and analytics operations.
    """

    def __init__(self, db_session: Session):
        super().__init__(FileTag, db_session)

    # ============================================================================
    # FILE TAG OPERATIONS
    # ============================================================================

    async def create_tag(
        self,
        tag_data: Dict[str, Any],
        created_by_user_id: Optional[str] = None,
    ) -> FileTag:
        """
        Create file tag.

        Args:
            tag_data: Tag configuration
            created_by_user_id: User creating the tag

        Returns:
            Created FileTag
        """
        tag = FileTag(
            tag_name=tag_data["tag_name"].lower().strip(),
            tag_type=tag_data.get("tag_type", "user"),
            parent_tag_id=tag_data.get("parent_tag_id"),
            description=tag_data.get("description"),
            color=tag_data.get("color"),
            icon=tag_data.get("icon"),
            created_by_user_id=created_by_user_id,
            usage_count=0,
            is_active=tag_data.get("is_active", True),
        )

        self.db_session.add(tag)
        self.db_session.commit()
        return tag

    async def get_tag_by_name(
        self,
        tag_name: str,
        tag_type: Optional[str] = None,
    ) -> Optional[FileTag]:
        """
        Get tag by name.

        Args:
            tag_name: Tag name
            tag_type: Optional tag type filter

        Returns:
            FileTag if found
        """
        query = self.db_session.query(FileTag).filter(
            FileTag.tag_name == tag_name.lower().strip(),
            FileTag.is_active == True,
        )

        if tag_type:
            query = query.filter(FileTag.tag_type == tag_type)

        return query.first()

    async def get_or_create_tag(
        self,
        tag_name: str,
        tag_type: str = "user",
        created_by_user_id: Optional[str] = None,
    ) -> FileTag:
        """
        Get existing tag or create new one.

        Args:
            tag_name: Tag name
            tag_type: Tag type
            created_by_user_id: User creating tag

        Returns:
            Existing or new FileTag
        """
        tag = await self.get_tag_by_name(tag_name, tag_type)

        if not tag:
            tag = await self.create_tag(
                {
                    "tag_name": tag_name,
                    "tag_type": tag_type,
                },
                created_by_user_id=created_by_user_id,
            )

        return tag

    async def get_popular_tags(
        self,
        limit: int = 50,
        tag_type: Optional[str] = None,
    ) -> List[FileTag]:
        """
        Get most popular tags by usage count.

        Args:
            limit: Maximum results
            tag_type: Filter by tag type

        Returns:
            List of popular tags
        """
        query = self.db_session.query(FileTag).filter(FileTag.is_active == True)

        if tag_type:
            query = query.filter(FileTag.tag_type == tag_type)

        return query.order_by(desc(FileTag.usage_count)).limit(limit).all()

    async def increment_tag_usage(
        self,
        tag_id: str,
    ) -> FileTag:
        """
        Increment tag usage count.

        Args:
            tag_id: Tag identifier

        Returns:
            Updated FileTag
        """
        tag = await self.find_by_id(tag_id)
        if tag:
            tag.usage_count += 1
            self.db_session.commit()
        return tag

    async def decrement_tag_usage(
        self,
        tag_id: str,
    ) -> FileTag:
        """
        Decrement tag usage count.

        Args:
            tag_id: Tag identifier

        Returns:
            Updated FileTag
        """
        tag = await self.find_by_id(tag_id)
        if tag and tag.usage_count > 0:
            tag.usage_count -= 1
            self.db_session.commit()
        return tag

    async def search_tags(
        self,
        query: str,
        tag_type: Optional[str] = None,
        limit: int = 20,
    ) -> List[FileTag]:
        """
        Search tags by name.

        Args:
            query: Search query
            tag_type: Filter by tag type
            limit: Maximum results

        Returns:
            List of matching tags
        """
        db_query = self.db_session.query(FileTag).filter(
            FileTag.tag_name.like(f"%{query.lower()}%"),
            FileTag.is_active == True,
        )

        if tag_type:
            db_query = db_query.filter(FileTag.tag_type == tag_type)

        return db_query.order_by(desc(FileTag.usage_count)).limit(limit).all()

    # ============================================================================
    # FILE ACCESS CONTROL OPERATIONS
    # ============================================================================

    async def grant_access(
        self,
        file_id: str,
        access_data: Dict[str, Any],
        granted_by_user_id: str,
    ) -> FileAccess:
        """
        Grant file access to user/role/group.

        Args:
            file_id: File identifier (file_id, not internal id)
            access_data: Access configuration
            granted_by_user_id: User granting access

        Returns:
            Created FileAccess

        Raises:
            ValueError: If access already exists
        """
        # Check if access already exists
        existing = await self.get_access(
            file_id=file_id,
            subject_type=access_data["subject_type"],
            subject_id=access_data["subject_id"],
        )

        if existing and not existing.is_revoked:
            raise ValueError("Access already granted to this subject")

        access = FileAccess(
            file_id=file_id,
            access_type=access_data.get("access_type", "user"),
            subject_type=access_data["subject_type"],
            subject_id=access_data["subject_id"],
            can_view=access_data.get("can_view", True),
            can_download=access_data.get("can_download", True),
            can_edit=access_data.get("can_edit", False),
            can_delete=access_data.get("can_delete", False),
            can_share=access_data.get("can_share", False),
            expires_at=access_data.get("expires_at"),
            granted_by_user_id=granted_by_user_id,
            granted_at=datetime.utcnow(),
        )

        self.db_session.add(access)
        self.db_session.commit()
        return access

    async def get_access(
        self,
        file_id: str,
        subject_type: str,
        subject_id: str,
    ) -> Optional[FileAccess]:
        """
        Get access record for specific subject.

        Args:
            file_id: File identifier
            subject_type: Subject type
            subject_id: Subject identifier

        Returns:
            FileAccess if exists
        """
        return (
            self.db_session.query(FileAccess)
            .filter(
                FileAccess.file_id == file_id,
                FileAccess.subject_type == subject_type,
                FileAccess.subject_id == subject_id,
            )
            .first()
        )

    async def get_file_access_list(
        self,
        file_id: str,
        include_revoked: bool = False,
    ) -> List[FileAccess]:
        """
        Get all access records for a file.

        Args:
            file_id: File identifier
            include_revoked: Whether to include revoked access

        Returns:
            List of access records
        """
        query = self.db_session.query(FileAccess).filter(FileAccess.file_id == file_id)

        if not include_revoked:
            query = query.filter(FileAccess.is_revoked == False)

        return query.order_by(desc(FileAccess.granted_at)).all()

    async def check_access_permission(
        self,
        file_id: str,
        subject_type: str,
        subject_id: str,
        permission: str,
    ) -> bool:
        """
        Check if subject has specific permission on file.

        Args:
            file_id: File identifier
            subject_type: Subject type
            subject_id: Subject identifier
            permission: Permission to check (view, download, edit, delete, share)

        Returns:
            True if permission granted
        """
        access = await self.get_access(file_id, subject_type, subject_id)

        if not access or access.is_revoked:
            return False

        # Check expiration
        if access.expires_at and access.expires_at < datetime.utcnow():
            access.is_expired = True
            self.db_session.commit()
            return False

        permission_map = {
            "view": access.can_view,
            "download": access.can_download,
            "edit": access.can_edit,
            "delete": access.can_delete,
            "share": access.can_share,
        }

        return permission_map.get(permission, False)

    async def revoke_access(
        self,
        access_id: str,
        revoked_by_user_id: str,
        revocation_reason: Optional[str] = None,
    ) -> FileAccess:
        """
        Revoke file access.

        Args:
            access_id: FileAccess identifier
            revoked_by_user_id: User revoking access
            revocation_reason: Reason for revocation

        Returns:
            Updated FileAccess
        """
        access = self.db_session.query(FileAccess).get(access_id)

        if not access:
            raise ValueError(f"Access not found: {access_id}")

        access.is_revoked = True
        access.revoked_by_user_id = revoked_by_user_id
        access.revoked_at = datetime.utcnow()
        access.revocation_reason = revocation_reason

        self.db_session.commit()
        return access

    async def cleanup_expired_access(
        self,
        batch_size: int = 100,
    ) -> int:
        """
        Mark expired access records.

        Args:
            batch_size: Number of records to process

        Returns:
            Number of records marked as expired
        """
        expired_access = (
            self.db_session.query(FileAccess)
            .filter(
                FileAccess.expires_at < datetime.utcnow(),
                FileAccess.is_expired == False,
                FileAccess.is_revoked == False,
            )
            .limit(batch_size)
            .all()
        )

        for access in expired_access:
            access.is_expired = True

        self.db_session.commit()
        return len(expired_access)

    # ============================================================================
    # FILE VERSION OPERATIONS
    # ============================================================================

    async def create_version(
        self,
        file_id: str,
        version_data: Dict[str, Any],
        created_by_user_id: str,
    ) -> FileVersion:
        """
        Create new file version.

        Args:
            file_id: File identifier (file_id)
            version_data: Version details
            created_by_user_id: User creating version

        Returns:
            Created FileVersion
        """
        # Get current version number
        current_version = await self.get_current_version(file_id)
        version_number = (current_version.version_number + 1) if current_version else 1

        # Mark all previous versions as not current
        if current_version:
            current_version.is_current = False

        version = FileVersion(
            file_id=file_id,
            version_number=version_number,
            version_label=version_data.get("version_label"),
            storage_key=version_data["storage_key"],
            size_bytes=version_data["size_bytes"],
            checksum=version_data.get("checksum"),
            change_type=version_data.get("change_type", "upload"),
            change_description=version_data.get("change_description"),
            change_summary=version_data.get("change_summary", {}),
            created_by_user_id=created_by_user_id,
            is_current=True,
            version_metadata=version_data.get("metadata", {}),
        )

        self.db_session.add(version)
        self.db_session.commit()
        return version

    async def get_current_version(
        self,
        file_id: str,
    ) -> Optional[FileVersion]:
        """
        Get current version of file.

        Args:
            file_id: File identifier

        Returns:
            Current FileVersion
        """
        return (
            self.db_session.query(FileVersion)
            .filter(
                FileVersion.file_id == file_id,
                FileVersion.is_current == True,
                FileVersion.is_deleted == False,
            )
            .first()
        )

    async def get_version_history(
        self,
        file_id: str,
        include_deleted: bool = False,
    ) -> List[FileVersion]:
        """
        Get complete version history for file.

        Args:
            file_id: File identifier
            include_deleted: Include deleted versions

        Returns:
            List of versions ordered by version number
        """
        query = self.db_session.query(FileVersion).filter(
            FileVersion.file_id == file_id
        )

        if not include_deleted:
            query = query.filter(FileVersion.is_deleted == False)

        return query.order_by(desc(FileVersion.version_number)).all()

    async def get_version_by_number(
        self,
        file_id: str,
        version_number: int,
    ) -> Optional[FileVersion]:
        """
        Get specific version by number.

        Args:
            file_id: File identifier
            version_number: Version number

        Returns:
            FileVersion if found
        """
        return (
            self.db_session.query(FileVersion)
            .filter(
                FileVersion.file_id == file_id,
                FileVersion.version_number == version_number,
                FileVersion.is_deleted == False,
            )
            .first()
        )

    async def restore_version(
        self,
        file_id: str,
        version_number: int,
        restored_by_user_id: str,
    ) -> FileVersion:
        """
        Restore previous version as current.

        Args:
            file_id: File identifier
            version_number: Version number to restore
            restored_by_user_id: User performing restoration

        Returns:
            New current version (copy of restored version)
        """
        version_to_restore = await self.get_version_by_number(file_id, version_number)

        if not version_to_restore:
            raise ValueError(
                f"Version {version_number} not found for file {file_id}"
            )

        # Create new version based on restored version
        new_version = await self.create_version(
            file_id=file_id,
            version_data={
                "storage_key": version_to_restore.storage_key,
                "size_bytes": version_to_restore.size_bytes,
                "checksum": version_to_restore.checksum,
                "change_type": "restore",
                "change_description": f"Restored from version {version_number}",
                "change_summary": {
                    "restored_from_version": version_number,
                    "restored_at": datetime.utcnow().isoformat(),
                },
            },
            created_by_user_id=restored_by_user_id,
        )

        return new_version

    # ============================================================================
    # FILE ANALYTICS OPERATIONS
    # ============================================================================

    async def get_or_create_analytics(
        self,
        file_id: str,
    ) -> FileAnalytics:
        """
        Get or create analytics record for file.

        Args:
            file_id: File identifier (file_id)

        Returns:
            FileAnalytics record
        """
        analytics = (
            self.db_session.query(FileAnalytics)
            .filter(FileAnalytics.file_id == file_id)
            .first()
        )

        if not analytics:
            analytics = FileAnalytics(
                file_id=file_id,
                total_views=0,
                total_downloads=0,
                unique_viewers=0,
                unique_downloaders=0,
                popularity_score=0.0,
                trending_score=0.0,
                views_today=0,
                views_this_week=0,
                views_this_month=0,
                last_calculated_at=datetime.utcnow(),
            )
            self.db_session.add(analytics)
            self.db_session.commit()

        return analytics

    async def increment_view_count(
        self,
        file_id: str,
        viewer_user_id: Optional[str] = None,
    ) -> FileAnalytics:
        """
        Increment view count for file.

        Args:
            file_id: File identifier
            viewer_user_id: User viewing file

        Returns:
            Updated FileAnalytics
        """
        analytics = await self.get_or_create_analytics(file_id)

        analytics.total_views += 1
        analytics.views_today += 1
        analytics.views_this_week += 1
        analytics.views_this_month += 1
        analytics.last_viewed_at = datetime.utcnow()

        if not analytics.first_viewed_at:
            analytics.first_viewed_at = datetime.utcnow()

        # Update unique viewers count (simplified - should use actual tracking)
        if viewer_user_id:
            analytics.unique_viewers = len(
                set(
                    [
                        log.accessed_by_user_id
                        for log in self.db_session.query(FileAccessLog)
                        .filter(
                            FileAccessLog.file_id == file_id,
                            FileAccessLog.access_type == "view",
                        )
                        .distinct(FileAccessLog.accessed_by_user_id)
                        .all()
                    ]
                )
            )

        self.db_session.commit()
        return analytics

    async def increment_download_count(
        self,
        file_id: str,
        downloader_user_id: Optional[str] = None,
    ) -> FileAnalytics:
        """
        Increment download count for file.

        Args:
            file_id: File identifier
            downloader_user_id: User downloading file

        Returns:
            Updated FileAnalytics
        """
        analytics = await self.get_or_create_analytics(file_id)

        analytics.total_downloads += 1
        analytics.last_downloaded_at = datetime.utcnow()

        # Update unique downloaders count
        if downloader_user_id:
            analytics.unique_downloaders = len(
                set(
                    [
                        log.accessed_by_user_id
                        for log in self.db_session.query(FileAccessLog)
                        .filter(
                            FileAccessLog.file_id == file_id,
                            FileAccessLog.access_type == "download",
                        )
                        .distinct(FileAccessLog.accessed_by_user_id)
                        .all()
                    ]
                )
            )

        self.db_session.commit()
        return analytics

    async def calculate_popularity_scores(
        self,
        batch_size: int = 100,
    ) -> int:
        """
        Calculate popularity and trending scores for files.

        Args:
            batch_size: Number of files to process

        Returns:
            Number of files updated
        """
        # Get analytics records to update
        analytics_list = (
            self.db_session.query(FileAnalytics)
            .order_by(asc(FileAnalytics.last_calculated_at))
            .limit(batch_size)
            .all()
        )

        for analytics in analytics_list:
            # Popularity score (weighted combination of views and downloads)
            view_weight = 1.0
            download_weight = 3.0
            analytics.popularity_score = (
                analytics.total_views * view_weight
                + analytics.total_downloads * download_weight
            )

            # Trending score (recent activity weighted more heavily)
            recent_weight = 5.0
            analytics.trending_score = (
                analytics.views_today * recent_weight
                + analytics.views_this_week * 2.0
                + analytics.views_this_month * 1.0
            )

            analytics.last_calculated_at = datetime.utcnow()

        self.db_session.commit()
        return len(analytics_list)

    async def reset_period_counters(
        self,
        period: str,
    ) -> int:
        """
        Reset period counters (daily, weekly, monthly).

        Args:
            period: Period to reset ('daily', 'weekly', 'monthly')

        Returns:
            Number of records updated
        """
        query = self.db_session.query(FileAnalytics)

        if period == "daily":
            query.update({"views_today": 0})
        elif period == "weekly":
            query.update({"views_this_week": 0})
        elif period == "monthly":
            query.update({"views_this_month": 0})

        count = query.count()
        self.db_session.commit()
        return count

    # ============================================================================
    # FILE ACCESS LOG OPERATIONS
    # ============================================================================

    async def log_file_access(
        self,
        file_id: str,
        access_data: Dict[str, Any],
    ) -> FileAccessLog:
        """
        Log file access event.

        Args:
            file_id: File identifier (file_id)
            access_data: Access event details

        Returns:
            Created FileAccessLog
        """
        access_log = FileAccessLog(
            file_id=file_id,
            storage_key=access_data["storage_key"],
            accessed_by_user_id=access_data.get("accessed_by_user_id"),
            access_type=access_data["access_type"],
            access_method=access_data.get("access_method", "web"),
            ip_address=access_data.get("ip_address"),
            user_agent=access_data.get("user_agent"),
            referrer=access_data.get("referrer"),
            device_type=access_data.get("device_type"),
            browser=access_data.get("browser"),
            operating_system=access_data.get("operating_system"),
            country_code=access_data.get("country_code"),
            country_name=access_data.get("country_name"),
            city=access_data.get("city"),
            region=access_data.get("region"),
            accessed_at=datetime.utcnow(),
            session_duration_seconds=access_data.get("session_duration_seconds"),
            success=access_data.get("success", True),
            status_code=access_data.get("status_code"),
            error_message=access_data.get("error_message"),
            response_time_ms=access_data.get("response_time_ms"),
            bytes_transferred=access_data.get("bytes_transferred"),
            access_context=access_data.get("context", {}),
        )

        self.db_session.add(access_log)
        self.db_session.commit()

        # Update analytics
        if access_data["access_type"] == "view":
            await self.increment_view_count(
                file_id, access_data.get("accessed_by_user_id")
            )
        elif access_data["access_type"] == "download":
            await self.increment_download_count(
                file_id, access_data.get("accessed_by_user_id")
            )

        return access_log

    async def get_access_logs(
        self,
        file_id: str,
        access_type: Optional[str] = None,
        start_date: Optional[datetime] = None,
        end_date: Optional[datetime] = None,
        limit: int = 100,
    ) -> List[FileAccessLog]:
        """
        Get access logs for file.

        Args:
            file_id: File identifier
            access_type: Filter by access type
            start_date: Start date filter
            end_date: End date filter
            limit: Maximum results

        Returns:
            List of access logs
        """
        query = self.db_session.query(FileAccessLog).filter(
            FileAccessLog.file_id == file_id
        )

        if access_type:
            query = query.filter(FileAccessLog.access_type == access_type)

        if start_date:
            query = query.filter(FileAccessLog.accessed_at >= start_date)

        if end_date:
            query = query.filter(FileAccessLog.accessed_at <= end_date)

        return query.order_by(desc(FileAccessLog.accessed_at)).limit(limit).all()

    async def get_user_access_history(
        self,
        user_id: str,
        access_type: Optional[str] = None,
        limit: int = 50,
    ) -> List[FileAccessLog]:
        """
        Get user's file access history.

        Args:
            user_id: User identifier
            access_type: Filter by access type
            limit: Maximum results

        Returns:
            List of access logs
        """
        query = self.db_session.query(FileAccessLog).filter(
            FileAccessLog.accessed_by_user_id == user_id
        )

        if access_type:
            query = query.filter(FileAccessLog.access_type == access_type)

        return query.order_by(desc(FileAccessLog.accessed_at)).limit(limit).all()

    async def get_access_analytics(
        self,
        file_id: str,
        start_date: Optional[datetime] = None,
        end_date: Optional[datetime] = None,
    ) -> Dict[str, Any]:
        """
        Get access analytics for file.

        Args:
            file_id: File identifier
            start_date: Start date filter
            end_date: End date filter

        Returns:
            Access analytics summary
        """
        query = self.db_session.query(
            func.count(FileAccessLog.id).label("total_accesses"),
            func.count(
                case([(FileAccessLog.access_type == "view", 1)])
            ).label("views"),
            func.count(
                case([(FileAccessLog.access_type == "download", 1)])
            ).label("downloads"),
            func.count(
                case([(FileAccessLog.success == True, 1)])
            ).label("successful"),
            func.count(
                case([(FileAccessLog.success == False, 1)])
            ).label("failed"),
            func.avg(FileAccessLog.response_time_ms).label("avg_response_time"),
        ).filter(FileAccessLog.file_id == file_id)

        if start_date:
            query = query.filter(FileAccessLog.accessed_at >= start_date)

        if end_date:
            query = query.filter(FileAccessLog.accessed_at <= end_date)

        result = query.first()

        return {
            "total_accesses": result.total_accesses or 0,
            "views": result.views or 0,
            "downloads": result.downloads or 0,
            "successful_accesses": result.successful or 0,
            "failed_accesses": result.failed or 0,
            "average_response_time_ms": round(result.avg_response_time or 0, 2),
        }

    # ============================================================================
    # FILE FAVORITE OPERATIONS
    # ============================================================================

    async def add_favorite(
        self,
        file_id: str,
        user_id: str,
        note: Optional[str] = None,
        folder_name: Optional[str] = None,
        tags: Optional[List[str]] = None,
    ) -> FileFavorite:
        """
        Add file to user favorites.

        Args:
            file_id: File identifier (file_id)
            user_id: User identifier
            note: Personal note
            folder_name: Favorite folder
            tags: Personal tags

        Returns:
            Created FileFavorite

        Raises:
            ValueError: If already favorited
        """
        # Check if already favorited
        existing = await self.get_favorite(file_id, user_id)
        if existing:
            raise ValueError("File already in favorites")

        favorite = FileFavorite(
            file_id=file_id,
            user_id=user_id,
            favorited_at=datetime.utcnow(),
            note=note,
            folder_name=folder_name,
            tags=tags or [],
            access_count=0,
        )

        self.db_session.add(favorite)
        self.db_session.commit()
        return favorite

    async def get_favorite(
        self,
        file_id: str,
        user_id: str,
    ) -> Optional[FileFavorite]:
        """
        Get favorite record.

        Args:
            file_id: File identifier
            user_id: User identifier

        Returns:
            FileFavorite if exists
        """
        return (
            self.db_session.query(FileFavorite)
            .filter(
                FileFavorite.file_id == file_id,
                FileFavorite.user_id == user_id,
            )
            .first()
        )

    async def get_user_favorites(
        self,
        user_id: str,
        folder_name: Optional[str] = None,
        tags: Optional[List[str]] = None,
        limit: int = 100,
    ) -> List[FileFavorite]:
        """
        Get user's favorite files.

        Args:
            user_id: User identifier
            folder_name: Filter by folder
            tags: Filter by tags
            limit: Maximum results

        Returns:
            List of favorite files
        """
        query = self.db_session.query(FileFavorite).filter(
            FileFavorite.user_id == user_id
        )

        if folder_name:
            query = query.filter(FileFavorite.folder_name == folder_name)

        if tags:
            for tag in tags:
                query = query.filter(FileFavorite.tags.contains([tag]))

        return query.order_by(desc(FileFavorite.favorited_at)).limit(limit).all()

    async def remove_favorite(
        self,
        file_id: str,
        user_id: str,
    ) -> bool:
        """
        Remove file from favorites.

        Args:
            file_id: File identifier
            user_id: User identifier

        Returns:
            True if removed
        """
        favorite = await self.get_favorite(file_id, user_id)

        if favorite:
            self.db_session.delete(favorite)
            self.db_session.commit()
            return True

        return False

    async def track_favorite_access(
        self,
        file_id: str,
        user_id: str,
    ) -> FileFavorite:
        """
        Track access via favorite.

        Args:
            file_id: File identifier
            user_id: User identifier

        Returns:
            Updated FileFavorite
        """
        favorite = await self.get_favorite(file_id, user_id)

        if favorite:
            favorite.last_accessed_at = datetime.utcnow()
            favorite.access_count += 1
            self.db_session.commit()

        return favorite

# --- File: C:\Hostel-Main\app\repositories\file_management\file_upload_repository.py ---
"""
File Upload Repository

Core file upload operations with session management, validation,
multipart uploads, and quota enforcement.
"""

from datetime import datetime, timedelta
from typing import List, Optional, Dict, Any, Tuple
from sqlalchemy import and_, or_, func, case, desc, asc
from sqlalchemy.orm import Session, joinedload, selectinload

from app.repositories.base.base_repository import BaseRepository
from app.repositories.base.query_builder import QueryBuilder
from app.repositories.base.specifications import Specification
from app.repositories.base.pagination import PaginationManager, PaginatedResult
from app.models.file_management.file_upload import (
    FileUpload,
    UploadSession,
    FileValidation,
    UploadProgress,
    FileQuota,
    MultipartUpload,
    MultipartUploadPart,
)
from app.models.user.user import User
from app.models.hostel.hostel import Hostel


class FileUploadRepository(BaseRepository[FileUpload]):
    """
    Repository for file upload operations with advanced querying,
    validation, and quota management.
    """

    def __init__(self, db_session: Session):
        super().__init__(FileUpload, db_session)

    # ============================================================================
    # CORE FILE OPERATIONS
    # ============================================================================

    async def create_file_upload(
        self,
        file_data: Dict[str, Any],
        uploaded_by_user_id: str,
        audit_context: Optional[Dict[str, Any]] = None,
    ) -> FileUpload:
        """
        Create new file upload with validation and quota checking.

        Args:
            file_data: File metadata and storage information
            uploaded_by_user_id: User ID who uploaded the file
            audit_context: Audit trail context

        Returns:
            Created FileUpload instance

        Raises:
            QuotaExceededException: If quota is exceeded
            ValidationException: If file data is invalid
        """
        # Check quota before upload
        await self._check_quota(
            owner_type=file_data.get("owner_type", "user"),
            owner_id=uploaded_by_user_id,
            file_size=file_data.get("size_bytes", 0),
        )

        # Create file upload
        file_upload = FileUpload(
            file_id=file_data["file_id"],
            storage_key=file_data["storage_key"],
            filename=file_data["filename"],
            content_type=file_data["content_type"],
            size_bytes=file_data["size_bytes"],
            extension=file_data.get("extension"),
            checksum=file_data.get("checksum"),
            etag=file_data.get("etag"),
            uploaded_by_user_id=uploaded_by_user_id,
            hostel_id=file_data.get("hostel_id"),
            student_id=file_data.get("student_id"),
            folder=file_data.get("folder"),
            category=file_data.get("category"),
            tags=file_data.get("tags", []),
            is_public=file_data.get("is_public", False),
            public_url=file_data.get("public_url"),
            url=file_data["url"],
            metadata=file_data.get("metadata", {}),
            processing_status="pending",
            virus_scan_status="pending",
        )

        created_file = await self.create(file_upload, audit_context)

        # Update quota usage
        await self._update_quota_usage(
            owner_type=file_data.get("owner_type", "user"),
            owner_id=uploaded_by_user_id,
            size_delta=file_data["size_bytes"],
            file_count_delta=1,
        )

        return created_file

    async def find_by_file_id(
        self,
        file_id: str,
        include_deleted: bool = False,
        load_relationships: bool = False,
    ) -> Optional[FileUpload]:
        """
        Find file by unique file ID.

        Args:
            file_id: Unique file identifier
            include_deleted: Whether to include soft-deleted files
            load_relationships: Whether to eager load relationships

        Returns:
            FileUpload if found, None otherwise
        """
        query = self.db_session.query(FileUpload).filter(
            FileUpload.file_id == file_id
        )

        if not include_deleted:
            query = query.filter(FileUpload.deleted_at.is_(None))

        if load_relationships:
            query = query.options(
                joinedload(FileUpload.uploaded_by),
                joinedload(FileUpload.hostel),
                joinedload(FileUpload.student),
                selectinload(FileUpload.validations),
            )

        return query.first()

    async def find_by_storage_key(
        self, storage_key: str
    ) -> Optional[FileUpload]:
        """Find file by storage key."""
        return self.db_session.query(FileUpload).filter(
            FileUpload.storage_key == storage_key,
            FileUpload.deleted_at.is_(None),
        ).first()

    async def find_by_checksum(
        self,
        checksum: str,
        uploaded_by_user_id: Optional[str] = None,
    ) -> List[FileUpload]:
        """
        Find files by checksum for duplicate detection.

        Args:
            checksum: File checksum
            uploaded_by_user_id: Optional user ID filter

        Returns:
            List of files with matching checksum
        """
        query = self.db_session.query(FileUpload).filter(
            FileUpload.checksum == checksum,
            FileUpload.deleted_at.is_(None),
        )

        if uploaded_by_user_id:
            query = query.filter(
                FileUpload.uploaded_by_user_id == uploaded_by_user_id
            )

        return query.all()

    async def search_files(
        self,
        criteria: Dict[str, Any],
        pagination: Optional[Dict[str, Any]] = None,
        sort_by: Optional[str] = "created_at",
        sort_order: str = "desc",
    ) -> PaginatedResult[FileUpload]:
        """
        Search files with flexible criteria and pagination.

        Args:
            criteria: Search criteria dictionary
            pagination: Pagination parameters
            sort_by: Field to sort by
            sort_order: Sort order ('asc' or 'desc')

        Returns:
            Paginated search results

        Criteria Options:
            - uploaded_by_user_id: Filter by uploader
            - hostel_id: Filter by hostel
            - student_id: Filter by student
            - category: Filter by category
            - content_type: Filter by MIME type
            - folder: Filter by folder path
            - tags: Filter by tags (list)
            - is_public: Filter by public status
            - processing_status: Filter by processing status
            - virus_scan_status: Filter by scan status
            - min_size_bytes: Minimum file size
            - max_size_bytes: Maximum file size
            - filename_search: Search in filename
            - created_after: Files created after date
            - created_before: Files created before date
        """
        query = QueryBuilder(FileUpload, self.db_session)

        # Apply filters
        if "uploaded_by_user_id" in criteria:
            query = query.where(
                FileUpload.uploaded_by_user_id == criteria["uploaded_by_user_id"]
            )

        if "hostel_id" in criteria:
            query = query.where(FileUpload.hostel_id == criteria["hostel_id"])

        if "student_id" in criteria:
            query = query.where(FileUpload.student_id == criteria["student_id"])

        if "category" in criteria:
            query = query.where(FileUpload.category == criteria["category"])

        if "content_type" in criteria:
            query = query.where(
                FileUpload.content_type == criteria["content_type"]
            )

        if "folder" in criteria:
            query = query.where(FileUpload.folder.like(f"{criteria['folder']}%"))

        if "tags" in criteria and criteria["tags"]:
            # JSON array contains any of the specified tags
            for tag in criteria["tags"]:
                query = query.where(
                    FileUpload.tags.contains([tag])
                )

        if "is_public" in criteria:
            query = query.where(FileUpload.is_public == criteria["is_public"])

        if "processing_status" in criteria:
            query = query.where(
                FileUpload.processing_status == criteria["processing_status"]
            )

        if "virus_scan_status" in criteria:
            query = query.where(
                FileUpload.virus_scan_status == criteria["virus_scan_status"]
            )

        if "min_size_bytes" in criteria:
            query = query.where(
                FileUpload.size_bytes >= criteria["min_size_bytes"]
            )

        if "max_size_bytes" in criteria:
            query = query.where(
                FileUpload.size_bytes <= criteria["max_size_bytes"]
            )

        if "filename_search" in criteria:
            search_term = f"%{criteria['filename_search']}%"
            query = query.where(FileUpload.filename.ilike(search_term))

        if "created_after" in criteria:
            query = query.where(
                FileUpload.created_at >= criteria["created_after"]
            )

        if "created_before" in criteria:
            query = query.where(
                FileUpload.created_at <= criteria["created_before"]
            )

        # Exclude soft-deleted
        query = query.where(FileUpload.deleted_at.is_(None))

        # Apply sorting
        sort_field = getattr(FileUpload, sort_by, FileUpload.created_at)
        if sort_order == "desc":
            query = query.order_by(desc(sort_field))
        else:
            query = query.order_by(asc(sort_field))

        # Apply pagination
        return await PaginationManager.paginate(
            query.build(),
            pagination or {"page": 1, "page_size": 50}
        )

    async def update_processing_status(
        self,
        file_id: str,
        status: str,
        error_message: Optional[str] = None,
        audit_context: Optional[Dict[str, Any]] = None,
    ) -> FileUpload:
        """
        Update file processing status.

        Args:
            file_id: File identifier
            status: New processing status
            error_message: Error message if failed
            audit_context: Audit context

        Returns:
            Updated FileUpload
        """
        file_upload = await self.find_by_file_id(file_id)
        if not file_upload:
            raise ValueError(f"File not found: {file_id}")

        update_data = {
            "processing_status": status,
            "is_processed": status == "completed",
        }

        if status == "processing" and not file_upload.processing_started_at:
            update_data["processing_started_at"] = datetime.utcnow()
        elif status in ["completed", "failed"]:
            update_data["processing_completed_at"] = datetime.utcnow()

        if error_message:
            update_data["processing_error"] = error_message

        return await self.update(
            file_upload.id,
            update_data,
            audit_context=audit_context
        )

    async def update_virus_scan_status(
        self,
        file_id: str,
        status: str,
        scan_result: Optional[Dict[str, Any]] = None,
        audit_context: Optional[Dict[str, Any]] = None,
    ) -> FileUpload:
        """
        Update virus scan status and results.

        Args:
            file_id: File identifier
            status: Scan status
            scan_result: Detailed scan results
            audit_context: Audit context

        Returns:
            Updated FileUpload
        """
        file_upload = await self.find_by_file_id(file_id)
        if not file_upload:
            raise ValueError(f"File not found: {file_id}")

        update_data = {
            "virus_scan_status": status,
            "virus_scan_timestamp": datetime.utcnow(),
        }

        if scan_result:
            update_data["virus_scan_result"] = scan_result

        return await self.update(
            file_upload.id,
            update_data,
            audit_context=audit_context
        )

    async def track_file_access(
        self,
        file_id: str,
        accessed_by_user_id: str,
        audit_context: Optional[Dict[str, Any]] = None,
    ) -> FileUpload:
        """
        Track file access and update access metrics.

        Args:
            file_id: File identifier
            accessed_by_user_id: User accessing the file
            audit_context: Audit context

        Returns:
            Updated FileUpload
        """
        file_upload = await self.find_by_file_id(file_id)
        if not file_upload:
            raise ValueError(f"File not found: {file_id}")

        update_data = {
            "access_count": file_upload.access_count + 1,
            "last_accessed_at": datetime.utcnow(),
            "last_accessed_by_user_id": accessed_by_user_id,
        }

        return await self.update(
            file_upload.id,
            update_data,
            audit_context=audit_context
        )

    # ============================================================================
    # UPLOAD SESSION OPERATIONS
    # ============================================================================

    async def create_upload_session(
        self,
        session_data: Dict[str, Any],
        uploaded_by_user_id: str,
        audit_context: Optional[Dict[str, Any]] = None,
    ) -> UploadSession:
        """
        Create upload session for direct or multipart upload.

        Args:
            session_data: Session configuration
            uploaded_by_user_id: User initiating upload
            audit_context: Audit context

        Returns:
            Created UploadSession
        """
        # Check quota before creating session
        await self._check_quota(
            owner_type=session_data.get("owner_type", "user"),
            owner_id=uploaded_by_user_id,
            file_size=session_data["expected_size_bytes"],
        )

        # Reserve quota
        await self._update_quota_usage(
            owner_type=session_data.get("owner_type", "user"),
            owner_id=uploaded_by_user_id,
            reserved_delta=session_data["expected_size_bytes"],
        )

        session = UploadSession(
            upload_id=session_data["upload_id"],
            session_type=session_data["session_type"],
            storage_key=session_data["storage_key"],
            filename=session_data["filename"],
            content_type=session_data["content_type"],
            expected_size_bytes=session_data["expected_size_bytes"],
            uploaded_by_user_id=uploaded_by_user_id,
            status="initialized",
            upload_url=session_data.get("upload_url"),
            upload_method=session_data.get("upload_method", "PUT"),
            upload_headers=session_data.get("upload_headers"),
            expires_at=session_data["expires_at"],
            session_metadata=session_data.get("metadata", {}),
        )

        return self.db_session.add(session)
        self.db_session.commit()
        return session

    async def find_upload_session(
        self,
        upload_id: str,
        include_expired: bool = False,
    ) -> Optional[UploadSession]:
        """
        Find upload session by ID.

        Args:
            upload_id: Upload session identifier
            include_expired: Whether to include expired sessions

        Returns:
            UploadSession if found
        """
        query = self.db_session.query(UploadSession).filter(
            UploadSession.upload_id == upload_id
        )

        if not include_expired:
            query = query.filter(UploadSession.expires_at > datetime.utcnow())

        return query.first()

    async def update_upload_session_status(
        self,
        upload_id: str,
        status: str,
        error_message: Optional[str] = None,
        actual_size_bytes: Optional[int] = None,
        checksum: Optional[str] = None,
        etag: Optional[str] = None,
    ) -> UploadSession:
        """
        Update upload session status.

        Args:
            upload_id: Session identifier
            status: New status
            error_message: Error if failed
            actual_size_bytes: Actual uploaded size
            checksum: File checksum
            etag: Storage provider ETag

        Returns:
            Updated UploadSession
        """
        session = await self.find_upload_session(upload_id, include_expired=True)
        if not session:
            raise ValueError(f"Upload session not found: {upload_id}")

        session.status = status

        if status == "uploading" and not session.upload_started_at:
            session.upload_started_at = datetime.utcnow()
        elif status in ["completed", "failed", "expired"]:
            session.upload_completed_at = datetime.utcnow()

        if error_message:
            session.error_message = error_message
            session.retry_count += 1

        if actual_size_bytes is not None:
            session.actual_size_bytes = actual_size_bytes

        if checksum:
            session.checksum = checksum

        if etag:
            session.etag = etag

        self.db_session.commit()
        return session

    async def cleanup_expired_sessions(
        self, batch_size: int = 100
    ) -> int:
        """
        Clean up expired upload sessions and release reserved quota.

        Args:
            batch_size: Number of sessions to clean per batch

        Returns:
            Number of sessions cleaned up
        """
        expired_sessions = (
            self.db_session.query(UploadSession)
            .filter(
                UploadSession.expires_at < datetime.utcnow(),
                UploadSession.status.in_(["initialized", "uploading"]),
            )
            .limit(batch_size)
            .all()
        )

        cleaned_count = 0
        for session in expired_sessions:
            # Release reserved quota
            await self._update_quota_usage(
                owner_type="user",  # Should be stored in session metadata
                owner_id=session.uploaded_by_user_id,
                reserved_delta=-session.expected_size_bytes,
            )

            session.status = "expired"
            cleaned_count += 1

        self.db_session.commit()
        return cleaned_count

    # ============================================================================
    # FILE VALIDATION OPERATIONS
    # ============================================================================

    async def create_validation_result(
        self,
        file_id: str,
        validation_data: Dict[str, Any],
    ) -> FileValidation:
        """
        Store file validation results.

        Args:
            file_id: File identifier
            validation_data: Validation results

        Returns:
            Created FileValidation record
        """
        file_upload = await self.find_by_file_id(file_id)
        if not file_upload:
            raise ValueError(f"File not found: {file_id}")

        validation = FileValidation(
            file_id=file_upload.id,
            validation_type=validation_data["validation_type"],
            is_valid=validation_data["is_valid"],
            validation_score=validation_data.get("validation_score"),
            checks_passed=validation_data.get("checks_passed", []),
            checks_failed=validation_data.get("checks_failed", []),
            warnings=validation_data.get("warnings", []),
            reason=validation_data.get("reason"),
            error_details=validation_data.get("error_details"),
            extracted_metadata=validation_data.get("extracted_metadata"),
            detected_type=validation_data.get("detected_type"),
            confidence_level=validation_data.get("confidence_level"),
            validated_at=datetime.utcnow(),
            validation_duration_ms=validation_data.get("validation_duration_ms"),
            validator_name=validation_data.get("validator_name"),
            validator_version=validation_data.get("validator_version"),
        )

        self.db_session.add(validation)
        self.db_session.commit()
        return validation

    async def get_validation_results(
        self,
        file_id: str,
        validation_type: Optional[str] = None,
    ) -> List[FileValidation]:
        """
        Get validation results for a file.

        Args:
            file_id: File identifier
            validation_type: Optional filter by validation type

        Returns:
            List of validation results
        """
        file_upload = await self.find_by_file_id(file_id)
        if not file_upload:
            return []

        query = self.db_session.query(FileValidation).filter(
            FileValidation.file_id == file_upload.id
        )

        if validation_type:
            query = query.filter(FileValidation.validation_type == validation_type)

        return query.order_by(desc(FileValidation.validated_at)).all()

    # ============================================================================
    # MULTIPART UPLOAD OPERATIONS
    # ============================================================================

    async def create_multipart_upload(
        self,
        session_id: str,
        multipart_data: Dict[str, Any],
    ) -> MultipartUpload:
        """
        Create multipart upload tracking.

        Args:
            session_id: Upload session ID
            multipart_data: Multipart upload configuration

        Returns:
            Created MultipartUpload
        """
        multipart = MultipartUpload(
            session_id=session_id,
            multipart_upload_id=multipart_data["multipart_upload_id"],
            total_size_bytes=multipart_data["total_size_bytes"],
            part_size_bytes=multipart_data["part_size_bytes"],
            total_parts=multipart_data["total_parts"],
            status="in_progress",
        )

        self.db_session.add(multipart)
        self.db_session.commit()
        return multipart

    async def create_multipart_part(
        self,
        multipart_upload_id: str,
        part_data: Dict[str, Any],
    ) -> MultipartUploadPart:
        """
        Create multipart upload part.

        Args:
            multipart_upload_id: Multipart upload ID
            part_data: Part configuration

        Returns:
            Created MultipartUploadPart
        """
        multipart = (
            self.db_session.query(MultipartUpload)
            .filter(MultipartUpload.id == multipart_upload_id)
            .first()
        )

        if not multipart:
            raise ValueError(f"Multipart upload not found: {multipart_upload_id}")

        part = MultipartUploadPart(
            multipart_upload_id=multipart_upload_id,
            part_number=part_data["part_number"],
            size_bytes=part_data["size_bytes"],
            upload_url=part_data["upload_url"],
            url_expires_at=part_data["url_expires_at"],
            status="pending",
        )

        self.db_session.add(part)
        self.db_session.commit()
        return part

    async def update_multipart_part_status(
        self,
        multipart_upload_id: str,
        part_number: int,
        status: str,
        etag: Optional[str] = None,
        checksum: Optional[str] = None,
        error_message: Optional[str] = None,
    ) -> MultipartUploadPart:
        """
        Update multipart part upload status.

        Args:
            multipart_upload_id: Multipart upload ID
            part_number: Part number
            status: New status
            etag: Part ETag from storage
            checksum: Part checksum
            error_message: Error if failed

        Returns:
            Updated MultipartUploadPart
        """
        part = (
            self.db_session.query(MultipartUploadPart)
            .filter(
                MultipartUploadPart.multipart_upload_id == multipart_upload_id,
                MultipartUploadPart.part_number == part_number,
            )
            .first()
        )

        if not part:
            raise ValueError(
                f"Part {part_number} not found for multipart upload {multipart_upload_id}"
            )

        part.status = status

        if status == "completed":
            part.uploaded_at = datetime.utcnow()
            if etag:
                part.etag = etag
            if checksum:
                part.checksum = checksum

            # Update multipart upload progress
            multipart = part.multipart_upload
            multipart.uploaded_parts += 1
            multipart.uploaded_bytes += part.size_bytes

            if multipart.uploaded_parts >= multipart.total_parts:
                multipart.status = "assembling"
                multipart.assembly_started_at = datetime.utcnow()

        elif status == "failed":
            part.error_message = error_message
            part.retry_count += 1

        self.db_session.commit()
        return part

    async def complete_multipart_upload(
        self,
        multipart_upload_id: str,
        file_id: Optional[str] = None,
    ) -> MultipartUpload:
        """
        Mark multipart upload as completed.

        Args:
            multipart_upload_id: Multipart upload ID
            file_id: Final file ID after assembly

        Returns:
            Updated MultipartUpload
        """
        multipart = (
            self.db_session.query(MultipartUpload)
            .filter(MultipartUpload.id == multipart_upload_id)
            .first()
        )

        if not multipart:
            raise ValueError(f"Multipart upload not found: {multipart_upload_id}")

        multipart.status = "completed"
        multipart.assembly_completed_at = datetime.utcnow()

        # Update session
        session = multipart.session
        session.status = "completed"
        session.upload_completed_at = datetime.utcnow()
        session.actual_size_bytes = multipart.total_size_bytes

        if file_id:
            session.file_id = file_id

        # Release reserved quota and add actual usage
        await self._update_quota_usage(
            owner_type="user",
            owner_id=session.uploaded_by_user_id,
            reserved_delta=-session.expected_size_bytes,
            size_delta=multipart.total_size_bytes,
        )

        self.db_session.commit()
        return multipart

    # ============================================================================
    # QUOTA MANAGEMENT OPERATIONS
    # ============================================================================

    async def get_quota(
        self,
        owner_type: str,
        owner_id: str,
    ) -> Optional[FileQuota]:
        """
        Get quota for owner.

        Args:
            owner_type: Owner type (user, hostel, tenant)
            owner_id: Owner identifier

        Returns:
            FileQuota if exists
        """
        return (
            self.db_session.query(FileQuota)
            .filter(
                FileQuota.owner_type == owner_type,
                FileQuota.owner_id == owner_id,
            )
            .first()
        )

    async def create_or_update_quota(
        self,
        owner_type: str,
        owner_id: str,
        quota_bytes: int,
        max_files: Optional[int] = None,
        max_file_size_bytes: Optional[int] = None,
    ) -> FileQuota:
        """
        Create or update quota settings.

        Args:
            owner_type: Owner type
            owner_id: Owner identifier
            quota_bytes: Total quota in bytes
            max_files: Maximum number of files
            max_file_size_bytes: Maximum size per file

        Returns:
            FileQuota record
        """
        quota = await self.get_quota(owner_type, owner_id)

        if quota:
            quota.quota_bytes = quota_bytes
            if max_files is not None:
                quota.max_files = max_files
            if max_file_size_bytes is not None:
                quota.max_file_size_bytes = max_file_size_bytes
        else:
            quota = FileQuota(
                owner_type=owner_type,
                owner_id=owner_id,
                quota_bytes=quota_bytes,
                max_files=max_files,
                max_file_size_bytes=max_file_size_bytes,
                used_bytes=0,
                reserved_bytes=0,
                current_file_count=0,
                is_enforced=True,
            )
            self.db_session.add(quota)

        self.db_session.commit()
        return quota

    async def _check_quota(
        self,
        owner_type: str,
        owner_id: str,
        file_size: int,
    ) -> None:
        """
        Check if upload would exceed quota.

        Args:
            owner_type: Owner type
            owner_id: Owner identifier
            file_size: File size to check

        Raises:
            QuotaExceededException: If quota would be exceeded
        """
        quota = await self.get_quota(owner_type, owner_id)

        if not quota or not quota.is_enforced:
            return

        # Check total size quota
        total_used = quota.used_bytes + quota.reserved_bytes + file_size
        if total_used > quota.quota_bytes:
            raise QuotaExceededException(
                f"Quota exceeded: {total_used} bytes would exceed limit of {quota.quota_bytes} bytes"
            )

        # Check file count quota
        if quota.max_files and quota.current_file_count >= quota.max_files:
            raise QuotaExceededException(
                f"File count limit reached: {quota.max_files} files"
            )

        # Check per-file size quota
        if quota.max_file_size_bytes and file_size > quota.max_file_size_bytes:
            raise QuotaExceededException(
                f"File too large: {file_size} bytes exceeds limit of {quota.max_file_size_bytes} bytes"
            )

    async def _update_quota_usage(
        self,
        owner_type: str,
        owner_id: str,
        size_delta: int = 0,
        reserved_delta: int = 0,
        file_count_delta: int = 0,
    ) -> None:
        """
        Update quota usage.

        Args:
            owner_type: Owner type
            owner_id: Owner identifier
            size_delta: Change in used bytes
            reserved_delta: Change in reserved bytes
            file_count_delta: Change in file count
        """
        quota = await self.get_quota(owner_type, owner_id)

        if not quota:
            # Create default quota if doesn't exist
            quota = await self.create_or_update_quota(
                owner_type=owner_type,
                owner_id=owner_id,
                quota_bytes=10 * 1024 * 1024 * 1024,  # 10 GB default
            )

        quota.used_bytes += size_delta
        quota.reserved_bytes += reserved_delta
        quota.current_file_count += file_count_delta
        quota.last_usage_update_at = datetime.utcnow()

        # Check if quota exceeded
        total_used = quota.used_bytes + quota.reserved_bytes
        quota.is_exceeded = total_used > quota.quota_bytes

        # Send alert if needed
        usage_percentage = (total_used / quota.quota_bytes * 100) if quota.quota_bytes > 0 else 0
        if (
            usage_percentage >= quota.alert_threshold_percentage
            and not quota.alert_sent_at
        ):
            quota.alert_sent_at = datetime.utcnow()
            # TODO: Trigger quota alert notification

        self.db_session.commit()

    async def get_quota_statistics(
        self,
        owner_type: str,
        owner_id: str,
    ) -> Dict[str, Any]:
        """
        Get detailed quota statistics.

        Args:
            owner_type: Owner type
            owner_id: Owner identifier

        Returns:
            Quota statistics dictionary
        """
        quota = await self.get_quota(owner_type, owner_id)

        if not quota:
            return {
                "quota_bytes": 0,
                "used_bytes": 0,
                "reserved_bytes": 0,
                "available_bytes": 0,
                "usage_percentage": 0,
                "file_count": 0,
                "is_exceeded": False,
            }

        total_used = quota.used_bytes + quota.reserved_bytes
        available = max(0, quota.quota_bytes - total_used)
        usage_percentage = (total_used / quota.quota_bytes * 100) if quota.quota_bytes > 0 else 0

        return {
            "quota_bytes": quota.quota_bytes,
            "used_bytes": quota.used_bytes,
            "reserved_bytes": quota.reserved_bytes,
            "available_bytes": available,
            "usage_percentage": round(usage_percentage, 2),
            "file_count": quota.current_file_count,
            "max_files": quota.max_files,
            "max_file_size_bytes": quota.max_file_size_bytes,
            "is_exceeded": quota.is_exceeded,
            "alert_threshold_percentage": quota.alert_threshold_percentage,
        }

    # ============================================================================
    # ANALYTICS AND REPORTING
    # ============================================================================

    async def get_upload_statistics(
        self,
        start_date: Optional[datetime] = None,
        end_date: Optional[datetime] = None,
        uploaded_by_user_id: Optional[str] = None,
        hostel_id: Optional[str] = None,
    ) -> Dict[str, Any]:
        """
        Get upload statistics for specified period.

        Args:
            start_date: Start date for statistics
            end_date: End date for statistics
            uploaded_by_user_id: Filter by uploader
            hostel_id: Filter by hostel

        Returns:
            Statistics dictionary
        """
        query = self.db_session.query(
            func.count(FileUpload.id).label("total_uploads"),
            func.sum(FileUpload.size_bytes).label("total_size"),
            func.avg(FileUpload.size_bytes).label("average_size"),
            func.count(
                case([(FileUpload.processing_status == "completed", 1)])
            ).label("processed_count"),
            func.count(
                case([(FileUpload.virus_scan_status == "clean", 1)])
            ).label("clean_files"),
            func.count(
                case([(FileUpload.virus_scan_status == "infected", 1)])
            ).label("infected_files"),
        ).filter(FileUpload.deleted_at.is_(None))

        if start_date:
            query = query.filter(FileUpload.created_at >= start_date)

        if end_date:
            query = query.filter(FileUpload.created_at <= end_date)

        if uploaded_by_user_id:
            query = query.filter(
                FileUpload.uploaded_by_user_id == uploaded_by_user_id
            )

        if hostel_id:
            query = query.filter(FileUpload.hostel_id == hostel_id)

        result = query.first()

        return {
            "total_uploads": result.total_uploads or 0,
            "total_size_bytes": result.total_size or 0,
            "average_size_bytes": round(result.average_size or 0, 2),
            "processed_count": result.processed_count or 0,
            "clean_files": result.clean_files or 0,
            "infected_files": result.infected_files or 0,
        }

    async def get_storage_breakdown_by_category(
        self,
        uploaded_by_user_id: Optional[str] = None,
        hostel_id: Optional[str] = None,
    ) -> List[Dict[str, Any]]:
        """
        Get storage usage breakdown by category.

        Args:
            uploaded_by_user_id: Filter by uploader
            hostel_id: Filter by hostel

        Returns:
            List of category breakdowns
        """
        query = self.db_session.query(
            FileUpload.category,
            func.count(FileUpload.id).label("file_count"),
            func.sum(FileUpload.size_bytes).label("total_size"),
        ).filter(FileUpload.deleted_at.is_(None))

        if uploaded_by_user_id:
            query = query.filter(
                FileUpload.uploaded_by_user_id == uploaded_by_user_id
            )

        if hostel_id:
            query = query.filter(FileUpload.hostel_id == hostel_id)

        results = query.group_by(FileUpload.category).all()

        return [
            {
                "category": row.category or "uncategorized",
                "file_count": row.file_count,
                "total_size_bytes": row.total_size or 0,
            }
            for row in results
        ]

    async def find_large_files(
        self,
        min_size_bytes: int,
        limit: int = 100,
        uploaded_by_user_id: Optional[str] = None,
    ) -> List[FileUpload]:
        """
        Find files larger than specified size.

        Args:
            min_size_bytes: Minimum file size
            limit: Maximum results
            uploaded_by_user_id: Filter by uploader

        Returns:
            List of large files
        """
        query = (
            self.db_session.query(FileUpload)
            .filter(
                FileUpload.size_bytes >= min_size_bytes,
                FileUpload.deleted_at.is_(None),
            )
            .order_by(desc(FileUpload.size_bytes))
            .limit(limit)
        )

        if uploaded_by_user_id:
            query = query.filter(
                FileUpload.uploaded_by_user_id == uploaded_by_user_id
            )

        return query.all()

    async def find_unused_files(
        self,
        days_unused: int = 90,
        limit: int = 100,
    ) -> List[FileUpload]:
        """
        Find files not accessed for specified days.

        Args:
            days_unused: Days of inactivity
            limit: Maximum results

        Returns:
            List of unused files
        """
        threshold_date = datetime.utcnow() - timedelta(days=days_unused)

        return (
            self.db_session.query(FileUpload)
            .filter(
                FileUpload.deleted_at.is_(None),
                or_(
                    FileUpload.last_accessed_at.is_(None),
                    FileUpload.last_accessed_at < threshold_date,
                ),
                FileUpload.created_at < threshold_date,
            )
            .order_by(asc(FileUpload.last_accessed_at))
            .limit(limit)
            .all()
        )


class QuotaExceededException(Exception):
    """Exception raised when file quota is exceeded."""
    pass

# --- File: C:\Hostel-Main\app\repositories\file_management\image_upload_repository.py ---
"""
Image Upload Repository

Image-specific operations with variant generation, processing queue,
optimization, and metadata extraction.
"""

from datetime import datetime, timedelta
from typing import List, Optional, Dict, Any
from sqlalchemy import and_, or_, func, desc, asc, case
from sqlalchemy.orm import Session, joinedload, selectinload

from app.repositories.base.base_repository import BaseRepository
from app.repositories.base.query_builder import QueryBuilder
from app.repositories.base.pagination import PaginationManager, PaginatedResult
from app.models.file_management.image_upload import (
    ImageUpload,
    ImageVariant,
    ImageProcessing,
    ImageOptimization,
    ImageMetadata,
)
from app.models.file_management.file_upload import FileUpload


class ImageUploadRepository(BaseRepository[ImageUpload]):
    """
    Repository for image upload operations with variant management,
    processing queue, and optimization tracking.
    """

    def __init__(self, db_session: Session):
        super().__init__(ImageUpload, db_session)

    # ============================================================================
    # CORE IMAGE OPERATIONS
    # ============================================================================

    async def create_image_upload(
        self,
        file_id: str,
        image_data: Dict[str, Any],
        audit_context: Optional[Dict[str, Any]] = None,
    ) -> ImageUpload:
        """
        Create image upload record with processing configuration.

        Args:
            file_id: Associated file upload ID
            image_data: Image properties and configuration
            audit_context: Audit context

        Returns:
            Created ImageUpload
        """
        image = ImageUpload(
            file_id=file_id,
            usage=image_data["usage"],
            original_width=image_data["original_width"],
            original_height=image_data["original_height"],
            original_format=image_data["original_format"],
            original_mode=image_data.get("original_mode"),
            has_alpha=image_data.get("has_alpha", False),
            color_space=image_data.get("color_space"),
            dominant_colors=image_data.get("dominant_colors", []),
            generate_variants=image_data.get("generate_variants", True),
            auto_optimize=image_data.get("auto_optimize", True),
            convert_to_webp=image_data.get("convert_to_webp", False),
            quality=image_data.get("quality", 85),
            add_watermark=image_data.get("add_watermark", False),
            original_size_bytes=image_data["original_size_bytes"],
        )

        created_image = await self.create(image, audit_context)

        # Create processing queue entry if auto-processing enabled
        if image_data.get("generate_variants") or image_data.get("auto_optimize"):
            await self.queue_for_processing(
                created_image.id,
                priority=image_data.get("processing_priority", 5),
            )

        return created_image

    async def find_by_file_id(
        self,
        file_id: str,
        load_relationships: bool = False,
    ) -> Optional[ImageUpload]:
        """
        Find image by file ID.

        Args:
            file_id: File upload ID
            load_relationships: Whether to load relationships

        Returns:
            ImageUpload if found
        """
        query = self.db_session.query(ImageUpload).filter(
            ImageUpload.file_id == file_id
        )

        if load_relationships:
            query = query.options(
                joinedload(ImageUpload.file),
                selectinload(ImageUpload.variants),
                joinedload(ImageUpload.processing),
                joinedload(ImageUpload.optimization),
                joinedload(ImageUpload.metadata),
            )

        return query.first()

    async def search_images(
        self,
        criteria: Dict[str, Any],
        pagination: Optional[Dict[str, Any]] = None,
        sort_by: str = "created_at",
        sort_order: str = "desc",
    ) -> PaginatedResult[ImageUpload]:
        """
        Search images with flexible criteria.

        Args:
            criteria: Search criteria
            pagination: Pagination parameters
            sort_by: Sort field
            sort_order: Sort order

        Returns:
            Paginated image results

        Criteria:
            - usage: Filter by usage type
            - min_width: Minimum width
            - max_width: Maximum width
            - min_height: Minimum height
            - max_height: Maximum height
            - format: Original format
            - has_alpha: Filter by alpha channel
            - variants_generated: Filter by variant status
            - optimization_completed: Filter by optimization status
            - created_after: Images created after date
            - created_before: Images created before date
        """
        query = QueryBuilder(ImageUpload, self.db_session)

        if "usage" in criteria:
            query = query.where(ImageUpload.usage == criteria["usage"])

        if "min_width" in criteria:
            query = query.where(ImageUpload.original_width >= criteria["min_width"])

        if "max_width" in criteria:
            query = query.where(ImageUpload.original_width <= criteria["max_width"])

        if "min_height" in criteria:
            query = query.where(ImageUpload.original_height >= criteria["min_height"])

        if "max_height" in criteria:
            query = query.where(ImageUpload.original_height <= criteria["max_height"])

        if "format" in criteria:
            query = query.where(ImageUpload.original_format == criteria["format"])

        if "has_alpha" in criteria:
            query = query.where(ImageUpload.has_alpha == criteria["has_alpha"])

        if "variants_generated" in criteria:
            query = query.where(
                ImageUpload.variants_generated == criteria["variants_generated"]
            )

        if "optimization_completed" in criteria:
            query = query.where(
                ImageUpload.optimization_completed == criteria["optimization_completed"]
            )

        if "created_after" in criteria:
            query = query.where(ImageUpload.created_at >= criteria["created_after"])

        if "created_before" in criteria:
            query = query.where(ImageUpload.created_at <= criteria["created_before"])

        # Apply sorting
        sort_field = getattr(ImageUpload, sort_by, ImageUpload.created_at)
        if sort_order == "desc":
            query = query.order_by(desc(sort_field))
        else:
            query = query.order_by(asc(sort_field))

        return await PaginationManager.paginate(
            query.build(),
            pagination or {"page": 1, "page_size": 50}
        )

    # ============================================================================
    # IMAGE VARIANT OPERATIONS
    # ============================================================================

    async def create_variant(
        self,
        image_id: str,
        variant_data: Dict[str, Any],
    ) -> ImageVariant:
        """
        Create image variant record.

        Args:
            image_id: Parent image ID
            variant_data: Variant properties

        Returns:
            Created ImageVariant
        """
        variant = ImageVariant(
            image_id=image_id,
            variant_name=variant_data["variant_name"],
            storage_key=variant_data["storage_key"],
            width=variant_data["width"],
            height=variant_data["height"],
            format=variant_data["format"],
            size_bytes=variant_data["size_bytes"],
            url=variant_data["url"],
            public_url=variant_data.get("public_url"),
            is_optimized=variant_data.get("is_optimized", False),
            quality=variant_data.get("quality"),
            generated_at=datetime.utcnow(),
            generation_duration_ms=variant_data.get("generation_duration_ms"),
        )

        self.db_session.add(variant)
        self.db_session.commit()
        return variant

    async def get_variants(
        self,
        image_id: str,
        variant_name: Optional[str] = None,
    ) -> List[ImageVariant]:
        """
        Get image variants.

        Args:
            image_id: Image ID
            variant_name: Optional specific variant name

        Returns:
            List of variants
        """
        query = self.db_session.query(ImageVariant).filter(
            ImageVariant.image_id == image_id
        )

        if variant_name:
            query = query.filter(ImageVariant.variant_name == variant_name)

        return query.all()

    async def get_variant_by_name(
        self,
        image_id: str,
        variant_name: str,
    ) -> Optional[ImageVariant]:
        """
        Get specific variant by name.

        Args:
            image_id: Image ID
            variant_name: Variant name

        Returns:
            ImageVariant if found
        """
        return (
            self.db_session.query(ImageVariant)
            .filter(
                ImageVariant.image_id == image_id,
                ImageVariant.variant_name == variant_name,
            )
            .first()
        )

    async def mark_variants_generated(
        self,
        image_id: str,
        audit_context: Optional[Dict[str, Any]] = None,
    ) -> ImageUpload:
        """
        Mark image variants as generated.

        Args:
            image_id: Image ID
            audit_context: Audit context

        Returns:
            Updated ImageUpload
        """
        return await self.update(
            image_id,
            {
                "variants_generated": True,
                "processing_completed_at": datetime.utcnow(),
            },
            audit_context=audit_context,
        )

    # ============================================================================
    # IMAGE PROCESSING OPERATIONS
    # ============================================================================

    async def queue_for_processing(
        self,
        image_id: str,
        priority: int = 5,
        processing_config: Optional[Dict[str, Any]] = None,
    ) -> ImageProcessing:
        """
        Add image to processing queue.

        Args:
            image_id: Image ID
            priority: Processing priority (1-10, higher is more urgent)
            processing_config: Processing configuration

        Returns:
            Created ImageProcessing record
        """
        # Determine required processing steps
        image = await self.find_by_id(image_id)
        steps = []

        if image.generate_variants:
            steps.append("generate_variants")

        if image.auto_optimize:
            steps.append("optimize")

        if image.convert_to_webp:
            steps.append("convert_webp")

        if image.add_watermark:
            steps.append("add_watermark")

        processing = ImageProcessing(
            image_id=image_id,
            status="pending",
            priority=priority,
            pending_steps=steps,
            completed_steps=[],
            queued_at=datetime.utcnow(),
            processing_config=processing_config or {},
        )

        self.db_session.add(processing)
        self.db_session.commit()
        return processing

    async def get_next_processing_job(
        self,
        worker_id: str,
    ) -> Optional[ImageProcessing]:
        """
        Get next image processing job for worker.

        Args:
            worker_id: Worker identifier

        Returns:
            ImageProcessing job if available
        """
        job = (
            self.db_session.query(ImageProcessing)
            .filter(ImageProcessing.status == "pending")
            .order_by(desc(ImageProcessing.priority), asc(ImageProcessing.queued_at))
            .first()
        )

        if job:
            job.status = "processing"
            job.started_at = datetime.utcnow()
            job.worker_id = worker_id
            self.db_session.commit()

        return job

    async def update_processing_progress(
        self,
        processing_id: str,
        current_step: str,
        progress_percentage: float,
        completed_steps: Optional[List[str]] = None,
    ) -> ImageProcessing:
        """
        Update processing progress.

        Args:
            processing_id: Processing job ID
            current_step: Current step being processed
            progress_percentage: Progress percentage
            completed_steps: List of completed steps

        Returns:
            Updated ImageProcessing
        """
        processing = self.db_session.query(ImageProcessing).get(processing_id)

        if not processing:
            raise ValueError(f"Processing job not found: {processing_id}")

        processing.current_step = current_step
        processing.progress_percentage = progress_percentage

        if completed_steps:
            processing.completed_steps = completed_steps

        self.db_session.commit()
        return processing

    async def complete_processing(
        self,
        processing_id: str,
        success: bool = True,
        error_message: Optional[str] = None,
        error_details: Optional[Dict[str, Any]] = None,
    ) -> ImageProcessing:
        """
        Mark processing as completed.

        Args:
            processing_id: Processing job ID
            success: Whether processing succeeded
            error_message: Error message if failed
            error_details: Detailed error information

        Returns:
            Updated ImageProcessing
        """
        processing = self.db_session.query(ImageProcessing).get(processing_id)

        if not processing:
            raise ValueError(f"Processing job not found: {processing_id}")

        processing.status = "completed" if success else "failed"
        processing.completed_at = datetime.utcnow()
        processing.progress_percentage = 100.0 if success else processing.progress_percentage

        if not success:
            processing.error_message = error_message
            processing.error_details = error_details

            # Retry if under limit
            if processing.retry_count < processing.max_retries:
                processing.status = "pending"
                processing.retry_count += 1
                processing.last_retry_at = datetime.utcnow()
                processing.started_at = None
                processing.worker_id = None

        self.db_session.commit()
        return processing

    async def get_processing_queue_stats(self) -> Dict[str, Any]:
        """
        Get processing queue statistics.

        Returns:
            Queue statistics
        """
        stats = self.db_session.query(
            func.count(ImageProcessing.id).label("total"),
            func.count(
                case([(ImageProcessing.status == "pending", 1)])
            ).label("pending"),
            func.count(
                case([(ImageProcessing.status == "processing", 1)])
            ).label("processing"),
            func.count(
                case([(ImageProcessing.status == "completed", 1)])
            ).label("completed"),
            func.count(
                case([(ImageProcessing.status == "failed", 1)])
            ).label("failed"),
        ).first()

        return {
            "total_jobs": stats.total or 0,
            "pending": stats.pending or 0,
            "processing": stats.processing or 0,
            "completed": stats.completed or 0,
            "failed": stats.failed or 0,
        }

    # ============================================================================
    # IMAGE OPTIMIZATION OPERATIONS
    # ============================================================================

    async def create_optimization_result(
        self,
        image_id: str,
        optimization_data: Dict[str, Any],
    ) -> ImageOptimization:
        """
        Store image optimization results.

        Args:
            image_id: Image ID
            optimization_data: Optimization results

        Returns:
            Created ImageOptimization
        """
        optimization = ImageOptimization(
            image_id=image_id,
            optimization_level=optimization_data.get("optimization_level", "medium"),
            target_format=optimization_data.get("target_format"),
            target_quality=optimization_data.get("target_quality", 85),
            compression_algorithm=optimization_data.get("compression_algorithm"),
            compression_level=optimization_data.get("compression_level"),
            strip_metadata=optimization_data.get("strip_metadata", True),
            preserve_exif=optimization_data.get("preserve_exif", False),
            auto_orient=optimization_data.get("auto_orient", True),
            original_size_bytes=optimization_data["original_size_bytes"],
            optimized_size_bytes=optimization_data["optimized_size_bytes"],
            bytes_saved=optimization_data["original_size_bytes"] - optimization_data["optimized_size_bytes"],
            reduction_percentage=(
                (optimization_data["original_size_bytes"] - optimization_data["optimized_size_bytes"])
                / optimization_data["original_size_bytes"] * 100
            ),
            visual_quality_score=optimization_data.get("visual_quality_score"),
            ssim_score=optimization_data.get("ssim_score"),
            optimization_duration_ms=optimization_data.get("optimization_duration_ms"),
            optimized_at=datetime.utcnow(),
            optimization_tool=optimization_data.get("optimization_tool"),
            tool_version=optimization_data.get("tool_version"),
        )

        self.db_session.add(optimization)
        self.db_session.commit()

        # Update parent image
        image = await self.find_by_id(image_id)
        image.optimization_completed = True
        image.optimized_size_bytes = optimization_data["optimized_size_bytes"]
        image.size_reduction_percentage = optimization.reduction_percentage
        self.db_session.commit()

        return optimization

    async def get_optimization_result(
        self,
        image_id: str,
    ) -> Optional[ImageOptimization]:
        """
        Get optimization result for image.

        Args:
            image_id: Image ID

        Returns:
            ImageOptimization if exists
        """
        return (
            self.db_session.query(ImageOptimization)
            .filter(ImageOptimization.image_id == image_id)
            .first()
        )

    async def get_optimization_statistics(
        self,
        start_date: Optional[datetime] = None,
        end_date: Optional[datetime] = None,
    ) -> Dict[str, Any]:
        """
        Get optimization statistics.

        Args:
            start_date: Start date filter
            end_date: End date filter

        Returns:
            Optimization statistics
        """
        query = self.db_session.query(
            func.count(ImageOptimization.id).label("total_optimizations"),
            func.sum(ImageOptimization.bytes_saved).label("total_bytes_saved"),
            func.avg(ImageOptimization.reduction_percentage).label("avg_reduction"),
            func.avg(ImageOptimization.optimization_duration_ms).label("avg_duration"),
        )

        if start_date:
            query = query.filter(ImageOptimization.optimized_at >= start_date)

        if end_date:
            query = query.filter(ImageOptimization.optimized_at <= end_date)

        result = query.first()

        return {
            "total_optimizations": result.total_optimizations or 0,
            "total_bytes_saved": result.total_bytes_saved or 0,
            "average_reduction_percentage": round(result.avg_reduction or 0, 2),
            "average_duration_ms": round(result.avg_duration or 0, 2),
        }

    # ============================================================================
    # IMAGE METADATA OPERATIONS
    # ============================================================================

    async def create_metadata(
        self,
        image_id: str,
        metadata_data: Dict[str, Any],
    ) -> ImageMetadata:
        """
        Store image metadata (EXIF, etc.).

        Args:
            image_id: Image ID
            metadata_data: Extracted metadata

        Returns:
            Created ImageMetadata
        """
        metadata = ImageMetadata(
            image_id=image_id,
            camera_make=metadata_data.get("camera_make"),
            camera_model=metadata_data.get("camera_model"),
            lens_model=metadata_data.get("lens_model"),
            iso=metadata_data.get("iso"),
            aperture=metadata_data.get("aperture"),
            shutter_speed=metadata_data.get("shutter_speed"),
            focal_length=metadata_data.get("focal_length"),
            flash=metadata_data.get("flash"),
            date_taken=metadata_data.get("date_taken"),
            date_modified=metadata_data.get("date_modified"),
            gps_latitude=metadata_data.get("gps_latitude"),
            gps_longitude=metadata_data.get("gps_longitude"),
            gps_altitude=metadata_data.get("gps_altitude"),
            location_name=metadata_data.get("location_name"),
            orientation=metadata_data.get("orientation"),
            resolution_x=metadata_data.get("resolution_x"),
            resolution_y=metadata_data.get("resolution_y"),
            bit_depth=metadata_data.get("bit_depth"),
            software=metadata_data.get("software"),
            copyright=metadata_data.get("copyright"),
            artist=metadata_data.get("artist"),
            description=metadata_data.get("description"),
            keywords=metadata_data.get("keywords", []),
            raw_exif=metadata_data.get("raw_exif", {}),
            extracted_at=datetime.utcnow(),
            extraction_tool=metadata_data.get("extraction_tool"),
        )

        self.db_session.add(metadata)
        self.db_session.commit()
        return metadata

    async def get_metadata(
        self,
        image_id: str,
    ) -> Optional[ImageMetadata]:
        """
        Get image metadata.

        Args:
            image_id: Image ID

        Returns:
            ImageMetadata if exists
        """
        return (
            self.db_session.query(ImageMetadata)
            .filter(ImageMetadata.image_id == image_id)
            .first()
        )

    async def search_by_location(
        self,
        latitude: float,
        longitude: float,
        radius_km: float = 10.0,
        limit: int = 100,
    ) -> List[ImageMetadata]:
        """
        Search images by geographic location.

        Args:
            latitude: Center latitude
            longitude: Center longitude
            radius_km: Search radius in kilometers
            limit: Maximum results

        Returns:
            List of images within radius
        """
        # Simple bounding box search (for precise distance, use PostGIS or similar)
        lat_delta = radius_km / 111.0  # Approximate km per degree latitude
        lon_delta = radius_km / (111.0 * abs(latitude))  # Adjust for latitude

        return (
            self.db_session.query(ImageMetadata)
            .filter(
                ImageMetadata.gps_latitude.between(
                    latitude - lat_delta, latitude + lat_delta
                ),
                ImageMetadata.gps_longitude.between(
                    longitude - lon_delta, longitude + lon_delta
                ),
            )
            .limit(limit)
            .all()
        )

    # ============================================================================
    # ANALYTICS AND REPORTING
    # ============================================================================

    async def get_image_statistics(
        self,
        usage: Optional[str] = None,
        start_date: Optional[datetime] = None,
        end_date: Optional[datetime] = None,
    ) -> Dict[str, Any]:
        """
        Get image upload statistics.

        Args:
            usage: Filter by usage type
            start_date: Start date
            end_date: End date

        Returns:
            Image statistics
        """
        query = self.db_session.query(
            func.count(ImageUpload.id).label("total_images"),
            func.avg(ImageUpload.original_width).label("avg_width"),
            func.avg(ImageUpload.original_height).label("avg_height"),
            func.sum(ImageUpload.original_size_bytes).label("total_size"),
            func.count(
                case([(ImageUpload.variants_generated == True, 1)])
            ).label("with_variants"),
            func.count(
                case([(ImageUpload.optimization_completed == True, 1)])
            ).label("optimized"),
        )

        if usage:
            query = query.filter(ImageUpload.usage == usage)

        if start_date:
            query = query.filter(ImageUpload.created_at >= start_date)

        if end_date:
            query = query.filter(ImageUpload.created_at <= end_date)

        result = query.first()

        return {
            "total_images": result.total_images or 0,
            "average_width": round(result.avg_width or 0, 2),
            "average_height": round(result.avg_height or 0, 2),
            "total_size_bytes": result.total_size or 0,
            "images_with_variants": result.with_variants or 0,
            "optimized_images": result.optimized or 0,
        }

    async def get_format_distribution(self) -> List[Dict[str, Any]]:
        """
        Get distribution of image formats.

        Returns:
            List of format statistics
        """
        results = (
            self.db_session.query(
                ImageUpload.original_format,
                func.count(ImageUpload.id).label("count"),
                func.sum(ImageUpload.original_size_bytes).label("total_size"),
            )
            .group_by(ImageUpload.original_format)
            .order_by(desc("count"))
            .all()
        )

        return [
            {
                "format": row.original_format,
                "count": row.count,
                "total_size_bytes": row.total_size or 0,
            }
            for row in results
        ]

    async def find_images_needing_optimization(
        self,
        limit: int = 100,
    ) -> List[ImageUpload]:
        """
        Find images that need optimization.

        Args:
            limit: Maximum results

        Returns:
            List of images needing optimization
        """
        return (
            self.db_session.query(ImageUpload)
            .filter(
                ImageUpload.auto_optimize == True,
                ImageUpload.optimization_completed == False,
            )
            .order_by(desc(ImageUpload.original_size_bytes))
            .limit(limit)
            .all()
        )

    async def find_images_needing_variants(
        self,
        limit: int = 100,
    ) -> List[ImageUpload]:
        """
        Find images that need variant generation.

        Args:
            limit: Maximum results

        Returns:
            List of images needing variants
        """
        return (
            self.db_session.query(ImageUpload)
            .filter(
                ImageUpload.generate_variants == True,
                ImageUpload.variants_generated == False,
            )
            .order_by(asc(ImageUpload.created_at))
            .limit(limit)
            .all()
        )

# --- File: C:\Hostel-Main\app\repositories\file_management\__init__.py ---
"""
File Management Repositories Package

Comprehensive repositories for file upload, image processing,
document management, metadata, and aggregated analytics.
"""

from app.repositories.file_management.file_upload_repository import (
    FileUploadRepository,
    QuotaExceededException,
)
from app.repositories.file_management.image_upload_repository import (
    ImageUploadRepository,
)
from app.repositories.file_management.document_upload_repository import (
    DocumentUploadRepository,
)
from app.repositories.file_management.file_metadata_repository import (
    FileMetadataRepository,
)
from app.repositories.file_management.file_aggregate_repository import (
    FileAggregateRepository,
)

__all__ = [
    # File Upload
    "FileUploadRepository",
    "QuotaExceededException",
    # Image Upload
    "ImageUploadRepository",
    # Document Upload
    "DocumentUploadRepository",
    # File Metadata
    "FileMetadataRepository",
    # Aggregates
    "FileAggregateRepository",
]

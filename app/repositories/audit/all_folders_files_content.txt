### Combined Content from Folder: C:\Hostel-Main\app\repositories\audit ###



# ===== Folder: C:\Hostel-Main\app\repositories\audit =====

# --- File: C:\Hostel-Main\app\repositories\audit\admin_override_log_repository.py ---
"""
Admin override log repository for governance and accountability.

Tracks admin interventions and overrides with impact analysis,
approval workflows, and compliance monitoring.
"""

from datetime import datetime, timedelta
from typing import Any, Dict, List, Optional, Tuple
from uuid import UUID
from decimal import Decimal

from sqlalchemy import and_, or_, func, desc
from sqlalchemy.orm import Session

from app.models.audit import AdminOverrideLog
from app.repositories.base.base_repository import BaseRepository


class AdminOverrideLogRepository(BaseRepository):
    """
    Repository for admin override tracking and governance.
    
    Provides comprehensive override logging, approval workflows,
    and impact analysis for administrative accountability.
    """
    
    def __init__(self, session: Session):
        """Initialize repository with database session."""
        super().__init__(session, AdminOverrideLog)
    
    # ==================== CRUD Operations ====================
    
    def create_override_log(
        self,
        admin_id: UUID,
        hostel_id: UUID,
        override_type: str,
        override_category: str,
        entity_type: str,
        entity_id: UUID,
        reason: str,
        override_action: Dict[str, Any],
        supervisor_id: Optional[UUID] = None,
        original_action: Optional[Dict] = None,
        **kwargs
    ) -> AdminOverrideLog:
        """
        Create a new admin override log entry.
        
        Args:
            admin_id: Admin performing override
            hostel_id: Hostel where override occurred
            override_type: Type of override
            override_category: Category of override
            entity_type: Type of entity affected
            entity_id: Entity ID
            reason: Override reason/justification
            override_action: Admin's override decision
            supervisor_id: Optional supervisor being overridden
            original_action: Optional original supervisor action
            **kwargs: Additional fields
            
        Returns:
            Created AdminOverrideLog instance
        """
        override_log = AdminOverrideLog(
            admin_id=admin_id,
            hostel_id=hostel_id,
            override_type=override_type,
            override_category=override_category,
            entity_type=entity_type,
            entity_id=entity_id,
            reason=reason,
            override_action=override_action,
            supervisor_id=supervisor_id,
            original_action=original_action or {},
            **kwargs
        )
        
        # Auto-calculate impact score if not provided
        if 'impact_score' not in kwargs:
            override_log.impact_score = self._calculate_impact_score(
                override_category,
                kwargs.get('severity', 'medium')
            )
        
        return self.create(override_log)
    
    # ==================== Query Operations ====================
    
    def find_by_admin(
        self,
        admin_id: UUID,
        start_date: Optional[datetime] = None,
        end_date: Optional[datetime] = None,
        override_category: Optional[str] = None,
        limit: int = 100,
        offset: int = 0
    ) -> Tuple[List[AdminOverrideLog], int]:
        """
        Find overrides by admin with filtering.
        
        Args:
            admin_id: Admin ID
            start_date: Start date filter
            end_date: End date filter
            override_category: Optional category filter
            limit: Maximum results
            offset: Results to skip
            
        Returns:
            Tuple of (override logs, total count)
        """
        query = self.session.query(AdminOverrideLog).filter(
            AdminOverrideLog.admin_id == admin_id
        )
        
        if start_date:
            query = query.filter(AdminOverrideLog.created_at >= start_date)
        
        if end_date:
            query = query.filter(AdminOverrideLog.created_at <= end_date)
        
        if override_category:
            query = query.filter(AdminOverrideLog.override_category == override_category)
        
        total = query.count()
        
        results = query.order_by(desc(AdminOverrideLog.created_at))\
            .limit(limit)\
            .offset(offset)\
            .all()
        
        return results, total
    
    def find_by_supervisor(
        self,
        supervisor_id: UUID,
        start_date: Optional[datetime] = None,
        end_date: Optional[datetime] = None,
        limit: int = 100
    ) -> List[AdminOverrideLog]:
        """
        Find overrides of a supervisor's decisions.
        
        Args:
            supervisor_id: Supervisor ID
            start_date: Start date filter
            end_date: End date filter
            limit: Maximum results
            
        Returns:
            List of override logs
        """
        query = self.session.query(AdminOverrideLog).filter(
            AdminOverrideLog.supervisor_id == supervisor_id
        )
        
        if start_date:
            query = query.filter(AdminOverrideLog.created_at >= start_date)
        
        if end_date:
            query = query.filter(AdminOverrideLog.created_at <= end_date)
        
        return query.order_by(desc(AdminOverrideLog.created_at))\
            .limit(limit)\
            .all()
    
    def find_by_hostel(
        self,
        hostel_id: UUID,
        start_date: Optional[datetime] = None,
        end_date: Optional[datetime] = None,
        severity: Optional[str] = None,
        limit: int = 100
    ) -> List[AdminOverrideLog]:
        """
        Find overrides by hostel.
        
        Args:
            hostel_id: Hostel ID
            start_date: Start date filter
            end_date: End date filter
            severity: Optional severity filter
            limit: Maximum results
            
        Returns:
            List of override logs
        """
        query = self.session.query(AdminOverrideLog).filter(
            AdminOverrideLog.hostel_id == hostel_id
        )
        
        if start_date:
            query = query.filter(AdminOverrideLog.created_at >= start_date)
        
        if end_date:
            query = query.filter(AdminOverrideLog.created_at <= end_date)
        
        if severity:
            query = query.filter(AdminOverrideLog.severity == severity)
        
        return query.order_by(desc(AdminOverrideLog.created_at))\
            .limit(limit)\
            .all()
    
    def find_by_entity(
        self,
        entity_type: str,
        entity_id: UUID,
        limit: int = 100
    ) -> List[AdminOverrideLog]:
        """
        Find overrides for a specific entity.
        
        Args:
            entity_type: Entity type
            entity_id: Entity ID
            limit: Maximum results
            
        Returns:
            List of override logs
        """
        return self.session.query(AdminOverrideLog).filter(
            AdminOverrideLog.entity_type == entity_type,
            AdminOverrideLog.entity_id == entity_id
        ).order_by(desc(AdminOverrideLog.created_at))\
            .limit(limit)\
            .all()
    
    def find_pending_approval(
        self,
        admin_id: Optional[UUID] = None,
        hostel_id: Optional[UUID] = None,
        limit: int = 100
    ) -> List[AdminOverrideLog]:
        """
        Find overrides pending approval.
        
        Args:
            admin_id: Optional admin filter
            hostel_id: Optional hostel filter
            limit: Maximum results
            
        Returns:
            List of pending override logs
        """
        query = self.session.query(AdminOverrideLog).filter(
            AdminOverrideLog.requires_approval == True,
            AdminOverrideLog.approved_at.is_(None)
        )
        
        if admin_id:
            query = query.filter(AdminOverrideLog.admin_id == admin_id)
        
        if hostel_id:
            query = query.filter(AdminOverrideLog.hostel_id == hostel_id)
        
        return query.order_by(AdminOverrideLog.created_at)\
            .limit(limit)\
            .all()
    
    def find_requiring_follow_up(
        self,
        hostel_id: Optional[UUID] = None,
        overdue_only: bool = False,
        limit: int = 100
    ) -> List[AdminOverrideLog]:
        """
        Find overrides requiring follow-up.
        
        Args:
            hostel_id: Optional hostel filter
            overdue_only: Only return overdue items
            limit: Maximum results
            
        Returns:
            List of override logs requiring follow-up
        """
        query = self.session.query(AdminOverrideLog).filter(
            AdminOverrideLog.follow_up_required == True,
            AdminOverrideLog.follow_up_completed != True
        )
        
        if hostel_id:
            query = query.filter(AdminOverrideLog.hostel_id == hostel_id)
        
        if overdue_only:
            # Add logic for overdue check if you have a follow_up_date field
            pass
        
        return query.order_by(AdminOverrideLog.created_at)\
            .limit(limit)\
            .all()
    
    def find_high_impact(
        self,
        threshold: Decimal = Decimal('75.0'),
        start_date: Optional[datetime] = None,
        limit: int = 100
    ) -> List[AdminOverrideLog]:
        """
        Find high-impact overrides.
        
        Args:
            threshold: Minimum impact score
            start_date: Start date filter
            limit: Maximum results
            
        Returns:
            List of high-impact override logs
        """
        query = self.session.query(AdminOverrideLog).filter(
            AdminOverrideLog.impact_score >= threshold
        )
        
        if start_date:
            query = query.filter(AdminOverrideLog.created_at >= start_date)
        
        return query.order_by(desc(AdminOverrideLog.impact_score))\
            .limit(limit)\
            .all()
    
    # ==================== Approval Operations ====================
    
    def approve_override(
        self,
        override_id: UUID,
        approved_by: UUID
    ) -> AdminOverrideLog:
        """
        Approve a pending override.
        
        Args:
            override_id: Override log ID
            approved_by: User approving
            
        Returns:
            Updated override log
        """
        override = self.get_by_id(override_id)
        if not override:
            raise ValueError(f"Override {override_id} not found")
        
        if not override.requires_approval:
            raise ValueError("Override does not require approval")
        
        if override.approved_at:
            raise ValueError("Override already approved")
        
        override.approved_by = approved_by
        override.approved_at = datetime.utcnow()
        
        self.session.commit()
        
        return override
    
    def reject_override(
        self,
        override_id: UUID,
        rejected_by: UUID,
        rejection_reason: str
    ) -> AdminOverrideLog:
        """
        Reject a pending override.
        
        Args:
            override_id: Override log ID
            rejected_by: User rejecting
            rejection_reason: Reason for rejection
            
        Returns:
            Updated override log
        """
        override = self.get_by_id(override_id)
        if not override:
            raise ValueError(f"Override {override_id} not found")
        
        override.outcome_status = 'reversed'
        override.outcome = f"Rejected by {rejected_by}: {rejection_reason}"
        
        self.session.commit()
        
        return override
    
    # ==================== Analytics Operations ====================
    
    def get_override_statistics(
        self,
        start_date: datetime,
        end_date: datetime,
        hostel_id: Optional[UUID] = None
    ) -> Dict[str, Any]:
        """
        Get comprehensive override statistics.
        
        Args:
            start_date: Period start
            end_date: Period end
            hostel_id: Optional hostel filter
            
        Returns:
            Statistics dictionary
        """
        query = self.session.query(AdminOverrideLog).filter(
            AdminOverrideLog.created_at >= start_date,
            AdminOverrideLog.created_at <= end_date
        )
        
        if hostel_id:
            query = query.filter(AdminOverrideLog.hostel_id == hostel_id)
        
        all_overrides = query.all()
        total = len(all_overrides)
        
        if total == 0:
            return {
                'period': {'start': start_date.isoformat(), 'end': end_date.isoformat()},
                'total_overrides': 0,
                'message': 'No overrides in period'
            }
        
        # By category
        category_counts = {}
        for override in all_overrides:
            cat = override.override_category
            category_counts[cat] = category_counts.get(cat, 0) + 1
        
        # By severity
        severity_counts = {}
        for override in all_overrides:
            sev = override.severity
            severity_counts[sev] = severity_counts.get(sev, 0) + 1
        
        # By outcome
        outcome_counts = {}
        for override in all_overrides:
            outcome = override.outcome_status
            outcome_counts[outcome] = outcome_counts.get(outcome, 0) + 1
        
        # Top admins
        admin_counts = self.session.query(
            AdminOverrideLog.admin_id,
            AdminOverrideLog.admin_name,
            func.count(AdminOverrideLog.id).label('count')
        ).filter(
            AdminOverrideLog.created_at >= start_date,
            AdminOverrideLog.created_at <= end_date
        )
        
        if hostel_id:
            admin_counts = admin_counts.filter(AdminOverrideLog.hostel_id == hostel_id)
        
        admin_counts = admin_counts.group_by(
            AdminOverrideLog.admin_id,
            AdminOverrideLog.admin_name
        ).order_by(desc('count')).limit(10).all()
        
        # Average impact score
        impact_scores = [float(o.impact_score) for o in all_overrides if o.impact_score]
        avg_impact = sum(impact_scores) / len(impact_scores) if impact_scores else 0
        
        return {
            'period': {
                'start': start_date.isoformat(),
                'end': end_date.isoformat()
            },
            'total_overrides': total,
            'by_category': category_counts,
            'by_severity': severity_counts,
            'by_outcome': outcome_counts,
            'top_admins': [
                {'admin_id': str(aid), 'admin_name': name, 'count': count}
                for aid, name, count in admin_counts
            ],
            'average_impact_score': round(avg_impact, 2),
            'approval_metrics': {
                'requiring_approval': sum(1 for o in all_overrides if o.requires_approval),
                'approved': sum(1 for o in all_overrides if o.approved_at),
                'pending_approval': sum(
                    1 for o in all_overrides 
                    if o.requires_approval and not o.approved_at
                )
            }
        }
    
    def get_supervisor_override_analysis(
        self,
        supervisor_id: UUID,
        start_date: datetime,
        end_date: datetime
    ) -> Dict[str, Any]:
        """
        Analyze overrides of a supervisor's decisions.
        
        Args:
            supervisor_id: Supervisor ID
            start_date: Period start
            end_date: Period end
            
        Returns:
            Analysis dictionary
        """
        overrides = self.find_by_supervisor(supervisor_id, start_date, end_date)
        
        if not overrides:
            return {
                'supervisor_id': str(supervisor_id),
                'total_overrides': 0,
                'message': 'No overrides found'
            }
        
        # By category
        category_counts = {}
        for override in overrides:
            cat = override.override_category
            category_counts[cat] = category_counts.get(cat, 0) + 1
        
        # By admin
        admin_counts = {}
        for override in overrides:
            admin = override.admin_name or str(override.admin_id)
            admin_counts[admin] = admin_counts.get(admin, 0) + 1
        
        # Average impact
        impact_scores = [float(o.impact_score) for o in overrides if o.impact_score]
        avg_impact = sum(impact_scores) / len(impact_scores) if impact_scores else 0
        
        return {
            'supervisor_id': str(supervisor_id),
            'period': {
                'start': start_date.isoformat(),
                'end': end_date.isoformat()
            },
            'total_overrides': len(overrides),
            'by_category': category_counts,
            'by_admin': admin_counts,
            'average_impact_score': round(avg_impact, 2),
            'trend_analysis': 'increasing' if len(overrides) > 5 else 'stable'
        }
    
    # ==================== Helper Methods ====================
    
    def _calculate_impact_score(
        self,
        override_category: str,
        severity: str
    ) -> Decimal:
        """
        Calculate impact score based on category and severity.
        
        Args:
            override_category: Override category
            severity: Severity level
            
        Returns:
            Calculated impact score
        """
        # Base scores by category
        category_scores = {
            'decision_reversal': 40,
            'task_reassignment': 30,
            'priority_change': 25,
            'approval': 35,
            'rejection': 45,
            'other': 20
        }
        
        # Severity multipliers
        severity_multipliers = {
            'low': 0.5,
            'medium': 1.0,
            'high': 1.5,
            'critical': 2.0
        }
        
        base_score = category_scores.get(override_category, 30)
        multiplier = severity_multipliers.get(severity, 1.0)
        
        score = base_score * multiplier
        
        # Cap at 100
        return Decimal(str(min(100, score)))

# --- File: C:\Hostel-Main\app\repositories\audit\audit_aggregate_repository.py ---
"""
Audit aggregate repository for cross-functional audit analytics.

Provides aggregated insights, trend analysis, and comprehensive
reporting across all audit data sources.
"""

from datetime import datetime, timedelta
from typing import Any, Dict, List, Optional, Tuple
from uuid import UUID
from decimal import Decimal

from sqlalchemy import and_, or_, func, desc, case
from sqlalchemy.orm import Session

from app.models.audit import (
    AuditLog,
    EntityChangeLog,
    SupervisorActivityLog,
    AdminOverrideLog
)
from app.repositories.base.base_repository import BaseRepository
from app.schemas.common.enums import AuditActionCategory


class AuditAggregateRepository(BaseRepository):
    """
    Repository for aggregated audit analytics and reporting.
    
    Provides cross-functional insights combining data from all
    audit sources for comprehensive analysis and reporting.
    """
    
    def __init__(self, session: Session):
        """Initialize repository with database session."""
        super().__init__(session, AuditLog)  # Use AuditLog as base model
    
    # ==================== Dashboard Metrics ====================
    
    def get_dashboard_metrics(
        self,
        hostel_id: Optional[UUID] = None,
        start_date: Optional[datetime] = None,
        end_date: Optional[datetime] = None
    ) -> Dict[str, Any]:
        """
        Get comprehensive dashboard metrics for audit overview.
        
        Args:
            hostel_id: Optional hostel filter
            start_date: Period start (defaults to 30 days ago)
            end_date: Period end (defaults to now)
            
        Returns:
            Dashboard metrics dictionary
        """
        if not start_date:
            start_date = datetime.utcnow() - timedelta(days=30)
        if not end_date:
            end_date = datetime.utcnow()
        
        # Audit logs summary
        audit_query = self.session.query(AuditLog).filter(
            AuditLog.created_at >= start_date,
            AuditLog.created_at <= end_date
        )
        if hostel_id:
            audit_query = audit_query.filter(AuditLog.hostel_id == hostel_id)
        
        total_actions = audit_query.count()
        failed_actions = audit_query.filter(AuditLog.status == 'failure').count()
        sensitive_actions = audit_query.filter(AuditLog.is_sensitive == True).count()
        
        # Entity changes summary
        changes_query = self.session.query(EntityChangeLog).filter(
            EntityChangeLog.created_at >= start_date,
            EntityChangeLog.created_at <= end_date
        )
        
        total_changes = changes_query.count()
        sensitive_changes = changes_query.filter(
            or_(
                EntityChangeLog.is_sensitive == True,
                EntityChangeLog.is_pii == True
            )
        ).count()
        
        # Supervisor activities summary
        activities_query = self.session.query(SupervisorActivityLog).filter(
            SupervisorActivityLog.created_at >= start_date,
            SupervisorActivityLog.created_at <= end_date
        )
        if hostel_id:
            activities_query = activities_query.filter(SupervisorActivityLog.hostel_id == hostel_id)
        
        total_activities = activities_query.count()
        failed_activities = activities_query.filter(
            SupervisorActivityLog.status == 'failed'
        ).count()
        
        # Admin overrides summary
        overrides_query = self.session.query(AdminOverrideLog).filter(
            AdminOverrideLog.created_at >= start_date,
            AdminOverrideLog.created_at <= end_date
        )
        if hostel_id:
            overrides_query = overrides_query.filter(AdminOverrideLog.hostel_id == hostel_id)
        
        total_overrides = overrides_query.count()
        pending_approvals = overrides_query.filter(
            AdminOverrideLog.requires_approval == True,
            AdminOverrideLog.approved_at.is_(None)
        ).count()
        
        return {
            'period': {
                'start': start_date.isoformat(),
                'end': end_date.isoformat(),
                'days': (end_date - start_date).days
            },
            'hostel_id': str(hostel_id) if hostel_id else 'all',
            'audit_logs': {
                'total': total_actions,
                'failed': failed_actions,
                'sensitive': sensitive_actions,
                'failure_rate': round(failed_actions / total_actions * 100, 2) if total_actions else 0
            },
            'entity_changes': {
                'total': total_changes,
                'sensitive': sensitive_changes,
                'sensitive_percentage': round(sensitive_changes / total_changes * 100, 2) if total_changes else 0
            },
            'supervisor_activities': {
                'total': total_activities,
                'failed': failed_activities,
                'success_rate': round((total_activities - failed_activities) / total_activities * 100, 2) if total_activities else 0
            },
            'admin_overrides': {
                'total': total_overrides,
                'pending_approvals': pending_approvals,
                'approval_rate': round((total_overrides - pending_approvals) / total_overrides * 100, 2) if total_overrides else 0
            }
        }
    
    # ==================== Trend Analysis ====================
    
    def get_activity_trends(
        self,
        hostel_id: Optional[UUID] = None,
        days: int = 30,
        group_by: str = 'day'  # 'hour', 'day', 'week'
    ) -> Dict[str, List[Dict[str, Any]]]:
        """
        Get activity trends across all audit sources.
        
        Args:
            hostel_id: Optional hostel filter
            days: Number of days to analyze
            group_by: Time grouping interval
            
        Returns:
            Trends dictionary with data for each audit source
        """
        start_date = datetime.utcnow() - timedelta(days=days)
        end_date = datetime.utcnow()
        
        # Determine time truncation
        if group_by == 'hour':
            time_trunc = func.date_trunc('hour', AuditLog.created_at)
        elif group_by == 'day':
            time_trunc = func.date_trunc('day', AuditLog.created_at)
        elif group_by == 'week':
            time_trunc = func.date_trunc('week', AuditLog.created_at)
        else:
            time_trunc = func.date_trunc('day', AuditLog.created_at)
        
        # Audit logs trend
        audit_trend = self.session.query(
            time_trunc.label('time_bucket'),
            func.count(AuditLog.id).label('count')
        ).filter(
            AuditLog.created_at >= start_date,
            AuditLog.created_at <= end_date
        )
        if hostel_id:
            audit_trend = audit_trend.filter(AuditLog.hostel_id == hostel_id)
        audit_trend = audit_trend.group_by('time_bucket').order_by('time_bucket').all()
        
        # Entity changes trend
        changes_trend = self.session.query(
            func.date_trunc(group_by, EntityChangeLog.created_at).label('time_bucket'),
            func.count(EntityChangeLog.id).label('count')
        ).filter(
            EntityChangeLog.created_at >= start_date,
            EntityChangeLog.created_at <= end_date
        ).group_by('time_bucket').order_by('time_bucket').all()
        
        # Supervisor activities trend
        activities_trend_query = self.session.query(
            func.date_trunc(group_by, SupervisorActivityLog.created_at).label('time_bucket'),
            func.count(SupervisorActivityLog.id).label('count')
        ).filter(
            SupervisorActivityLog.created_at >= start_date,
            SupervisorActivityLog.created_at <= end_date
        )
        if hostel_id:
            activities_trend_query = activities_trend_query.filter(
                SupervisorActivityLog.hostel_id == hostel_id
            )
        activities_trend = activities_trend_query.group_by('time_bucket').order_by('time_bucket').all()
        
        # Admin overrides trend
        overrides_trend_query = self.session.query(
            func.date_trunc(group_by, AdminOverrideLog.created_at).label('time_bucket'),
            func.count(AdminOverrideLog.id).label('count')
        ).filter(
            AdminOverrideLog.created_at >= start_date,
            AdminOverrideLog.created_at <= end_date
        )
        if hostel_id:
            overrides_trend_query = overrides_trend_query.filter(
                AdminOverrideLog.hostel_id == hostel_id
            )
        overrides_trend = overrides_trend_query.group_by('time_bucket').order_by('time_bucket').all()
        
        return {
            'period': {
                'start': start_date.isoformat(),
                'end': end_date.isoformat(),
                'grouping': group_by
            },
            'audit_logs': [
                {'timestamp': bucket.isoformat(), 'count': count}
                for bucket, count in audit_trend
            ],
            'entity_changes': [
                {'timestamp': bucket.isoformat(), 'count': count}
                for bucket, count in changes_trend
            ],
            'supervisor_activities': [
                {'timestamp': bucket.isoformat(), 'count': count}
                for bucket, count in activities_trend
            ],
            'admin_overrides': [
                {'timestamp': bucket.isoformat(), 'count': count}
                for bucket, count in overrides_trend
            ]
        }
    
    # ==================== User Activity Analysis ====================
    
    def get_user_comprehensive_activity(
        self,
        user_id: UUID,
        start_date: datetime,
        end_date: datetime
    ) -> Dict[str, Any]:
        """
        Get comprehensive activity analysis for a user across all audit sources.
        
        Args:
            user_id: User ID
            start_date: Period start
            end_date: Period end
            
        Returns:
            Comprehensive activity dictionary
        """
        # Audit logs
        audit_logs = self.session.query(AuditLog).filter(
            AuditLog.user_id == user_id,
            AuditLog.created_at >= start_date,
            AuditLog.created_at <= end_date
        ).all()
        
        # Entity changes
        entity_changes = self.session.query(EntityChangeLog).filter(
            EntityChangeLog.changed_by_user_id == user_id,
            EntityChangeLog.created_at >= start_date,
            EntityChangeLog.created_at <= end_date
        ).all()
        
        # Supervisor activities (if user is supervisor)
        supervisor_activities = self.session.query(SupervisorActivityLog).filter(
            SupervisorActivityLog.supervisor_id == user_id,
            SupervisorActivityLog.created_at >= start_date,
            SupervisorActivityLog.created_at <= end_date
        ).all()
        
        # Admin overrides (if user is admin)
        admin_overrides = self.session.query(AdminOverrideLog).filter(
            AdminOverrideLog.admin_id == user_id,
            AdminOverrideLog.created_at >= start_date,
            AdminOverrideLog.created_at <= end_date
        ).all()
        
        # Calculate activity scores
        total_activities = (
            len(audit_logs) +
            len(entity_changes) +
            len(supervisor_activities) +
            len(admin_overrides)
        )
        
        # Action categories breakdown
        category_breakdown = {}
        for log in audit_logs:
            cat = log.action_category.value
            category_breakdown[cat] = category_breakdown.get(cat, 0) + 1
        
        # Most active times (hour of day)
        hour_distribution = {}
        for log in audit_logs:
            hour = log.created_at.hour
            hour_distribution[hour] = hour_distribution.get(hour, 0) + 1
        
        # Sensitive data access
        sensitive_access_count = sum(1 for log in audit_logs if log.is_sensitive)
        
        return {
            'user_id': str(user_id),
            'period': {
                'start': start_date.isoformat(),
                'end': end_date.isoformat()
            },
            'total_activities': total_activities,
            'breakdown': {
                'audit_actions': len(audit_logs),
                'entity_changes': len(entity_changes),
                'supervisor_activities': len(supervisor_activities),
                'admin_overrides': len(admin_overrides)
            },
            'action_categories': category_breakdown,
            'activity_by_hour': hour_distribution,
            'sensitive_data_access': {
                'count': sensitive_access_count,
                'percentage': round(sensitive_access_count / len(audit_logs) * 100, 2) if audit_logs else 0
            },
            'average_daily_activity': round(total_activities / max((end_date - start_date).days, 1), 2)
        }
    
    # ==================== Security Analysis ====================
    
    def get_security_overview(
        self,
        hostel_id: Optional[UUID] = None,
        days: int = 7
    ) -> Dict[str, Any]:
        """
        Get comprehensive security overview from all audit sources.
        
        Args:
            hostel_id: Optional hostel filter
            days: Number of days to analyze
            
        Returns:
            Security overview dictionary
        """
        start_date = datetime.utcnow() - timedelta(days=days)
        end_date = datetime.utcnow()
        
        # Failed authentication attempts (from audit logs)
        failed_auth_query = self.session.query(AuditLog).filter(
            AuditLog.created_at >= start_date,
            AuditLog.action_category == AuditActionCategory.SECURITY,
            AuditLog.status == 'failure'
        )
        if hostel_id:
            failed_auth_query = failed_auth_query.filter(AuditLog.hostel_id == hostel_id)
        failed_auth = failed_auth_query.count()
        
        # Sensitive data access
        sensitive_access_query = self.session.query(AuditLog).filter(
            AuditLog.created_at >= start_date,
            AuditLog.is_sensitive == True
        )
        if hostel_id:
            sensitive_access_query = sensitive_access_query.filter(AuditLog.hostel_id == hostel_id)
        sensitive_access = sensitive_access_query.count()
        
        # Sensitive entity changes
        sensitive_changes = self.session.query(EntityChangeLog).filter(
            EntityChangeLog.created_at >= start_date,
            or_(
                EntityChangeLog.is_sensitive == True,
                EntityChangeLog.is_pii == True
            )
        ).count()
        
        # High-impact admin overrides
        high_impact_overrides_query = self.session.query(AdminOverrideLog).filter(
            AdminOverrideLog.created_at >= start_date,
            AdminOverrideLog.impact_score >= 75
        )
        if hostel_id:
            high_impact_overrides_query = high_impact_overrides_query.filter(
                AdminOverrideLog.hostel_id == hostel_id
            )
        high_impact_overrides = high_impact_overrides_query.count()
        
        # Unique IPs with failed actions
        failed_ips = self.session.query(
            func.distinct(AuditLog.ip_address)
        ).filter(
            AuditLog.created_at >= start_date,
            AuditLog.status == 'failure',
            AuditLog.ip_address.isnot(None)
        )
        if hostel_id:
            failed_ips = failed_ips.filter(AuditLog.hostel_id == hostel_id)
        unique_failed_ips = failed_ips.count()
        
        # Security events requiring review
        requiring_review_query = self.session.query(AuditLog).filter(
            AuditLog.created_at >= start_date,
            AuditLog.requires_review == True
        )
        if hostel_id:
            requiring_review_query = requiring_review_query.filter(AuditLog.hostel_id == hostel_id)
        requiring_review = requiring_review_query.count()
        
        # Critical severity events
        critical_events_query = self.session.query(AuditLog).filter(
            AuditLog.created_at >= start_date,
            AuditLog.severity_level == 'critical'
        )
        if hostel_id:
            critical_events_query = critical_events_query.filter(AuditLog.hostel_id == hostel_id)
        critical_events = critical_events_query.count()
        
        return {
            'period': {
                'start': start_date.isoformat(),
                'end': end_date.isoformat(),
                'days': days
            },
            'hostel_id': str(hostel_id) if hostel_id else 'all',
            'security_metrics': {
                'failed_authentication_attempts': failed_auth,
                'sensitive_data_access': sensitive_access,
                'sensitive_data_changes': sensitive_changes,
                'high_impact_overrides': high_impact_overrides,
                'unique_ips_with_failures': unique_failed_ips,
                'events_requiring_review': requiring_review,
                'critical_severity_events': critical_events
            },
            'security_score': self._calculate_security_score(
                failed_auth,
                sensitive_access,
                critical_events,
                requiring_review
            )
        }
    
    def detect_suspicious_patterns(
        self,
        hostel_id: Optional[UUID] = None,
        days: int = 7
    ) -> List[Dict[str, Any]]:
        """
        Detect suspicious activity patterns across all audit sources.
        
        Args:
            hostel_id: Optional hostel filter
            days: Number of days to analyze
            
        Returns:
            List of detected suspicious patterns
        """
        start_date = datetime.utcnow() - timedelta(days=days)
        end_date = datetime.utcnow()
        
        suspicious_patterns = []
        
        # Pattern 1: Multiple failed login attempts from same IP
        failed_by_ip = self.session.query(
            AuditLog.ip_address,
            func.count(AuditLog.id).label('failure_count')
        ).filter(
            AuditLog.created_at >= start_date,
            AuditLog.action_type.like('%login%'),
            AuditLog.status == 'failure',
            AuditLog.ip_address.isnot(None)
        )
        if hostel_id:
            failed_by_ip = failed_by_ip.filter(AuditLog.hostel_id == hostel_id)
        
        failed_by_ip = failed_by_ip.group_by(AuditLog.ip_address)\
            .having(func.count(AuditLog.id) > 5)\
            .all()
        
        for ip, count in failed_by_ip:
            suspicious_patterns.append({
                'pattern_type': 'multiple_failed_logins',
                'severity': 'high',
                'ip_address': str(ip),
                'failure_count': count,
                'description': f'IP {ip} had {count} failed login attempts'
            })
        
        # Pattern 2: Unusual activity volume by user
        user_activity = self.session.query(
            AuditLog.user_id,
            AuditLog.user_email,
            func.count(AuditLog.id).label('action_count')
        ).filter(
            AuditLog.created_at >= start_date,
            AuditLog.user_id.isnot(None)
        )
        if hostel_id:
            user_activity = user_activity.filter(AuditLog.hostel_id == hostel_id)
        
        user_activity = user_activity.group_by(
            AuditLog.user_id,
            AuditLog.user_email
        ).all()
        
        if user_activity:
            action_counts = [count for _, _, count in user_activity]
            avg_count = sum(action_counts) / len(action_counts)
            threshold = avg_count * 3  # 3x average
            
            for user_id, email, count in user_activity:
                if count > threshold:
                    suspicious_patterns.append({
                        'pattern_type': 'unusual_activity_volume',
                        'severity': 'medium',
                        'user_id': str(user_id),
                        'user_email': email,
                        'action_count': count,
                        'threshold': round(threshold, 2),
                        'description': f'User {email} had {count} actions (threshold: {threshold:.0f})'
                    })
        
        # Pattern 3: Multiple sensitive data access in short time
        sensitive_access = self.session.query(
            AuditLog.user_id,
            AuditLog.user_email,
            func.count(AuditLog.id).label('access_count')
        ).filter(
            AuditLog.created_at >= start_date,
            AuditLog.is_sensitive == True,
            AuditLog.user_id.isnot(None)
        )
        if hostel_id:
            sensitive_access = sensitive_access.filter(AuditLog.hostel_id == hostel_id)
        
        sensitive_access = sensitive_access.group_by(
            AuditLog.user_id,
            AuditLog.user_email
        ).having(func.count(AuditLog.id) > 10).all()
        
        for user_id, email, count in sensitive_access:
            suspicious_patterns.append({
                'pattern_type': 'excessive_sensitive_access',
                'severity': 'high',
                'user_id': str(user_id),
                'user_email': email,
                'access_count': count,
                'description': f'User {email} accessed sensitive data {count} times'
            })
        
        # Pattern 4: Off-hours activity
        off_hours_activity = self.session.query(
            AuditLog.user_id,
            AuditLog.user_email,
            func.count(AuditLog.id).label('activity_count')
        ).filter(
            AuditLog.created_at >= start_date,
            or_(
                func.extract('hour', AuditLog.created_at) < 6,
                func.extract('hour', AuditLog.created_at) > 22
            ),
            AuditLog.user_id.isnot(None)
        )
        if hostel_id:
            off_hours_activity = off_hours_activity.filter(AuditLog.hostel_id == hostel_id)
        
        off_hours_activity = off_hours_activity.group_by(
            AuditLog.user_id,
            AuditLog.user_email
        ).having(func.count(AuditLog.id) > 20).all()
        
        for user_id, email, count in off_hours_activity:
            suspicious_patterns.append({
                'pattern_type': 'off_hours_activity',
                'severity': 'medium',
                'user_id': str(user_id),
                'user_email': email,
                'activity_count': count,
                'description': f'User {email} had {count} actions during off-hours (10PM-6AM)'
            })
        
        return sorted(suspicious_patterns, key=lambda x: {'high': 3, 'medium': 2, 'low': 1}.get(x['severity'], 0), reverse=True)
    
    # ==================== Compliance Reporting ====================
    
    def generate_compliance_report(
        self,
        start_date: datetime,
        end_date: datetime,
        hostel_id: Optional[UUID] = None,
        compliance_framework: Optional[str] = None
    ) -> Dict[str, Any]:
        """
        Generate comprehensive compliance report across all audit sources.
        
        Args:
            start_date: Report period start
            end_date: Report period end
            hostel_id: Optional hostel filter
            compliance_framework: Optional framework filter (GDPR, HIPAA, etc.)
            
        Returns:
            Compliance report dictionary
        """
        # Audit logs compliance
        audit_query = self.session.query(AuditLog).filter(
            AuditLog.created_at >= start_date,
            AuditLog.created_at <= end_date
        )
        if hostel_id:
            audit_query = audit_query.filter(AuditLog.hostel_id == hostel_id)
        if compliance_framework:
            audit_query = audit_query.filter(
                AuditLog.compliance_tags.contains([compliance_framework])
            )
        
        total_audit_logs = audit_query.count()
        sensitive_audit_logs = audit_query.filter(AuditLog.is_sensitive == True).count()
        
        # Entity changes compliance
        changes_query = self.session.query(EntityChangeLog).filter(
            EntityChangeLog.created_at >= start_date,
            EntityChangeLog.created_at <= end_date
        )
        
        total_changes = changes_query.count()
        pii_changes = changes_query.filter(EntityChangeLog.is_pii == True).count()
        
        # Data retention compliance
        retention_violations = self.session.query(AuditLog).filter(
            AuditLog.created_at < start_date - timedelta(days=365),  # Example: 1 year retention
            AuditLog.retention_days.isnot(None)
        ).count()
        
        # Access control compliance (failed attempts)
        failed_access = self.session.query(AuditLog).filter(
            AuditLog.created_at >= start_date,
            AuditLog.created_at <= end_date,
            AuditLog.status == 'failure'
        )
        if hostel_id:
            failed_access = failed_access.filter(AuditLog.hostel_id == hostel_id)
        failed_access_count = failed_access.count()
        
        # Admin oversight compliance
        overrides_requiring_approval = self.session.query(AdminOverrideLog).filter(
            AdminOverrideLog.created_at >= start_date,
            AdminOverrideLog.created_at <= end_date,
            AdminOverrideLog.requires_approval == True
        )
        if hostel_id:
            overrides_requiring_approval = overrides_requiring_approval.filter(
                AdminOverrideLog.hostel_id == hostel_id
            )
        
        total_overrides_needing_approval = overrides_requiring_approval.count()
        approved_overrides = overrides_requiring_approval.filter(
            AdminOverrideLog.approved_at.isnot(None)
        ).count()
        
        approval_compliance_rate = (
            approved_overrides / total_overrides_needing_approval * 100
            if total_overrides_needing_approval else 100
        )
        
        return {
            'report_metadata': {
                'period': {
                    'start': start_date.isoformat(),
                    'end': end_date.isoformat()
                },
                'hostel_id': str(hostel_id) if hostel_id else 'all',
                'compliance_framework': compliance_framework or 'all',
                'generated_at': datetime.utcnow().isoformat()
            },
            'audit_trail_compliance': {
                'total_audit_logs': total_audit_logs,
                'sensitive_data_logs': sensitive_audit_logs,
                'sensitive_percentage': round(sensitive_audit_logs / total_audit_logs * 100, 2) if total_audit_logs else 0,
                'compliance_status': 'compliant' if total_audit_logs > 0 else 'no_activity'
            },
            'data_change_tracking': {
                'total_changes': total_changes,
                'pii_changes': pii_changes,
                'pii_percentage': round(pii_changes / total_changes * 100, 2) if total_changes else 0,
                'compliance_status': 'compliant'
            },
            'data_retention': {
                'retention_violations': retention_violations,
                'compliance_status': 'compliant' if retention_violations == 0 else 'violations_found'
            },
            'access_control': {
                'failed_access_attempts': failed_access_count,
                'compliance_status': 'review_required' if failed_access_count > 100 else 'compliant'
            },
            'administrative_oversight': {
                'overrides_requiring_approval': total_overrides_needing_approval,
                'approved_overrides': approved_overrides,
                'approval_rate': round(approval_compliance_rate, 2),
                'compliance_status': 'compliant' if approval_compliance_rate >= 95 else 'review_required'
            },
            'overall_compliance_score': self._calculate_compliance_score(
                total_audit_logs,
                retention_violations,
                approval_compliance_rate
            )
        }
    
    # ==================== Performance Analytics ====================
    
    def get_system_performance_metrics(
        self,
        hostel_id: Optional[UUID] = None,
        days: int = 7
    ) -> Dict[str, Any]:
        """
        Get system performance metrics from audit data.
        
        Args:
            hostel_id: Optional hostel filter
            days: Number of days to analyze
            
        Returns:
            Performance metrics dictionary
        """
        start_date = datetime.utcnow() - timedelta(days=days)
        end_date = datetime.utcnow()
        
        # Action success rates
        audit_query = self.session.query(AuditLog).filter(
            AuditLog.created_at >= start_date,
            AuditLog.created_at <= end_date
        )
        if hostel_id:
            audit_query = audit_query.filter(AuditLog.hostel_id == hostel_id)
        
        total_actions = audit_query.count()
        successful_actions = audit_query.filter(AuditLog.status == 'success').count()
        
        # Supervisor performance
        supervisor_query = self.session.query(SupervisorActivityLog).filter(
            SupervisorActivityLog.created_at >= start_date,
            SupervisorActivityLog.created_at <= end_date
        )
        if hostel_id:
            supervisor_query = supervisor_query.filter(SupervisorActivityLog.hostel_id == hostel_id)
        
        total_supervisor_activities = supervisor_query.count()
        completed_activities = supervisor_query.filter(
            SupervisorActivityLog.status == 'completed'
        ).count()
        
        # Average efficiency scores
        avg_efficiency = self.session.query(
            func.avg(SupervisorActivityLog.efficiency_score)
        ).filter(
            SupervisorActivityLog.created_at >= start_date,
            SupervisorActivityLog.created_at <= end_date,
            SupervisorActivityLog.efficiency_score.isnot(None)
        )
        if hostel_id:
            avg_efficiency = avg_efficiency.filter(SupervisorActivityLog.hostel_id == hostel_id)
        avg_efficiency = avg_efficiency.scalar()
        
        # Average quality scores
        avg_quality = self.session.query(
            func.avg(SupervisorActivityLog.quality_score)
        ).filter(
            SupervisorActivityLog.created_at >= start_date,
            SupervisorActivityLog.created_at <= end_date,
            SupervisorActivityLog.quality_score.isnot(None)
        )
        if hostel_id:
            avg_quality = avg_quality.filter(SupervisorActivityLog.hostel_id == hostel_id)
        avg_quality = avg_quality.scalar()
        
        # Override impact
        avg_override_impact = self.session.query(
            func.avg(AdminOverrideLog.impact_score)
        ).filter(
            AdminOverrideLog.created_at >= start_date,
            AdminOverrideLog.created_at <= end_date
        )
        if hostel_id:
            avg_override_impact = avg_override_impact.filter(AdminOverrideLog.hostel_id == hostel_id)
        avg_override_impact = avg_override_impact.scalar()
        
        return {
            'period': {
                'start': start_date.isoformat(),
                'end': end_date.isoformat(),
                'days': days
            },
            'hostel_id': str(hostel_id) if hostel_id else 'all',
            'action_success_rate': round(successful_actions / total_actions * 100, 2) if total_actions else 0,
            'supervisor_completion_rate': round(completed_activities / total_supervisor_activities * 100, 2) if total_supervisor_activities else 0,
            'average_efficiency_score': round(float(avg_efficiency), 2) if avg_efficiency else 0,
            'average_quality_score': round(float(avg_quality), 2) if avg_quality else 0,
            'average_override_impact': round(float(avg_override_impact), 2) if avg_override_impact else 0,
            'performance_rating': self._calculate_performance_rating(
                successful_actions / total_actions * 100 if total_actions else 0,
                float(avg_efficiency) if avg_efficiency else 0,
                float(avg_quality) if avg_quality else 0
            )
        }
    
    # ==================== Helper Methods ====================
    
    def _calculate_security_score(
        self,
        failed_auth: int,
        sensitive_access: int,
        critical_events: int,
        requiring_review: int
    ) -> int:
        """
        Calculate overall security score (0-100).
        
        Args:
            failed_auth: Number of failed authentication attempts
            sensitive_access: Number of sensitive data access
            critical_events: Number of critical severity events
            requiring_review: Number of events requiring review
            
        Returns:
            Security score (0-100, higher is better)
        """
        # Start with perfect score
        score = 100
        
        # Deduct for security issues
        score -= min(failed_auth * 2, 30)  # Max 30 points for failed auth
        score -= min(critical_events * 5, 25)  # Max 25 points for critical events
        score -= min(requiring_review * 1, 20)  # Max 20 points for reviews needed
        
        # Bonus for low sensitive access (within reason)
        if sensitive_access < 10:
            score += 5
        
        return max(0, min(100, score))
    
    def _calculate_compliance_score(
        self,
        total_logs: int,
        retention_violations: int,
        approval_rate: float
    ) -> int:
        """
        Calculate overall compliance score (0-100).
        
        Args:
            total_logs: Total audit logs
            retention_violations: Number of retention policy violations
            approval_rate: Admin override approval rate
            
        Returns:
            Compliance score (0-100)
        """
        score = 100
        
        # Deduct for violations
        score -= min(retention_violations * 10, 40)
        
        # Deduct based on approval rate
        if approval_rate < 95:
            score -= (95 - approval_rate) * 2
        
        # Bonus for comprehensive logging
        if total_logs > 1000:
            score += 10
        
        return max(0, min(100, int(score)))
    
    def _calculate_performance_rating(
        self,
        success_rate: float,
        efficiency: float,
        quality: float
    ) -> str:
        """
        Calculate overall performance rating.
        
        Args:
            success_rate: Action success rate percentage
            efficiency: Average efficiency score
            quality: Average quality score
            
        Returns:
            Performance rating string
        """
        # Weighted average
        score = (success_rate * 0.4) + (efficiency * 0.3) + (quality * 20 * 0.3)
        
        if score >= 90:
            return 'excellent'
        elif score >= 75:
            return 'good'
        elif score >= 60:
            return 'satisfactory'
        elif score >= 45:
            return 'needs_improvement'
        else:
            return 'poor'

# --- File: C:\Hostel-Main\app\repositories\audit\audit_log_repository.py ---
"""
Audit log repository for comprehensive system activity tracking.

Provides advanced querying, filtering, and analytics for audit logs
with performance optimization and compliance features.
"""

from datetime import datetime, timedelta
from typing import Any, Dict, List, Optional, Tuple
from uuid import UUID

from sqlalchemy import and_, or_, func, desc, asc
from sqlalchemy.orm import Session, joinedload

from app.models.audit import AuditLog
from app.repositories.base.base_repository import BaseRepository
from app.repositories.base.query_builder import QueryBuilder
from app.repositories.base.specifications import Specification
from app.repositories.base.pagination import PaginationManager
from app.repositories.base.filtering import FilteringEngine
from app.schemas.common.enums import AuditActionCategory


class AuditLogRepository(BaseRepository):
    """
    Repository for comprehensive audit log management.
    
    Provides advanced querying, analytics, and compliance features
    for system-wide audit trail tracking.
    """
    
    def __init__(self, session: Session):
        """Initialize repository with database session."""
        super().__init__(session, AuditLog)
    
    # ==================== CRUD Operations ====================
    
    def create_audit_log(
        self,
        user_id: Optional[UUID],
        action_type: str,
        action_category: AuditActionCategory,
        action_description: str,
        entity_type: Optional[str] = None,
        entity_id: Optional[UUID] = None,
        entity_name: Optional[str] = None,
        old_values: Optional[Dict] = None,
        new_values: Optional[Dict] = None,
        status: str = "success",
        context_metadata: Optional[Dict] = None,
        **kwargs
    ) -> AuditLog:
        """
        Create a new audit log entry.
        
        Args:
            user_id: User who performed the action
            action_type: Specific action identifier
            action_category: High-level action category
            action_description: Human-readable description
            entity_type: Type of entity affected
            entity_id: ID of affected entity
            entity_name: Display name of entity
            old_values: Previous values (for updates/deletes)
            new_values: New values (for creates/updates)
            status: Action outcome status
            context_metadata: Additional context
            **kwargs: Additional fields
            
        Returns:
            Created AuditLog instance
        """
        audit_log = AuditLog(
            user_id=user_id,
            action_type=action_type,
            action_category=action_category,
            action_description=action_description,
            entity_type=entity_type,
            entity_id=entity_id,
            entity_name=entity_name,
            old_values=old_values or {},
            new_values=new_values or {},
            status=status,
            context_metadata=context_metadata or {},
            **kwargs
        )
        
        # Auto-calculate severity level if not provided
        if 'severity_level' not in kwargs:
            audit_log.severity_level = self._calculate_severity(
                action_category, status, entity_type
            )
        
        # Determine if review is required
        if 'requires_review' not in kwargs:
            audit_log.requires_review = self._requires_review(
                action_category, audit_log.severity_level, status
            )
        
        return self.create(audit_log)
    
    def bulk_create_audit_logs(
        self,
        audit_logs: List[Dict[str, Any]],
        batch_size: int = 1000
    ) -> List[AuditLog]:
        """
        Bulk create audit log entries with batch processing.
        
        Args:
            audit_logs: List of audit log data dictionaries
            batch_size: Number of records per batch
            
        Returns:
            List of created AuditLog instances
        """
        created_logs = []
        
        for i in range(0, len(audit_logs), batch_size):
            batch = audit_logs[i:i + batch_size]
            log_objects = [AuditLog(**log_data) for log_data in batch]
            
            self.session.bulk_save_objects(log_objects, return_defaults=True)
            self.session.flush()
            
            created_logs.extend(log_objects)
        
        self.session.commit()
        return created_logs
    
    # ==================== Query Operations ====================
    
    def find_by_user(
        self,
        user_id: UUID,
        start_date: Optional[datetime] = None,
        end_date: Optional[datetime] = None,
        action_category: Optional[AuditActionCategory] = None,
        limit: int = 100,
        offset: int = 0
    ) -> Tuple[List[AuditLog], int]:
        """
        Find audit logs by user with optional filtering.
        
        Args:
            user_id: User ID to filter by
            start_date: Start date for filtering
            end_date: End date for filtering
            action_category: Optional action category filter
            limit: Maximum number of results
            offset: Number of results to skip
            
        Returns:
            Tuple of (audit logs, total count)
        """
        query = self.session.query(AuditLog).filter(
            AuditLog.user_id == user_id
        )
        
        if start_date:
            query = query.filter(AuditLog.created_at >= start_date)
        
        if end_date:
            query = query.filter(AuditLog.created_at <= end_date)
        
        if action_category:
            query = query.filter(AuditLog.action_category == action_category)
        
        total = query.count()
        
        results = query.order_by(desc(AuditLog.created_at))\
            .limit(limit)\
            .offset(offset)\
            .all()
        
        return results, total
    
    def find_by_entity(
        self,
        entity_type: str,
        entity_id: UUID,
        include_related: bool = True,
        limit: int = 100
    ) -> List[AuditLog]:
        """
        Find all audit logs for a specific entity.
        
        Args:
            entity_type: Type of entity
            entity_id: Entity ID
            include_related: Include related entity logs
            limit: Maximum number of results
            
        Returns:
            List of audit logs
        """
        query = self.session.query(AuditLog)
        
        if include_related:
            query = query.filter(
                or_(
                    and_(
                        AuditLog.entity_type == entity_type,
                        AuditLog.entity_id == entity_id
                    ),
                    and_(
                        AuditLog.related_entity_type == entity_type,
                        AuditLog.related_entity_id == entity_id
                    )
                )
            )
        else:
            query = query.filter(
                AuditLog.entity_type == entity_type,
                AuditLog.entity_id == entity_id
            )
        
        return query.order_by(desc(AuditLog.created_at))\
            .limit(limit)\
            .all()
    
    def find_by_hostel(
        self,
        hostel_id: UUID,
        start_date: Optional[datetime] = None,
        end_date: Optional[datetime] = None,
        action_categories: Optional[List[AuditActionCategory]] = None,
        severity_levels: Optional[List[str]] = None,
        limit: int = 100,
        offset: int = 0
    ) -> Tuple[List[AuditLog], int]:
        """
        Find audit logs by hostel with advanced filtering.
        
        Args:
            hostel_id: Hostel ID to filter by
            start_date: Start date filter
            end_date: End date filter
            action_categories: List of action categories to include
            severity_levels: List of severity levels to include
            limit: Maximum results
            offset: Results to skip
            
        Returns:
            Tuple of (audit logs, total count)
        """
        query = self.session.query(AuditLog).filter(
            AuditLog.hostel_id == hostel_id
        )
        
        if start_date:
            query = query.filter(AuditLog.created_at >= start_date)
        
        if end_date:
            query = query.filter(AuditLog.created_at <= end_date)
        
        if action_categories:
            query = query.filter(AuditLog.action_category.in_(action_categories))
        
        if severity_levels:
            query = query.filter(AuditLog.severity_level.in_(severity_levels))
        
        total = query.count()
        
        results = query.order_by(desc(AuditLog.created_at))\
            .limit(limit)\
            .offset(offset)\
            .all()
        
        return results, total
    
    def find_by_action_type(
        self,
        action_type: str,
        start_date: Optional[datetime] = None,
        end_date: Optional[datetime] = None,
        status: Optional[str] = None,
        limit: int = 100
    ) -> List[AuditLog]:
        """
        Find audit logs by action type.
        
        Args:
            action_type: Action type to filter by
            start_date: Start date filter
            end_date: End date filter
            status: Optional status filter
            limit: Maximum results
            
        Returns:
            List of audit logs
        """
        query = self.session.query(AuditLog).filter(
            AuditLog.action_type == action_type
        )
        
        if start_date:
            query = query.filter(AuditLog.created_at >= start_date)
        
        if end_date:
            query = query.filter(AuditLog.created_at <= end_date)
        
        if status:
            query = query.filter(AuditLog.status == status)
        
        return query.order_by(desc(AuditLog.created_at))\
            .limit(limit)\
            .all()
    
    def find_failed_actions(
        self,
        start_date: Optional[datetime] = None,
        end_date: Optional[datetime] = None,
        user_id: Optional[UUID] = None,
        limit: int = 100
    ) -> List[AuditLog]:
        """
        Find all failed actions for error analysis.
        
        Args:
            start_date: Start date filter
            end_date: End date filter
            user_id: Optional user filter
            limit: Maximum results
            
        Returns:
            List of failed audit logs
        """
        query = self.session.query(AuditLog).filter(
            AuditLog.status == 'failure'
        )
        
        if start_date:
            query = query.filter(AuditLog.created_at >= start_date)
        
        if end_date:
            query = query.filter(AuditLog.created_at <= end_date)
        
        if user_id:
            query = query.filter(AuditLog.user_id == user_id)
        
        return query.order_by(desc(AuditLog.created_at))\
            .limit(limit)\
            .all()
    
    def find_sensitive_actions(
        self,
        start_date: Optional[datetime] = None,
        end_date: Optional[datetime] = None,
        user_id: Optional[UUID] = None,
        limit: int = 100
    ) -> List[AuditLog]:
        """
        Find actions marked as sensitive for security review.
        
        Args:
            start_date: Start date filter
            end_date: End date filter
            user_id: Optional user filter
            limit: Maximum results
            
        Returns:
            List of sensitive audit logs
        """
        query = self.session.query(AuditLog).filter(
            AuditLog.is_sensitive == True
        )
        
        if start_date:
            query = query.filter(AuditLog.created_at >= start_date)
        
        if end_date:
            query = query.filter(AuditLog.created_at <= end_date)
        
        if user_id:
            query = query.filter(AuditLog.user_id == user_id)
        
        return query.order_by(desc(AuditLog.created_at))\
            .limit(limit)\
            .all()
    
    def find_requiring_review(
        self,
        severity_level: Optional[str] = None,
        action_category: Optional[AuditActionCategory] = None,
        limit: int = 100
    ) -> List[AuditLog]:
        """
        Find audit logs that require manual review.
        
        Args:
            severity_level: Optional severity filter
            action_category: Optional category filter
            limit: Maximum results
            
        Returns:
            List of audit logs requiring review
        """
        query = self.session.query(AuditLog).filter(
            AuditLog.requires_review == True
        )
        
        if severity_level:
            query = query.filter(AuditLog.severity_level == severity_level)
        
        if action_category:
            query = query.filter(AuditLog.action_category == action_category)
        
        return query.order_by(desc(AuditLog.created_at))\
            .limit(limit)\
            .all()
    
    def find_by_ip_address(
        self,
        ip_address: str,
        start_date: Optional[datetime] = None,
        end_date: Optional[datetime] = None,
        limit: int = 100
    ) -> List[AuditLog]:
        """
        Find audit logs by IP address for security analysis.
        
        Args:
            ip_address: IP address to search
            start_date: Start date filter
            end_date: End date filter
            limit: Maximum results
            
        Returns:
            List of audit logs from IP
        """
        query = self.session.query(AuditLog).filter(
            AuditLog.ip_address == ip_address
        )
        
        if start_date:
            query = query.filter(AuditLog.created_at >= start_date)
        
        if end_date:
            query = query.filter(AuditLog.created_at <= end_date)
        
        return query.order_by(desc(AuditLog.created_at))\
            .limit(limit)\
            .all()
    
    def find_by_request_id(
        self,
        request_id: str
    ) -> List[AuditLog]:
        """
        Find all audit logs for a specific request for tracing.
        
        Args:
            request_id: Request/trace ID
            
        Returns:
            List of audit logs for the request
        """
        return self.session.query(AuditLog).filter(
            AuditLog.request_id == request_id
        ).order_by(AuditLog.created_at).all()
    
    def find_by_session_id(
        self,
        session_id: str,
        limit: int = 100
    ) -> List[AuditLog]:
        """
        Find all audit logs for a user session.
        
        Args:
            session_id: User session ID
            limit: Maximum results
            
        Returns:
            List of audit logs for the session
        """
        return self.session.query(AuditLog).filter(
            AuditLog.session_id == session_id
        ).order_by(AuditLog.created_at)\
            .limit(limit)\
            .all()
    
    # ==================== Analytics Operations ====================
    
    def get_action_statistics(
        self,
        start_date: datetime,
        end_date: datetime,
        hostel_id: Optional[UUID] = None,
        user_id: Optional[UUID] = None
    ) -> Dict[str, Any]:
        """
        Get comprehensive action statistics for a period.
        
        Args:
            start_date: Period start date
            end_date: Period end date
            hostel_id: Optional hostel filter
            user_id: Optional user filter
            
        Returns:
            Dictionary with statistics
        """
        query = self.session.query(AuditLog).filter(
            AuditLog.created_at >= start_date,
            AuditLog.created_at <= end_date
        )
        
        if hostel_id:
            query = query.filter(AuditLog.hostel_id == hostel_id)
        
        if user_id:
            query = query.filter(AuditLog.user_id == user_id)
        
        total_actions = query.count()
        
        # Actions by status
        status_counts = self.session.query(
            AuditLog.status,
            func.count(AuditLog.id)
        ).filter(
            AuditLog.created_at >= start_date,
            AuditLog.created_at <= end_date
        )
        
        if hostel_id:
            status_counts = status_counts.filter(AuditLog.hostel_id == hostel_id)
        if user_id:
            status_counts = status_counts.filter(AuditLog.user_id == user_id)
        
        status_counts = status_counts.group_by(AuditLog.status).all()
        
        # Actions by category
        category_counts = self.session.query(
            AuditLog.action_category,
            func.count(AuditLog.id)
        ).filter(
            AuditLog.created_at >= start_date,
            AuditLog.created_at <= end_date
        )
        
        if hostel_id:
            category_counts = category_counts.filter(AuditLog.hostel_id == hostel_id)
        if user_id:
            category_counts = category_counts.filter(AuditLog.user_id == user_id)
        
        category_counts = category_counts.group_by(AuditLog.action_category).all()
        
        # Actions by severity
        severity_counts = self.session.query(
            AuditLog.severity_level,
            func.count(AuditLog.id)
        ).filter(
            AuditLog.created_at >= start_date,
            AuditLog.created_at <= end_date
        )
        
        if hostel_id:
            severity_counts = severity_counts.filter(AuditLog.hostel_id == hostel_id)
        if user_id:
            severity_counts = severity_counts.filter(AuditLog.user_id == user_id)
        
        severity_counts = severity_counts.group_by(AuditLog.severity_level).all()
        
        # Top users
        top_users = self.session.query(
            AuditLog.user_id,
            AuditLog.user_email,
            func.count(AuditLog.id).label('action_count')
        ).filter(
            AuditLog.created_at >= start_date,
            AuditLog.created_at <= end_date
        )
        
        if hostel_id:
            top_users = top_users.filter(AuditLog.hostel_id == hostel_id)
        
        top_users = top_users.group_by(
            AuditLog.user_id,
            AuditLog.user_email
        ).order_by(desc('action_count')).limit(10).all()
        
        # Top action types
        top_actions = self.session.query(
            AuditLog.action_type,
            func.count(AuditLog.id).label('count')
        ).filter(
            AuditLog.created_at >= start_date,
            AuditLog.created_at <= end_date
        )
        
        if hostel_id:
            top_actions = top_actions.filter(AuditLog.hostel_id == hostel_id)
        if user_id:
            top_actions = top_actions.filter(AuditLog.user_id == user_id)
        
        top_actions = top_actions.group_by(AuditLog.action_type)\
            .order_by(desc('count'))\
            .limit(10)\
            .all()
        
        return {
            'total_actions': total_actions,
            'by_status': dict(status_counts),
            'by_category': {cat.value: count for cat, count in category_counts},
            'by_severity': dict(severity_counts),
            'top_users': [
                {'user_id': str(uid), 'email': email, 'count': count}
                for uid, email, count in top_users
            ],
            'top_actions': [
                {'action_type': action, 'count': count}
                for action, count in top_actions
            ],
            'period': {
                'start': start_date.isoformat(),
                'end': end_date.isoformat()
            }
        }
    
    def get_user_activity_timeline(
        self,
        user_id: UUID,
        start_date: datetime,
        end_date: datetime,
        group_by: str = 'hour'  # 'hour', 'day', 'week'
    ) -> List[Dict[str, Any]]:
        """
        Get user activity timeline with time-based grouping.
        
        Args:
            user_id: User ID
            start_date: Period start
            end_date: Period end
            group_by: Grouping interval
            
        Returns:
            List of timeline data points
        """
        if group_by == 'hour':
            time_format = func.date_trunc('hour', AuditLog.created_at)
        elif group_by == 'day':
            time_format = func.date_trunc('day', AuditLog.created_at)
        elif group_by == 'week':
            time_format = func.date_trunc('week', AuditLog.created_at)
        else:
            time_format = func.date_trunc('day', AuditLog.created_at)
        
        timeline = self.session.query(
            time_format.label('time_bucket'),
            func.count(AuditLog.id).label('action_count'),
            AuditLog.action_category
        ).filter(
            AuditLog.user_id == user_id,
            AuditLog.created_at >= start_date,
            AuditLog.created_at <= end_date
        ).group_by(
            'time_bucket',
            AuditLog.action_category
        ).order_by('time_bucket').all()
        
        return [
            {
                'timestamp': bucket.isoformat(),
                'count': count,
                'category': category.value
            }
            for bucket, count, category in timeline
        ]
    
    def get_entity_change_summary(
        self,
        entity_type: str,
        entity_id: UUID
    ) -> Dict[str, Any]:
        """
        Get comprehensive change summary for an entity.
        
        Args:
            entity_type: Entity type
            entity_id: Entity ID
            
        Returns:
            Dictionary with change summary
        """
        logs = self.find_by_entity(entity_type, entity_id, include_related=False)
        
        if not logs:
            return {
                'entity_type': entity_type,
                'entity_id': str(entity_id),
                'total_changes': 0,
                'first_change': None,
                'last_change': None,
                'changes_by_user': {},
                'changes_by_type': {}
            }
        
        # Changes by user
        changes_by_user = {}
        for log in logs:
            user_key = log.user_email or str(log.user_id) if log.user_id else 'system'
            changes_by_user[user_key] = changes_by_user.get(user_key, 0) + 1
        
        # Changes by action type
        changes_by_type = {}
        for log in logs:
            changes_by_type[log.action_type] = changes_by_type.get(log.action_type, 0) + 1
        
        return {
            'entity_type': entity_type,
            'entity_id': str(entity_id),
            'entity_name': logs[0].entity_name if logs else None,
            'total_changes': len(logs),
            'first_change': logs[-1].created_at.isoformat() if logs else None,
            'last_change': logs[0].created_at.isoformat() if logs else None,
            'changes_by_user': changes_by_user,
            'changes_by_type': changes_by_type,
            'recent_changes': [
                {
                    'action_type': log.action_type,
                    'user': log.user_email,
                    'timestamp': log.created_at.isoformat(),
                    'status': log.status
                }
                for log in logs[:10]  # Last 10 changes
            ]
        }
    
    def get_security_events(
        self,
        start_date: datetime,
        end_date: datetime,
        severity_threshold: str = 'medium',
        hostel_id: Optional[UUID] = None
    ) -> List[AuditLog]:
        """
        Get security-related events for monitoring.
        
        Args:
            start_date: Period start
            end_date: Period end
            severity_threshold: Minimum severity level
            hostel_id: Optional hostel filter
            
        Returns:
            List of security-related audit logs
        """
        severity_order = {'info': 0, 'low': 1, 'medium': 2, 'high': 3, 'critical': 4}
        threshold_value = severity_order.get(severity_threshold, 2)
        
        query = self.session.query(AuditLog).filter(
            AuditLog.created_at >= start_date,
            AuditLog.created_at <= end_date,
            or_(
                AuditLog.is_sensitive == True,
                AuditLog.status == 'failure',
                AuditLog.requires_review == True
            )
        )
        
        if hostel_id:
            query = query.filter(AuditLog.hostel_id == hostel_id)
        
        results = query.order_by(desc(AuditLog.created_at)).all()
        
        # Filter by severity threshold
        filtered_results = [
            log for log in results
            if severity_order.get(log.severity_level, 0) >= threshold_value
        ]
        
        return filtered_results
    
    def get_compliance_report(
        self,
        start_date: datetime,
        end_date: datetime,
        compliance_tags: Optional[List[str]] = None,
        hostel_id: Optional[UUID] = None
    ) -> Dict[str, Any]:
        """
        Generate compliance report for audit purposes.
        
        Args:
            start_date: Report period start
            end_date: Report period end
            compliance_tags: Optional compliance framework tags
            hostel_id: Optional hostel filter
            
        Returns:
            Compliance report dictionary
        """
        query = self.session.query(AuditLog).filter(
            AuditLog.created_at >= start_date,
            AuditLog.created_at <= end_date
        )
        
        if hostel_id:
            query = query.filter(AuditLog.hostel_id == hostel_id)
        
        if compliance_tags:
            # Filter by compliance tags in JSONB array
            query = query.filter(
                AuditLog.compliance_tags.contains(compliance_tags)
            )
        
        all_logs = query.all()
        
        # Sensitive data access logs
        sensitive_access = [log for log in all_logs if log.is_sensitive]
        
        # Failed actions
        failed_actions = [log for log in all_logs if log.status == 'failure']
        
        # Critical severity actions
        critical_actions = [log for log in all_logs if log.severity_level == 'critical']
        
        # Actions by category
        category_breakdown = {}
        for log in all_logs:
            cat = log.action_category.value
            category_breakdown[cat] = category_breakdown.get(cat, 0) + 1
        
        return {
            'report_period': {
                'start': start_date.isoformat(),
                'end': end_date.isoformat()
            },
            'total_actions': len(all_logs),
            'sensitive_data_access': {
                'count': len(sensitive_access),
                'percentage': len(sensitive_access) / len(all_logs) * 100 if all_logs else 0
            },
            'failed_actions': {
                'count': len(failed_actions),
                'percentage': len(failed_actions) / len(all_logs) * 100 if all_logs else 0
            },
            'critical_actions': {
                'count': len(critical_actions),
                'percentage': len(critical_actions) / len(all_logs) * 100 if all_logs else 0
            },
            'actions_by_category': category_breakdown,
            'compliance_tags': compliance_tags or [],
            'hostel_id': str(hostel_id) if hostel_id else 'all'
        }
    
    # ==================== Maintenance Operations ====================
    
    def archive_old_logs(
        self,
        cutoff_date: datetime,
        batch_size: int = 1000
    ) -> int:
        """
        Archive audit logs older than cutoff date.
        
        Args:
            cutoff_date: Date before which logs should be archived
            batch_size: Number of records per batch
            
        Returns:
            Number of archived records
        """
        # This would typically move records to an archive table
        # For now, we'll just count what would be archived
        count = self.session.query(AuditLog).filter(
            AuditLog.created_at < cutoff_date
        ).count()
        
        # TODO: Implement actual archival to archive table or file storage
        
        return count
    
    def cleanup_by_retention_policy(
        self,
        default_retention_days: int = 365
    ) -> int:
        """
        Clean up logs based on retention policies.
        
        Args:
            default_retention_days: Default retention in days
            
        Returns:
            Number of deleted records
        """
        cutoff_date = datetime.utcnow() - timedelta(days=default_retention_days)
        
        # Find logs eligible for deletion
        eligible_logs = self.session.query(AuditLog).filter(
            or_(
                and_(
                    AuditLog.retention_days.isnot(None),
                    AuditLog.created_at < func.now() - func.cast(
                        func.concat(AuditLog.retention_days, ' days'),
                        type_=type(timedelta())
                    )
                ),
                and_(
                    AuditLog.retention_days.is_(None),
                    AuditLog.created_at < cutoff_date,
                    AuditLog.is_sensitive == False  # Keep sensitive logs longer
                )
            )
        ).all()
        
        count = len(eligible_logs)
        
        # Soft delete or hard delete based on requirements
        for log in eligible_logs:
            self.session.delete(log)
        
        self.session.commit()
        
        return count
    
    # ==================== Helper Methods ====================
    
    def _calculate_severity(
        self,
        action_category: AuditActionCategory,
        status: str,
        entity_type: Optional[str]
    ) -> str:
        """Calculate severity level based on action characteristics."""
        # Critical severity
        if status == 'failure' and action_category in [
            AuditActionCategory.SECURITY,
            AuditActionCategory.DATA_ACCESS
        ]:
            return 'critical'
        
        # High severity
        if action_category in [
            AuditActionCategory.SECURITY,
            AuditActionCategory.ADMIN_ACTION,
            AuditActionCategory.FINANCIAL
        ]:
            return 'high'
        
        # Medium severity
        if action_category in [
            AuditActionCategory.DATA_CHANGE,
            AuditActionCategory.SYSTEM_CONFIG
        ]:
            return 'medium'
        
        # Low severity
        if action_category in [
            AuditActionCategory.DATA_ACCESS,
            AuditActionCategory.USER_ACTION
        ]:
            return 'low'
        
        # Default to info
        return 'info'
    
    def _requires_review(
        self,
        action_category: AuditActionCategory,
        severity_level: str,
        status: str
    ) -> bool:
        """Determine if action requires manual review."""
        # Always review critical severity
        if severity_level == 'critical':
            return True
        
        # Review failed security actions
        if status == 'failure' and action_category == AuditActionCategory.SECURITY:
            return True
        
        # Review high severity admin actions
        if severity_level == 'high' and action_category == AuditActionCategory.ADMIN_ACTION:
            return True
        
        return False
    
    def get_audit_trail(
        self,
        entity_type: str,
        entity_id: UUID,
        include_metadata: bool = False
    ) -> List[Dict[str, Any]]:
        """
        Get formatted audit trail for an entity.
        
        Args:
            entity_type: Entity type
            entity_id: Entity ID
            include_metadata: Include full metadata
            
        Returns:
            List of formatted audit trail entries
        """
        logs = self.find_by_entity(entity_type, entity_id)
        
        trail = []
        for log in logs:
            entry = {
                'timestamp': log.created_at.isoformat(),
                'action': log.action_type,
                'description': log.action_description,
                'user': log.user_email or str(log.user_id) if log.user_id else 'System',
                'status': log.status,
                'severity': log.severity_level
            }
            
            if include_metadata:
                entry['old_values'] = log.old_values
                entry['new_values'] = log.new_values
                entry['context'] = log.context_metadata
                entry['ip_address'] = str(log.ip_address) if log.ip_address else None
            
            trail.append(entry)
        
        return trail

# --- File: C:\Hostel-Main\app\repositories\audit\entity_change_log_repository.py ---
"""
Entity change log repository for detailed field-level change tracking.

Provides granular change history with field-level tracking,
versioning, and comprehensive audit capabilities.
"""

from datetime import datetime, timedelta
from typing import Any, Dict, List, Optional, Tuple
from uuid import UUID

from sqlalchemy import and_, or_, func, desc
from sqlalchemy.orm import Session

from app.models.audit import EntityChangeLog, EntityChangeHistory
from app.repositories.base.base_repository import BaseRepository


class EntityChangeLogRepository(BaseRepository):
    """
    Repository for detailed entity change tracking.
    
    Provides field-level change history, versioning,
    and comprehensive audit capabilities.
    """
    
    def __init__(self, session: Session):
        """Initialize repository with database session."""
        super().__init__(session, EntityChangeLog)
    
    # ==================== CRUD Operations ====================
    
    def create_change_log(
        self,
        entity_type: str,
        entity_id: UUID,
        field_name: str,
        old_value: Any,
        new_value: Any,
        change_type: str,
        changed_by: Optional[UUID] = None,
        change_reason: Optional[str] = None,
        **kwargs
    ) -> EntityChangeLog:
        """
        Create a new change log entry.
        
        Args:
            entity_type: Type of entity
            entity_id: Entity ID
            field_name: Field that changed
            old_value: Previous value
            new_value: New value
            change_type: Type of change (created, updated, deleted, etc.)
            changed_by: User who made the change
            change_reason: Reason for change
            **kwargs: Additional fields
            
        Returns:
            Created EntityChangeLog instance
        """
        change_log = EntityChangeLog(
            entity_type=entity_type,
            entity_id=entity_id,
            field_name=field_name,
            old_value=old_value,
            new_value=new_value,
            change_type=change_type,
            changed_by_user_id=changed_by,
            change_reason=change_reason,
            **kwargs
        )
        
        # Auto-calculate impact score
        change_log.calculate_impact_score()
        
        return self.create(change_log)
    
    def bulk_create_changes(
        self,
        changes: List[Dict[str, Any]],
        batch_size: int = 1000
    ) -> List[EntityChangeLog]:
        """
        Bulk create change log entries.
        
        Args:
            changes: List of change dictionaries
            batch_size: Batch size for processing
            
        Returns:
            List of created change logs
        """
        created_logs = []
        
        for i in range(0, len(changes), batch_size):
            batch = changes[i:i + batch_size]
            log_objects = [EntityChangeLog(**change) for change in batch]
            
            # Calculate impact scores
            for log in log_objects:
                log.calculate_impact_score()
            
            self.session.bulk_save_objects(log_objects, return_defaults=True)
            self.session.flush()
            
            created_logs.extend(log_objects)
        
        self.session.commit()
        return created_logs
    
    # ==================== Query Operations ====================
    
    def get_entity_history(
        self,
        entity_type: str,
        entity_id: UUID,
        field_name: Optional[str] = None,
        only_valid: bool = True,
        limit: int = 100
    ) -> List[EntityChangeLog]:
        """
        Get change history for an entity.
        
        Args:
            entity_type: Type of entity
            entity_id: Entity ID
            field_name: Optional specific field
            only_valid: Only return valid (non-invalidated) changes
            limit: Maximum results
            
        Returns:
            List of change logs
        """
        query = self.session.query(EntityChangeLog).filter(
            EntityChangeLog.entity_type == entity_type,
            EntityChangeLog.entity_id == entity_id
        )
        
        if field_name:
            query = query.filter(EntityChangeLog.field_name == field_name)
        
        if only_valid:
            query = query.filter(EntityChangeLog.is_valid == True)
        
        return query.order_by(desc(EntityChangeLog.created_at))\
            .limit(limit)\
            .all()
    
    def get_field_history(
        self,
        entity_type: str,
        entity_id: UUID,
        field_name: str,
        only_valid: bool = True
    ) -> List[Dict[str, Any]]:
        """
        Get complete history of changes for a specific field.
        
        Args:
            entity_type: Type of entity
            entity_id: Entity ID
            field_name: Field name
            only_valid: Only valid changes
            
        Returns:
            List of change timeline entries
        """
        changes = self.get_entity_history(
            entity_type=entity_type,
            entity_id=entity_id,
            field_name=field_name,
            only_valid=only_valid
        )
        
        return [
            {
                'timestamp': change.created_at.isoformat(),
                'old_value': change.old_value,
                'new_value': change.new_value,
                'changed_by': change.changed_by_user_name,
                'change_type': change.change_type,
                'reason': change.change_reason,
                'is_valid': change.is_valid
            }
            for change in changes
        ]
    
    def get_changes_by_user(
        self,
        user_id: UUID,
        entity_type: Optional[str] = None,
        start_date: Optional[datetime] = None,
        end_date: Optional[datetime] = None,
        limit: int = 100
    ) -> List[EntityChangeLog]:
        """
        Get all changes made by a specific user.
        
        Args:
            user_id: User ID
            entity_type: Optional entity type filter
            start_date: Start date filter
            end_date: End date filter
            limit: Maximum results
            
        Returns:
            List of change logs
        """
        query = self.session.query(EntityChangeLog).filter(
            EntityChangeLog.changed_by_user_id == user_id
        )
        
        if entity_type:
            query = query.filter(EntityChangeLog.entity_type == entity_type)
        
        if start_date:
            query = query.filter(EntityChangeLog.created_at >= start_date)
        
        if end_date:
            query = query.filter(EntityChangeLog.created_at <= end_date)
        
        return query.order_by(desc(EntityChangeLog.created_at))\
            .limit(limit)\
            .all()
    
    def get_changes_requiring_review(
        self,
        entity_type: Optional[str] = None,
        reviewed: bool = False,
        limit: int = 100
    ) -> List[EntityChangeLog]:
        """
        Get changes that require or have been reviewed.
        
        Args:
            entity_type: Optional entity type filter
            reviewed: True for reviewed, False for pending review
            limit: Maximum results
            
        Returns:
            List of change logs
        """
        query = self.session.query(EntityChangeLog).filter(
            EntityChangeLog.requires_review == True
        )
        
        if entity_type:
            query = query.filter(EntityChangeLog.entity_type == entity_type)
        
        if reviewed:
            query = query.filter(EntityChangeLog.reviewed_at.isnot(None))
        else:
            query = query.filter(EntityChangeLog.reviewed_at.is_(None))
        
        return query.order_by(desc(EntityChangeLog.created_at))\
            .limit(limit)\
            .all()
    
    def get_sensitive_changes(
        self,
        entity_type: Optional[str] = None,
        include_pii: bool = True,
        start_date: Optional[datetime] = None,
        limit: int = 100
    ) -> List[EntityChangeLog]:
        """
        Get sensitive data changes for security audit.
        
        Args:
            entity_type: Optional entity type filter
            include_pii: Include PII changes
            start_date: Start date filter
            limit: Maximum results
            
        Returns:
            List of sensitive change logs
        """
        filters = []
        
        if include_pii:
            filters.append(
                or_(
                    EntityChangeLog.is_sensitive == True,
                    EntityChangeLog.is_pii == True
                )
            )
        else:
            filters.append(EntityChangeLog.is_sensitive == True)
        
        if entity_type:
            filters.append(EntityChangeLog.entity_type == entity_type)
        
        if start_date:
            filters.append(EntityChangeLog.created_at >= start_date)
        
        query = self.session.query(EntityChangeLog).filter(and_(*filters))
        
        return query.order_by(desc(EntityChangeLog.created_at))\
            .limit(limit)\
            .all()
    
    def get_invalidated_changes(
        self,
        entity_type: Optional[str] = None,
        start_date: Optional[datetime] = None,
        limit: int = 100
    ) -> List[EntityChangeLog]:
        """
        Get invalidated (rolled back) changes.
        
        Args:
            entity_type: Optional entity type filter
            start_date: Start date filter
            limit: Maximum results
            
        Returns:
            List of invalidated change logs
        """
        query = self.session.query(EntityChangeLog).filter(
            EntityChangeLog.is_valid == False
        )
        
        if entity_type:
            query = query.filter(EntityChangeLog.entity_type == entity_type)
        
        if start_date:
            query = query.filter(EntityChangeLog.invalidated_at >= start_date)
        
        return query.order_by(desc(EntityChangeLog.invalidated_at))\
            .limit(limit)\
            .all()
    
    def find_by_change_source(
        self,
        source: str,
        entity_type: Optional[str] = None,
        start_date: Optional[datetime] = None,
        limit: int = 100
    ) -> List[EntityChangeLog]:
        """
        Find changes by their source (web, api, import, etc.).
        
        Args:
            source: Change source
            entity_type: Optional entity type filter
            start_date: Start date filter
            limit: Maximum results
            
        Returns:
            List of change logs
        """
        query = self.session.query(EntityChangeLog).filter(
            EntityChangeLog.change_source == source
        )
        
        if entity_type:
            query = query.filter(EntityChangeLog.entity_type == entity_type)
        
        if start_date:
            query = query.filter(EntityChangeLog.created_at >= start_date)
        
        return query.order_by(desc(EntityChangeLog.created_at))\
            .limit(limit)\
            .all()
    
    def find_by_request_id(
        self,
        request_id: str
    ) -> List[EntityChangeLog]:
        """
        Find all changes related to a specific request.
        
        Args:
            request_id: Request ID
            
        Returns:
            List of change logs
        """
        return self.session.query(EntityChangeLog).filter(
            EntityChangeLog.request_id == request_id
        ).order_by(EntityChangeLog.created_at).all()
    
    # ==================== Invalidation Operations ====================
    
    def invalidate_change(
        self,
        change_id: UUID,
        invalidated_by: UUID,
        reason: str
    ) -> EntityChangeLog:
        """
        Invalidate a change (mark as rolled back).
        
        Args:
            change_id: Change log ID
            invalidated_by: User performing invalidation
            reason: Reason for invalidation
            
        Returns:
            Updated change log
        """
        change = self.get_by_id(change_id)
        if not change:
            raise ValueError(f"Change log {change_id} not found")
        
        change.invalidate(invalidated_by=invalidated_by, reason=reason)
        self.session.commit()
        
        return change
    
    def bulk_invalidate_changes(
        self,
        change_ids: List[UUID],
        invalidated_by: UUID,
        reason: str
    ) -> int:
        """
        Invalidate multiple changes at once.
        
        Args:
            change_ids: List of change log IDs
            invalidated_by: User performing invalidation
            reason: Reason for invalidation
            
        Returns:
            Number of invalidated changes
        """
        count = 0
        for change_id in change_ids:
            try:
                self.invalidate_change(change_id, invalidated_by, reason)
                count += 1
            except ValueError:
                continue
        
        return count
    
    # ==================== Review Operations ====================
    
    def mark_reviewed(
        self,
        change_id: UUID,
        reviewed_by: UUID
    ) -> EntityChangeLog:
        """
        Mark a change as reviewed.
        
        Args:
            change_id: Change log ID
            reviewed_by: User who reviewed
            
        Returns:
            Updated change log
        """
        change = self.get_by_id(change_id)
        if not change:
            raise ValueError(f"Change log {change_id} not found")
        
        change.mark_reviewed(reviewed_by=reviewed_by)
        self.session.commit()
        
        return change
    
    def bulk_mark_reviewed(
        self,
        change_ids: List[UUID],
        reviewed_by: UUID
    ) -> int:
        """
        Mark multiple changes as reviewed.
        
        Args:
            change_ids: List of change log IDs
            reviewed_by: User who reviewed
            
        Returns:
            Number of marked changes
        """
        count = 0
        for change_id in change_ids:
            try:
                self.mark_reviewed(change_id, reviewed_by)
                count += 1
            except ValueError:
                continue
        
        return count
    
    # ==================== Analytics Operations ====================
    
    def get_change_statistics(
        self,
        entity_type: str,
        start_date: datetime,
        end_date: datetime
    ) -> Dict[str, Any]:
        """
        Get change statistics for an entity type.
        
        Args:
            entity_type: Entity type
            start_date: Period start
            end_date: Period end
            
        Returns:
            Statistics dictionary
        """
        query = self.session.query(EntityChangeLog).filter(
            EntityChangeLog.entity_type == entity_type,
            EntityChangeLog.created_at >= start_date,
            EntityChangeLog.created_at <= end_date
        )
        
        total_changes = query.count()
        
        # Changes by type
        change_types = self.session.query(
            EntityChangeLog.change_type,
            func.count(EntityChangeLog.id)
        ).filter(
            EntityChangeLog.entity_type == entity_type,
            EntityChangeLog.created_at >= start_date,
            EntityChangeLog.created_at <= end_date
        ).group_by(EntityChangeLog.change_type).all()
        
        # Most changed fields
        top_fields = self.session.query(
            EntityChangeLog.field_name,
            func.count(EntityChangeLog.id).label('count')
        ).filter(
            EntityChangeLog.entity_type == entity_type,
            EntityChangeLog.created_at >= start_date,
            EntityChangeLog.created_at <= end_date
        ).group_by(EntityChangeLog.field_name)\
            .order_by(desc('count'))\
            .limit(10)\
            .all()
        
        # Top contributors
        top_users = self.session.query(
            EntityChangeLog.changed_by_user_name,
            func.count(EntityChangeLog.id).label('count')
        ).filter(
            EntityChangeLog.entity_type == entity_type,
            EntityChangeLog.created_at >= start_date,
            EntityChangeLog.created_at <= end_date,
            EntityChangeLog.changed_by_user_name.isnot(None)
        ).group_by(EntityChangeLog.changed_by_user_name)\
            .order_by(desc('count'))\
            .limit(10)\
            .all()
        
        # Sensitive changes
        sensitive_count = query.filter(
            or_(
                EntityChangeLog.is_sensitive == True,
                EntityChangeLog.is_pii == True
            )
        ).count()
        
        # Invalid changes
        invalid_count = query.filter(
            EntityChangeLog.is_valid == False
        ).count()
        
        return {
            'entity_type': entity_type,
            'period': {
                'start': start_date.isoformat(),
                'end': end_date.isoformat()
            },
            'total_changes': total_changes,
            'by_change_type': dict(change_types),
            'top_changed_fields': [
                {'field': field, 'count': count}
                for field, count in top_fields
            ],
            'top_contributors': [
                {'user': user, 'count': count}
                for user, count in top_users
            ],
            'sensitive_changes': {
                'count': sensitive_count,
                'percentage': sensitive_count / total_changes * 100 if total_changes else 0
            },
            'invalidated_changes': {
                'count': invalid_count,
                'percentage': invalid_count / total_changes * 100 if total_changes else 0
            }
        }
    
    def get_field_change_frequency(
        self,
        entity_type: str,
        days: int = 30
    ) -> List[Dict[str, Any]]:
        """
        Get frequency of changes for each field.
        
        Args:
            entity_type: Entity type
            days: Number of days to analyze
            
        Returns:
            List of field change frequencies
        """
        start_date = datetime.utcnow() - timedelta(days=days)
        
        results = self.session.query(
            EntityChangeLog.field_name,
            func.count(EntityChangeLog.id).label('change_count'),
            func.count(func.distinct(EntityChangeLog.entity_id)).label('entity_count'),
            func.avg(EntityChangeLog.impact_score).label('avg_impact')
        ).filter(
            EntityChangeLog.entity_type == entity_type,
            EntityChangeLog.created_at >= start_date,
            EntityChangeLog.is_valid == True
        ).group_by(EntityChangeLog.field_name)\
            .order_by(desc('change_count'))\
            .all()
        
        return [
            {
                'field_name': field,
                'total_changes': change_count,
                'affected_entities': entity_count,
                'average_impact': float(avg_impact) if avg_impact else 0.0,
                'changes_per_entity': change_count / entity_count if entity_count else 0
            }
            for field, change_count, entity_count, avg_impact in results
        ]
    
    def get_change_velocity(
        self,
        entity_type: str,
        entity_id: UUID,
        days: int = 30
    ) -> Dict[str, Any]:
        """
        Calculate change velocity for an entity.
        
        Args:
            entity_type: Entity type
            entity_id: Entity ID
            days: Analysis period in days
            
        Returns:
            Change velocity metrics
        """
        start_date = datetime.utcnow() - timedelta(days=days)
        
        changes = self.session.query(EntityChangeLog).filter(
            EntityChangeLog.entity_type == entity_type,
            EntityChangeLog.entity_id == entity_id,
            EntityChangeLog.created_at >= start_date,
            EntityChangeLog.is_valid == True
        ).all()
        
        if not changes:
            return {
                'entity_type': entity_type,
                'entity_id': str(entity_id),
                'period_days': days,
                'total_changes': 0,
                'changes_per_day': 0.0,
                'unique_fields_changed': 0,
                'average_impact': 0.0
            }
        
        unique_fields = set(change.field_name for change in changes)
        avg_impact = sum(
            change.impact_score for change in changes if change.impact_score
        ) / len(changes)
        
        return {
            'entity_type': entity_type,
            'entity_id': str(entity_id),
            'period_days': days,
            'total_changes': len(changes),
            'changes_per_day': len(changes) / days,
            'unique_fields_changed': len(unique_fields),
            'average_impact': avg_impact,
            'most_changed_field': max(
                unique_fields,
                key=lambda f: sum(1 for c in changes if c.field_name == f)
            ) if unique_fields else None
        }
    
    def detect_anomalies(
        self,
        entity_type: str,
        threshold_multiplier: float = 3.0,
        days: int = 7
    ) -> List[Dict[str, Any]]:
        """
        Detect anomalous change patterns.
        
        Args:
            entity_type: Entity type to analyze
            threshold_multiplier: Standard deviations for anomaly
            days: Analysis period
            
        Returns:
            List of detected anomalies
        """
        start_date = datetime.utcnow() - timedelta(days=days)
        
        # Get change counts per entity
        entity_changes = self.session.query(
            EntityChangeLog.entity_id,
            EntityChangeLog.entity_display_name,
            func.count(EntityChangeLog.id).label('change_count')
        ).filter(
            EntityChangeLog.entity_type == entity_type,
            EntityChangeLog.created_at >= start_date,
            EntityChangeLog.is_valid == True
        ).group_by(
            EntityChangeLog.entity_id,
            EntityChangeLog.entity_display_name
        ).all()
        
        if len(entity_changes) < 2:
            return []
        
        # Calculate statistics
        change_counts = [count for _, _, count in entity_changes]
        mean = sum(change_counts) / len(change_counts)
        variance = sum((x - mean) ** 2 for x in change_counts) / len(change_counts)
        std_dev = variance ** 0.5
        
        threshold = mean + (threshold_multiplier * std_dev)
        
        # Find anomalies
        anomalies = []
        for entity_id, entity_name, count in entity_changes:
            if count > threshold:
                anomalies.append({
                    'entity_id': str(entity_id),
                    'entity_name': entity_name,
                    'change_count': count,
                    'expected_range': f"{mean:.2f}  {std_dev:.2f}",
                    'deviation': count - mean,
                    'severity': 'high' if count > mean + (5 * std_dev) else 'medium'
                })
        
        return sorted(anomalies, key=lambda x: x['deviation'], reverse=True)


class EntityChangeHistoryRepository(BaseRepository):
    """
    Repository for entity state snapshots and version history.
    
    Provides snapshot-based history tracking for complete
    entity state at different points in time.
    """
    
    def __init__(self, session: Session):
        """Initialize repository with database session."""
        super().__init__(session, EntityChangeHistory)
    
    # ==================== CRUD Operations ====================
    
    def create_snapshot(
        self,
        entity_type: str,
        entity_id: UUID,
        snapshot_data: Dict[str, Any],
        created_by: Optional[UUID] = None,
        change_summary: Optional[str] = None,
        tags: Optional[List[str]] = None
    ) -> EntityChangeHistory:
        """
        Create a new entity state snapshot.
        
        Args:
            entity_type: Entity type
            entity_id: Entity ID
            snapshot_data: Complete entity state
            created_by: User creating snapshot
            change_summary: Summary of changes
            tags: Tags for categorization
            
        Returns:
            Created snapshot
        """
        # Get current version number
        current_version = self.session.query(
            func.max(EntityChangeHistory.version_number)
        ).filter(
            EntityChangeHistory.entity_type == entity_type,
            EntityChangeHistory.entity_id == entity_id
        ).scalar() or 0
        
        # Mark previous version as not current
        self.session.query(EntityChangeHistory).filter(
            EntityChangeHistory.entity_type == entity_type,
            EntityChangeHistory.entity_id == entity_id,
            EntityChangeHistory.is_current_version == True
        ).update({'is_current_version': False})
        
        snapshot = EntityChangeHistory(
            entity_type=entity_type,
            entity_id=entity_id,
            snapshot_data=snapshot_data,
            snapshot_timestamp=datetime.utcnow(),
            version_number=current_version + 1,
            is_current_version=True,
            created_by=created_by,
            change_summary=change_summary,
            tags=tags or []
        )
        
        return self.create(snapshot)
    
    # ==================== Query Operations ====================
    
    def get_current_version(
        self,
        entity_type: str,
        entity_id: UUID
    ) -> Optional[EntityChangeHistory]:
        """
        Get current version snapshot.
        
        Args:
            entity_type: Entity type
            entity_id: Entity ID
            
        Returns:
            Current version snapshot or None
        """
        return self.session.query(EntityChangeHistory).filter(
            EntityChangeHistory.entity_type == entity_type,
            EntityChangeHistory.entity_id == entity_id,
            EntityChangeHistory.is_current_version == True
        ).first()
    
    def get_version_history(
        self,
        entity_type: str,
        entity_id: UUID,
        limit: int = 50
    ) -> List[EntityChangeHistory]:
        """
        Get version history for an entity.
        
        Args:
            entity_type: Entity type
            entity_id: Entity ID
            limit: Maximum versions to return
            
        Returns:
            List of version snapshots
        """
        return self.session.query(EntityChangeHistory).filter(
            EntityChangeHistory.entity_type == entity_type,
            EntityChangeHistory.entity_id == entity_id
        ).order_by(desc(EntityChangeHistory.version_number))\
            .limit(limit)\
            .all()
    
    def get_version_by_number(
        self,
        entity_type: str,
        entity_id: UUID,
        version_number: int
    ) -> Optional[EntityChangeHistory]:
        """
        Get specific version by number.
        
        Args:
            entity_type: Entity type
            entity_id: Entity ID
            version_number: Version number
            
        Returns:
            Version snapshot or None
        """
        return self.session.query(EntityChangeHistory).filter(
            EntityChangeHistory.entity_type == entity_type,
            EntityChangeHistory.entity_id == entity_id,
            EntityChangeHistory.version_number == version_number
        ).first()
    
    def get_version_at_time(
        self,
        entity_type: str,
        entity_id: UUID,
        timestamp: datetime
    ) -> Optional[EntityChangeHistory]:
        """
        Get entity version at specific point in time.
        
        Args:
            entity_type: Entity type
            entity_id: Entity ID
            timestamp: Point in time
            
        Returns:
            Closest version snapshot before timestamp
        """
        return self.session.query(EntityChangeHistory).filter(
            EntityChangeHistory.entity_type == entity_type,
            EntityChangeHistory.entity_id == entity_id,
            EntityChangeHistory.snapshot_timestamp <= timestamp
        ).order_by(desc(EntityChangeHistory.snapshot_timestamp))\
            .first()
    
    def compare_versions(
        self,
        entity_type: str,
        entity_id: UUID,
        version1: int,
        version2: int
    ) -> Dict[str, Any]:
        """
        Compare two versions of an entity.
        
        Args:
            entity_type: Entity type
            entity_id: Entity ID
            version1: First version number
            version2: Second version number
            
        Returns:
            Comparison results
        """
        v1 = self.get_version_by_number(entity_type, entity_id, version1)
        v2 = self.get_version_by_number(entity_type, entity_id, version2)
        
        if not v1 or not v2:
            raise ValueError("One or both versions not found")
        
        # Compare snapshot data
        data1 = v1.snapshot_data
        data2 = v2.snapshot_data
        
        added = {k: v for k, v in data2.items() if k not in data1}
        removed = {k: v for k, v in data1.items() if k not in data2}
        changed = {
            k: {'old': data1[k], 'new': data2[k]}
            for k in data1.keys() & data2.keys()
            if data1[k] != data2[k]
        }
        
        return {
            'version1': version1,
            'version2': version2,
            'timestamp1': v1.snapshot_timestamp.isoformat(),
            'timestamp2': v2.snapshot_timestamp.isoformat(),
            'added_fields': added,
            'removed_fields': removed,
            'changed_fields': changed,
            'total_changes': len(added) + len(removed) + len(changed)
        }

# --- File: C:\Hostel-Main\app\repositories\audit\supervisor_activity_log_repository.py ---
"""
Supervisor activity log repository for tracking supervisor actions.

Provides comprehensive supervisor activity tracking with performance
metrics, analytics, and accountability features.
"""

from datetime import datetime, timedelta
from typing import Any, Dict, List, Optional, Tuple
from uuid import UUID
from decimal import Decimal

from sqlalchemy import and_, or_, func, desc, asc, cast, Integer
from sqlalchemy.orm import Session

from app.models.audit import SupervisorActivityLog
from app.repositories.base.base_repository import BaseRepository
from app.schemas.audit.supervisor_activity_log import SupervisorActionCategory


class SupervisorActivityLogRepository(BaseRepository):
    """
    Repository for supervisor activity tracking and analytics.
    
    Provides comprehensive supervisor action logging, performance
    monitoring, and accountability features.
    """
    
    def __init__(self, session: Session):
        """Initialize repository with database session."""
        super().__init__(session, SupervisorActivityLog)
    
    # ==================== CRUD Operations ====================
    
    def create_activity_log(
        self,
        supervisor_id: UUID,
        hostel_id: UUID,
        action_type: str,
        action_category: SupervisorActionCategory,
        action_description: str,
        entity_type: Optional[str] = None,
        entity_id: Optional[UUID] = None,
        status: str = "completed",
        metadata: Optional[Dict] = None,
        **kwargs
    ) -> SupervisorActivityLog:
        """
        Create a new supervisor activity log entry.
        
        Args:
            supervisor_id: Supervisor performing action
            hostel_id: Hostel where action occurred
            action_type: Specific action identifier
            action_category: High-level action category
            action_description: Human-readable description
            entity_type: Type of entity affected
            entity_id: ID of affected entity
            status: Action status
            metadata: Additional context
            **kwargs: Additional fields
            
        Returns:
            Created SupervisorActivityLog instance
        """
        activity_log = SupervisorActivityLog(
            supervisor_id=supervisor_id,
            hostel_id=hostel_id,
            action_type=action_type,
            action_category=action_category,
            action_description=action_description,
            entity_type=entity_type,
            entity_id=entity_id,
            status=status,
            metadata=metadata or {},
            **kwargs
        )
        
        return self.create(activity_log)
    
    def bulk_create_activity_logs(
        self,
        activity_logs: List[Dict[str, Any]],
        batch_size: int = 1000
    ) -> List[SupervisorActivityLog]:
        """
        Bulk create supervisor activity logs.
        
        Args:
            activity_logs: List of activity log dictionaries
            batch_size: Batch size for processing
            
        Returns:
            List of created activity logs
        """
        created_logs = []
        
        for i in range(0, len(activity_logs), batch_size):
            batch = activity_logs[i:i + batch_size]
            log_objects = [SupervisorActivityLog(**log_data) for log_data in batch]
            
            self.session.bulk_save_objects(log_objects, return_defaults=True)
            self.session.flush()
            
            created_logs.extend(log_objects)
        
        self.session.commit()
        return created_logs
    
    # ==================== Query Operations ====================
    
    def find_by_supervisor(
        self,
        supervisor_id: UUID,
        start_date: Optional[datetime] = None,
        end_date: Optional[datetime] = None,
        action_category: Optional[SupervisorActionCategory] = None,
        status: Optional[str] = None,
        limit: int = 100,
        offset: int = 0
    ) -> Tuple[List[SupervisorActivityLog], int]:
        """
        Find activity logs by supervisor with filtering.
        
        Args:
            supervisor_id: Supervisor ID
            start_date: Start date filter
            end_date: End date filter
            action_category: Optional category filter
            status: Optional status filter
            limit: Maximum results
            offset: Results to skip
            
        Returns:
            Tuple of (activity logs, total count)
        """
        query = self.session.query(SupervisorActivityLog).filter(
            SupervisorActivityLog.supervisor_id == supervisor_id
        )
        
        if start_date:
            query = query.filter(SupervisorActivityLog.created_at >= start_date)
        
        if end_date:
            query = query.filter(SupervisorActivityLog.created_at <= end_date)
        
        if action_category:
            query = query.filter(SupervisorActivityLog.action_category == action_category)
        
        if status:
            query = query.filter(SupervisorActivityLog.status == status)
        
        total = query.count()
        
        results = query.order_by(desc(SupervisorActivityLog.created_at))\
            .limit(limit)\
            .offset(offset)\
            .all()
        
        return results, total
    
    def find_by_hostel(
        self,
        hostel_id: UUID,
        start_date: Optional[datetime] = None,
        end_date: Optional[datetime] = None,
        action_categories: Optional[List[SupervisorActionCategory]] = None,
        supervisor_id: Optional[UUID] = None,
        limit: int = 100,
        offset: int = 0
    ) -> Tuple[List[SupervisorActivityLog], int]:
        """
        Find activity logs by hostel with filtering.
        
        Args:
            hostel_id: Hostel ID
            start_date: Start date filter
            end_date: End date filter
            action_categories: Optional categories filter
            supervisor_id: Optional supervisor filter
            limit: Maximum results
            offset: Results to skip
            
        Returns:
            Tuple of (activity logs, total count)
        """
        query = self.session.query(SupervisorActivityLog).filter(
            SupervisorActivityLog.hostel_id == hostel_id
        )
        
        if start_date:
            query = query.filter(SupervisorActivityLog.created_at >= start_date)
        
        if end_date:
            query = query.filter(SupervisorActivityLog.created_at <= end_date)
        
        if action_categories:
            query = query.filter(SupervisorActivityLog.action_category.in_(action_categories))
        
        if supervisor_id:
            query = query.filter(SupervisorActivityLog.supervisor_id == supervisor_id)
        
        total = query.count()
        
        results = query.order_by(desc(SupervisorActivityLog.created_at))\
            .limit(limit)\
            .offset(offset)\
            .all()
        
        return results, total
    
    def find_by_entity(
        self,
        entity_type: str,
        entity_id: UUID,
        action_category: Optional[SupervisorActionCategory] = None,
        limit: int = 100
    ) -> List[SupervisorActivityLog]:
        """
        Find activity logs for a specific entity.
        
        Args:
            entity_type: Entity type
            entity_id: Entity ID
            action_category: Optional category filter
            limit: Maximum results
            
        Returns:
            List of activity logs
        """
        query = self.session.query(SupervisorActivityLog).filter(
            SupervisorActivityLog.entity_type == entity_type,
            SupervisorActivityLog.entity_id == entity_id
        )
        
        if action_category:
            query = query.filter(SupervisorActivityLog.action_category == action_category)
        
        return query.order_by(desc(SupervisorActivityLog.created_at))\
            .limit(limit)\
            .all()
    
    def find_by_student(
        self,
        student_id: UUID,
        start_date: Optional[datetime] = None,
        end_date: Optional[datetime] = None,
        limit: int = 100
    ) -> List[SupervisorActivityLog]:
        """
        Find activities related to a specific student.
        
        Args:
            student_id: Student ID
            start_date: Start date filter
            end_date: End date filter
            limit: Maximum results
            
        Returns:
            List of activity logs
        """
        query = self.session.query(SupervisorActivityLog).filter(
            SupervisorActivityLog.related_student_id == student_id
        )
        
        if start_date:
            query = query.filter(SupervisorActivityLog.created_at >= start_date)
        
        if end_date:
            query = query.filter(SupervisorActivityLog.created_at <= end_date)
        
        return query.order_by(desc(SupervisorActivityLog.created_at))\
            .limit(limit)\
            .all()
    
    def find_by_room(
        self,
        room_id: UUID,
        start_date: Optional[datetime] = None,
        end_date: Optional[datetime] = None,
        limit: int = 100
    ) -> List[SupervisorActivityLog]:
        """
        Find activities related to a specific room.
        
        Args:
            room_id: Room ID
            start_date: Start date filter
            end_date: End date filter
            limit: Maximum results
            
        Returns:
            List of activity logs
        """
        query = self.session.query(SupervisorActivityLog).filter(
            SupervisorActivityLog.related_room_id == room_id
        )
        
        if start_date:
            query = query.filter(SupervisorActivityLog.created_at >= start_date)
        
        if end_date:
            query = query.filter(SupervisorActivityLog.created_at <= end_date)
        
        return query.order_by(desc(SupervisorActivityLog.created_at))\
            .limit(limit)\
            .all()
    
    def find_by_shift(
        self,
        shift_id: UUID,
        action_category: Optional[SupervisorActionCategory] = None,
        limit: int = 100
    ) -> List[SupervisorActivityLog]:
        """
        Find activities during a specific shift.
        
        Args:
            shift_id: Shift ID
            action_category: Optional category filter
            limit: Maximum results
            
        Returns:
            List of activity logs
        """
        query = self.session.query(SupervisorActivityLog).filter(
            SupervisorActivityLog.shift_id == shift_id
        )
        
        if action_category:
            query = query.filter(SupervisorActivityLog.action_category == action_category)
        
        return query.order_by(SupervisorActivityLog.created_at)\
            .limit(limit)\
            .all()
    
    def find_requiring_follow_up(
        self,
        supervisor_id: Optional[UUID] = None,
        hostel_id: Optional[UUID] = None,
        overdue_only: bool = False,
        limit: int = 100
    ) -> List[SupervisorActivityLog]:
        """
        Find activities requiring follow-up.
        
        Args:
            supervisor_id: Optional supervisor filter
            hostel_id: Optional hostel filter
            overdue_only: Only return overdue follow-ups
            limit: Maximum results
            
        Returns:
            List of activity logs requiring follow-up
        """
        query = self.session.query(SupervisorActivityLog).filter(
            SupervisorActivityLog.requires_follow_up == True,
            SupervisorActivityLog.follow_up_completed != True
        )
        
        if supervisor_id:
            query = query.filter(SupervisorActivityLog.supervisor_id == supervisor_id)
        
        if hostel_id:
            query = query.filter(SupervisorActivityLog.hostel_id == hostel_id)
        
        if overdue_only:
            query = query.filter(
                SupervisorActivityLog.follow_up_date < datetime.utcnow()
            )
        
        return query.order_by(SupervisorActivityLog.follow_up_date)\
            .limit(limit)\
            .all()
    
    def find_failed_actions(
        self,
        supervisor_id: Optional[UUID] = None,
        start_date: Optional[datetime] = None,
        end_date: Optional[datetime] = None,
        limit: int = 100
    ) -> List[SupervisorActivityLog]:
        """
        Find failed supervisor actions for analysis.
        
        Args:
            supervisor_id: Optional supervisor filter
            start_date: Start date filter
            end_date: End date filter
            limit: Maximum results
            
        Returns:
            List of failed activity logs
        """
        query = self.session.query(SupervisorActivityLog).filter(
            SupervisorActivityLog.status == 'failed'
        )
        
        if supervisor_id:
            query = query.filter(SupervisorActivityLog.supervisor_id == supervisor_id)
        
        if start_date:
            query = query.filter(SupervisorActivityLog.created_at >= start_date)
        
        if end_date:
            query = query.filter(SupervisorActivityLog.created_at <= end_date)
        
        return query.order_by(desc(SupervisorActivityLog.created_at))\
            .limit(limit)\
            .all()
    
    def find_high_priority_actions(
        self,
        hostel_id: Optional[UUID] = None,
        start_date: Optional[datetime] = None,
        limit: int = 100
    ) -> List[SupervisorActivityLog]:
        """
        Find high priority supervisor actions.
        
        Args:
            hostel_id: Optional hostel filter
            start_date: Start date filter
            limit: Maximum results
            
        Returns:
            List of high priority activity logs
        """
        query = self.session.query(SupervisorActivityLog).filter(
            SupervisorActivityLog.priority_level.in_(['urgent', 'critical'])
        )
        
        if hostel_id:
            query = query.filter(SupervisorActivityLog.hostel_id == hostel_id)
        
        if start_date:
            query = query.filter(SupervisorActivityLog.created_at >= start_date)
        
        return query.order_by(desc(SupervisorActivityLog.created_at))\
            .limit(limit)\
            .all()
    
    # ==================== Performance Analytics ====================
    
    def get_supervisor_performance_metrics(
        self,
        supervisor_id: UUID,
        start_date: datetime,
        end_date: datetime
    ) -> Dict[str, Any]:
        """
        Get comprehensive performance metrics for a supervisor.
        
        Args:
            supervisor_id: Supervisor ID
            start_date: Period start
            end_date: Period end
            
        Returns:
            Performance metrics dictionary
        """
        query = self.session.query(SupervisorActivityLog).filter(
            SupervisorActivityLog.supervisor_id == supervisor_id,
            SupervisorActivityLog.created_at >= start_date,
            SupervisorActivityLog.created_at <= end_date
        )
        
        all_activities = query.all()
        total_activities = len(all_activities)
        
        if total_activities == 0:
            return {
                'supervisor_id': str(supervisor_id),
                'period': {'start': start_date.isoformat(), 'end': end_date.isoformat()},
                'total_activities': 0,
                'message': 'No activities found in period'
            }
        
        # Activity breakdown by status
        status_counts = {}
        for activity in all_activities:
            status_counts[activity.status] = status_counts.get(activity.status, 0) + 1
        
        # Category breakdown
        category_counts = {}
        for activity in all_activities:
            cat = activity.action_category.value
            category_counts[cat] = category_counts.get(cat, 0) + 1
        
        # Calculate average time taken
        timed_activities = [a for a in all_activities if a.time_taken_minutes is not None]
        avg_time_taken = (
            sum(a.time_taken_minutes for a in timed_activities) / len(timed_activities)
            if timed_activities else 0
        )
        
        # Calculate average quality score
        scored_activities = [a for a in all_activities if a.quality_score is not None]
        avg_quality_score = (
            sum(float(a.quality_score) for a in scored_activities) / len(scored_activities)
            if scored_activities else 0
        )
        
        # Calculate average efficiency
        efficient_activities = [a for a in all_activities if a.efficiency_score is not None]
        avg_efficiency = (
            sum(float(a.efficiency_score) for a in efficient_activities) / len(efficient_activities)
            if efficient_activities else 0
        )
        
        # Student feedback
        feedback_activities = [a for a in all_activities if a.student_feedback_score is not None]
        avg_student_feedback = (
            sum(float(a.student_feedback_score) for a in feedback_activities) / len(feedback_activities)
            if feedback_activities else 0
        )
        
        # Follow-up metrics
        follow_up_required = [a for a in all_activities if a.requires_follow_up]
        follow_up_completed = [a for a in follow_up_required if a.follow_up_completed]
        follow_up_completion_rate = (
            len(follow_up_completed) / len(follow_up_required) * 100
            if follow_up_required else 0
        )
        
        # Priority distribution
        priority_counts = {}
        for activity in all_activities:
            if activity.priority_level:
                priority_counts[activity.priority_level] = priority_counts.get(activity.priority_level, 0) + 1
        
        return {
            'supervisor_id': str(supervisor_id),
            'period': {
                'start': start_date.isoformat(),
                'end': end_date.isoformat(),
                'days': (end_date - start_date).days
            },
            'activity_summary': {
                'total_activities': total_activities,
                'by_status': status_counts,
                'by_category': category_counts,
                'by_priority': priority_counts,
                'success_rate': (
                    status_counts.get('completed', 0) / total_activities * 100
                    if total_activities else 0
                )
            },
            'performance_metrics': {
                'average_time_taken_minutes': round(avg_time_taken, 2),
                'average_quality_score': round(avg_quality_score, 2),
                'average_efficiency_score': round(avg_efficiency, 2),
                'average_student_feedback': round(avg_student_feedback, 2)
            },
            'follow_up_metrics': {
                'total_requiring_follow_up': len(follow_up_required),
                'completed_follow_ups': len(follow_up_completed),
                'completion_rate': round(follow_up_completion_rate, 2)
            },
            'activities_per_day': round(total_activities / max((end_date - start_date).days, 1), 2)
        }
    
    def get_hostel_activity_summary(
        self,
        hostel_id: UUID,
        start_date: datetime,
        end_date: datetime
    ) -> Dict[str, Any]:
        """
        Get activity summary for a hostel.
        
        Args:
            hostel_id: Hostel ID
            start_date: Period start
            end_date: Period end
            
        Returns:
            Activity summary dictionary
        """
        query = self.session.query(SupervisorActivityLog).filter(
            SupervisorActivityLog.hostel_id == hostel_id,
            SupervisorActivityLog.created_at >= start_date,
            SupervisorActivityLog.created_at <= end_date
        )
        
        all_activities = query.all()
        total_activities = len(all_activities)
        
        # Supervisor activity counts
        supervisor_counts = self.session.query(
            SupervisorActivityLog.supervisor_id,
            SupervisorActivityLog.supervisor_name,
            func.count(SupervisorActivityLog.id).label('activity_count')
        ).filter(
            SupervisorActivityLog.hostel_id == hostel_id,
            SupervisorActivityLog.created_at >= start_date,
            SupervisorActivityLog.created_at <= end_date
        ).group_by(
            SupervisorActivityLog.supervisor_id,
            SupervisorActivityLog.supervisor_name
        ).order_by(desc('activity_count')).all()
        
        # Category breakdown
        category_counts = self.session.query(
            SupervisorActivityLog.action_category,
            func.count(SupervisorActivityLog.id).label('count')
        ).filter(
            SupervisorActivityLog.hostel_id == hostel_id,
            SupervisorActivityLog.created_at >= start_date,
            SupervisorActivityLog.created_at <= end_date
        ).group_by(SupervisorActivityLog.action_category).all()
        
        # Top action types
        top_actions = self.session.query(
            SupervisorActivityLog.action_type,
            func.count(SupervisorActivityLog.id).label('count')
        ).filter(
            SupervisorActivityLog.hostel_id == hostel_id,
            SupervisorActivityLog.created_at >= start_date,
            SupervisorActivityLog.created_at <= end_date
        ).group_by(SupervisorActivityLog.action_type)\
            .order_by(desc('count'))\
            .limit(10)\
            .all()
        
        return {
            'hostel_id': str(hostel_id),
            'period': {
                'start': start_date.isoformat(),
                'end': end_date.isoformat()
            },
            'total_activities': total_activities,
            'supervisor_activity': [
                {
                    'supervisor_id': str(sup_id),
                    'supervisor_name': sup_name,
                    'activity_count': count
                }
                for sup_id, sup_name, count in supervisor_counts
            ],
            'activities_by_category': {
                cat.value: count for cat, count in category_counts
            },
            'top_action_types': [
                {'action_type': action, 'count': count}
                for action, count in top_actions
            ]
        }
    
    def get_activity_timeline(
        self,
        supervisor_id: UUID,
        start_date: datetime,
        end_date: datetime,
        group_by: str = 'day'  # 'hour', 'day', 'week'
    ) -> List[Dict[str, Any]]:
        """
        Get supervisor activity timeline.
        
        Args:
            supervisor_id: Supervisor ID
            start_date: Period start
            end_date: Period end
            group_by: Grouping interval
            
        Returns:
            Timeline data points
        """
        if group_by == 'hour':
            time_format = func.date_trunc('hour', SupervisorActivityLog.created_at)
        elif group_by == 'day':
            time_format = func.date_trunc('day', SupervisorActivityLog.created_at)
        elif group_by == 'week':
            time_format = func.date_trunc('week', SupervisorActivityLog.created_at)
        else:
            time_format = func.date_trunc('day', SupervisorActivityLog.created_at)
        
        timeline = self.session.query(
            time_format.label('time_bucket'),
            func.count(SupervisorActivityLog.id).label('activity_count'),
            SupervisorActivityLog.action_category
        ).filter(
            SupervisorActivityLog.supervisor_id == supervisor_id,
            SupervisorActivityLog.created_at >= start_date,
            SupervisorActivityLog.created_at <= end_date
        ).group_by(
            'time_bucket',
            SupervisorActivityLog.action_category
        ).order_by('time_bucket').all()
        
        return [
            {
                'timestamp': bucket.isoformat(),
                'count': count,
                'category': category.value
            }
            for bucket, count, category in timeline
        ]
    
    def get_efficiency_analysis(
        self,
        supervisor_id: UUID,
        start_date: datetime,
        end_date: datetime
    ) -> Dict[str, Any]:
        """
        Analyze supervisor efficiency metrics.
        
        Args:
            supervisor_id: Supervisor ID
            start_date: Period start
            end_date: Period end
            
        Returns:
            Efficiency analysis
        """
        activities = self.session.query(SupervisorActivityLog).filter(
            SupervisorActivityLog.supervisor_id == supervisor_id,
            SupervisorActivityLog.created_at >= start_date,
            SupervisorActivityLog.created_at <= end_date,
            SupervisorActivityLog.efficiency_score.isnot(None)
        ).all()
        
        if not activities:
            return {
                'supervisor_id': str(supervisor_id),
                'message': 'No efficiency data available'
            }
        
        efficiency_scores = [float(a.efficiency_score) for a in activities]
        
        # Calculate statistics
        avg_efficiency = sum(efficiency_scores) / len(efficiency_scores)
        min_efficiency = min(efficiency_scores)
        max_efficiency = max(efficiency_scores)
        
        # Efficiency by category
        category_efficiency = {}
        for activity in activities:
            cat = activity.action_category.value
            if cat not in category_efficiency:
                category_efficiency[cat] = []
            category_efficiency[cat].append(float(activity.efficiency_score))
        
        category_avg = {
            cat: sum(scores) / len(scores)
            for cat, scores in category_efficiency.items()
        }
        
        # Trend analysis (simple linear trend)
        if len(activities) > 1:
            sorted_activities = sorted(activities, key=lambda a: a.created_at)
            first_half = sorted_activities[:len(sorted_activities)//2]
            second_half = sorted_activities[len(sorted_activities)//2:]
            
            first_avg = sum(float(a.efficiency_score) for a in first_half) / len(first_half)
            second_avg = sum(float(a.efficiency_score) for a in second_half) / len(second_half)
            
            trend = 'improving' if second_avg > first_avg else 'declining' if second_avg < first_avg else 'stable'
        else:
            trend = 'insufficient_data'
        
        return {
            'supervisor_id': str(supervisor_id),
            'period': {
                'start': start_date.isoformat(),
                'end': end_date.isoformat()
            },
            'overall_efficiency': {
                'average': round(avg_efficiency, 2),
                'minimum': round(min_efficiency, 2),
                'maximum': round(max_efficiency, 2),
                'total_activities': len(activities)
            },
            'efficiency_by_category': {
                cat: round(avg, 2)
                for cat, avg in category_avg.items()
            },
            'trend': trend
        }
    
    def compare_supervisor_performance(
        self,
        hostel_id: UUID,
        start_date: datetime,
        end_date: datetime,
        limit: int = 10
    ) -> List[Dict[str, Any]]:
        """
        Compare performance across supervisors in a hostel.
        
        Args:
            hostel_id: Hostel ID
            start_date: Period start
            end_date: Period end
            limit: Number of supervisors to include
            
        Returns:
            Comparative performance data
        """
        # Get supervisor activity counts and metrics
        supervisor_stats = self.session.query(
            SupervisorActivityLog.supervisor_id,
            SupervisorActivityLog.supervisor_name,
            func.count(SupervisorActivityLog.id).label('total_activities'),
            func.avg(SupervisorActivityLog.quality_score).label('avg_quality'),
            func.avg(SupervisorActivityLog.efficiency_score).label('avg_efficiency'),
            func.avg(SupervisorActivityLog.student_feedback_score).label('avg_feedback')
        ).filter(
            SupervisorActivityLog.hostel_id == hostel_id,
            SupervisorActivityLog.created_at >= start_date,
            SupervisorActivityLog.created_at <= end_date
        ).group_by(
            SupervisorActivityLog.supervisor_id,
            SupervisorActivityLog.supervisor_name
        ).order_by(desc('total_activities')).limit(limit).all()
        
        results = []
        for sup_id, sup_name, total, quality, efficiency, feedback in supervisor_stats:
            # Get success rate
            completed = self.session.query(func.count(SupervisorActivityLog.id)).filter(
                SupervisorActivityLog.supervisor_id == sup_id,
                SupervisorActivityLog.hostel_id == hostel_id,
                SupervisorActivityLog.created_at >= start_date,
                SupervisorActivityLog.created_at <= end_date,
                SupervisorActivityLog.status == 'completed'
            ).scalar()
            
            success_rate = (completed / total * 100) if total else 0
            
            results.append({
                'supervisor_id': str(sup_id),
                'supervisor_name': sup_name,
                'total_activities': total,
                'success_rate': round(success_rate, 2),
                'average_quality_score': round(float(quality), 2) if quality else None,
                'average_efficiency_score': round(float(efficiency), 2) if efficiency else None,
                'average_student_feedback': round(float(feedback), 2) if feedback else None
            })
        
        return results
    
    def get_workload_distribution(
        self,
        hostel_id: UUID,
        start_date: datetime,
        end_date: datetime
    ) -> Dict[str, Any]:
        """
        Analyze workload distribution across supervisors.
        
        Args:
            hostel_id: Hostel ID
            start_date: Period start
            end_date: Period end
            
        Returns:
            Workload distribution analysis
        """
        workload = self.session.query(
            SupervisorActivityLog.supervisor_id,
            SupervisorActivityLog.supervisor_name,
            func.count(SupervisorActivityLog.id).label('activity_count'),
            func.sum(SupervisorActivityLog.time_taken_minutes).label('total_time')
        ).filter(
            SupervisorActivityLog.hostel_id == hostel_id,
            SupervisorActivityLog.created_at >= start_date,
            SupervisorActivityLog.created_at <= end_date
        ).group_by(
            SupervisorActivityLog.supervisor_id,
            SupervisorActivityLog.supervisor_name
        ).all()
        
        if not workload:
            return {
                'hostel_id': str(hostel_id),
                'message': 'No workload data available'
            }
        
        total_activities = sum(w[2] for w in workload)
        activity_counts = [w[2] for w in workload]
        avg_activities = total_activities / len(workload)
        
        # Calculate workload balance (coefficient of variation)
        variance = sum((count - avg_activities) ** 2 for count in activity_counts) / len(activity_counts)
        std_dev = variance ** 0.5
        cv = (std_dev / avg_activities * 100) if avg_activities else 0
        
        # Identify overloaded and underloaded supervisors
        threshold_high = avg_activities * 1.5
        threshold_low = avg_activities * 0.5
        
        distribution = []
        for sup_id, sup_name, count, total_time in workload:
            status = 'balanced'
            if count > threshold_high:
                status = 'overloaded'
            elif count < threshold_low:
                status = 'underloaded'
            
            distribution.append({
                'supervisor_id': str(sup_id),
                'supervisor_name': sup_name,
                'activity_count': count,
                'total_time_minutes': int(total_time) if total_time else 0,
                'workload_status': status,
                'percentage_of_total': round(count / total_activities * 100, 2)
            })
        
        return {
            'hostel_id': str(hostel_id),
            'period': {
                'start': start_date.isoformat(),
                'end': end_date.isoformat()
            },
            'total_activities': total_activities,
            'average_activities_per_supervisor': round(avg_activities, 2),
            'workload_balance_coefficient': round(cv, 2),
            'balance_assessment': 'balanced' if cv < 30 else 'unbalanced',
            'supervisor_distribution': sorted(
                distribution,
                key=lambda x: x['activity_count'],
                reverse=True
            )
        }
    
    # ==================== Maintenance Operations ====================
    
    def mark_follow_up_completed(
        self,
        activity_id: UUID
    ) -> SupervisorActivityLog:
        """
        Mark an activity's follow-up as completed.
        
        Args:
            activity_id: Activity log ID
            
        Returns:
            Updated activity log
        """
        activity = self.get_by_id(activity_id)
        if not activity:
            raise ValueError(f"Activity {activity_id} not found")
        
        activity.follow_up_completed = True
        self.session.commit()
        
        return activity
    
    def cleanup_old_logs(
        self,
        cutoff_date: datetime,
        batch_size: int = 1000
    ) -> int:
        """
        Archive or delete old activity logs.
        
        Args:
            cutoff_date: Date before which to cleanup
            batch_size: Batch size for processing
            
        Returns:
            Number of cleaned up records
        """
        count = self.session.query(SupervisorActivityLog).filter(
            SupervisorActivityLog.created_at < cutoff_date
        ).count()
        
        # TODO: Implement actual archival or deletion
        
        return count

# --- File: C:\Hostel-Main\app\repositories\audit\__init__.py ---
"""
Audit repositories package.

Provides comprehensive audit trail and activity logging repositories
for system-wide tracking, compliance, and analytics.
"""

from app.repositories.audit.audit_log_repository import AuditLogRepository
from app.repositories.audit.entity_change_log_repository import (
    EntityChangeLogRepository,
    EntityChangeHistoryRepository
)
from app.repositories.audit.supervisor_activity_log_repository import (
    SupervisorActivityLogRepository
)
from app.repositories.audit.admin_override_log_repository import (
    AdminOverrideLogRepository
)
from app.repositories.audit.audit_aggregate_repository import (
    AuditAggregateRepository
)

__all__ = [
    # Core audit repositories
    "AuditLogRepository",
    "EntityChangeLogRepository",
    "EntityChangeHistoryRepository",
    
    # Specialized audit repositories
    "SupervisorActivityLogRepository",
    "AdminOverrideLogRepository",
    
    # Aggregation and analytics
    "AuditAggregateRepository",
]
